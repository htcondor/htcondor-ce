{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HTCondor-CE 4 and 5 \u00b6 The HTCondor-CE software is a Compute Entrypoint (CE) based on HTCondor for sites that are part of a larger computing grid (e.g. European Grid Infrastructure , Open Science Grid ). As such, HTCondor-CE serves as a \"door\" for incoming resource allocation requests \u2014 it handles authorization and delegation of these requests to a grid site's local batch system. Supported batch systems include Grid Engine , HTCondor , LSF , PBS Pro / Torque , and Slurm . For an introduction to HTCondor-CE, watch our recorded webinar from the EGI Community Webinar Programme: Or visit the overview page for more details on the features and architecture of HTCondor-CE. Contact Us \u00b6 HTCondor-CE is developed and maintained by the Center for High Throughput Computing . If you have questions or issues regarding HTCondor-CE, please see the HTCondor support page for how to contact us.","title":"Home"},{"location":"#htcondor-ce-4-and-5","text":"The HTCondor-CE software is a Compute Entrypoint (CE) based on HTCondor for sites that are part of a larger computing grid (e.g. European Grid Infrastructure , Open Science Grid ). As such, HTCondor-CE serves as a \"door\" for incoming resource allocation requests \u2014 it handles authorization and delegation of these requests to a grid site's local batch system. Supported batch systems include Grid Engine , HTCondor , LSF , PBS Pro / Torque , and Slurm . For an introduction to HTCondor-CE, watch our recorded webinar from the EGI Community Webinar Programme: Or visit the overview page for more details on the features and architecture of HTCondor-CE.","title":"HTCondor-CE 4 and 5"},{"location":"#contact-us","text":"HTCondor-CE is developed and maintained by the Center for High Throughput Computing . If you have questions or issues regarding HTCondor-CE, please see the HTCondor support page for how to contact us.","title":"Contact Us"},{"location":"batch-system-integration/","text":"Writing Routes For HTCondor-CE \u00b6 The JobRouter is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in /etc/condor-ce/config.d/02-ce-*.conf that provide enough basic functionality for a small site. If you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems. Definitions Incoming Job : A job which was submitted to the CE from an external source. Routed Job : A job that has been transformed by the JobRouter. Quirks and Pitfalls \u00b6 If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break. Make sure to run condor_ce_reconfig after changing your routes, otherwise they will not take effect. HTCondor batch system only: Local universe jobs are excluded from any routing. How Job Routes are Constructed \u00b6 Each job route\u2019s ClassAd is constructed by combining each entry from the JOB_ROUTER_ENTRIES with the JOB_ROUTER_DEFAULTS . Attributes that are set_* in JOB_ROUTER_ENTRIES will override those set_* in JOB_ROUTER_DEFAULTS JOB_ROUTER_ENTRIES \u00b6 JOB_ROUTER_ENTRIES is a configuration variable whose default is set in /etc/condor-ce/config.d/02-ce-*.conf but may be overriden by the administrator in /etc/condor-ce/config.d/99-local.conf . This document outlines the many changes you can make to JOB_ROUTER_ENTRIES to fit your site\u2019s needs. JOB_ROUTER_DEFAULTS \u00b6 JOB_ROUTER_DEFAULTS is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents in a readable format, run the following command: user@host $ condor_ce_config_val JOB_ROUTER_DEFAULTS | sed 's/;/;\\n/g' Warning If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break. Warning Do not set the JOB_ROUTER_DEFAULTS configuration variable yourself. This will cause the CE to stop functioning. How Jobs Match to Job Routes \u00b6 The job router considers jobs in the queue ( condor_ce_q ) that meet the following constraints: The job has not already been considered by the job router The job is associated with an unexpired x509 proxy The job's universe is standard or vanilla If the job meets the above constraints, then the job's ClassAd is compared against each route's requirements . If the job only meets one route's requirements, the job is matched to that route. If the job meets the requirements of multiple routes, the route that is chosen depends on your version of HTCondor ( condor_version ): If your version of HTCondor is... Then the route is chosen by... < 8.7.1 Round-robin between all matching routes. In this case, we recommend making each route's requirements mutually exclusive. >= 8.7.1, < 8.9.5 First matching route where routes are considered in hash-table order. In this case, we recommend making each route's requirements mutually exclusive. >= 8.9.5 First matching route where routes are considered in the order specified by JOB_ROUTER_ROUTE_NAMES Job Route Order For HTCondor versions < 8.9.5 (as well as versions >= 8.7.1 and < 8.8.7) the order of job routes does not match the order in which they are configured. As a result, we recommend updating to at least HTCondor 8.9.5 (or 8.8.7) and specifying the names of your routes in JOB_ROUTER_ROUTE_NAMES in the order that you'd like them considered. If you are using HTCondor >= 8.7.1 and would like to use round-robin matching, add the following text to a file in /etc/condor-ce/config.d/ : JOB_ROUTER_ROUND_ROBIN_SELECTION = True Generic Routes \u00b6 This section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in /etc/condor-ce/config.d/99-local.conf , not the original 02-ce-*.conf . Required fields \u00b6 The minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in /usr/share/condor-ce/config.d/02-ce-<batch system>-defaults.conf , provided by the htcondor-ce-<batch system> packages. Batch system \u00b6 Each route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the TargetUniverse attribute needs to be set to 5 or \"vanilla\" . For all other batch systems, the TargetUniverse attribute needs to be set to 9 or \"grid\" and the GridResource attribute needs to be set to \"batch <batch system>\" (where <batch system> can be one of pbs , slurm , lsf , or sge ). JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Route jobs to PBS\"; ] @jre Route name \u00b6 To identify routes, you will need to assign a name to the route with the name attribute: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; ] @jre The name of the route will be useful in debugging since it shows up in the output of condor_ce_job_router_info , the JobRouterLog , and in the ClassAd of the routed job, which can be viewed with condor_ce_q or condor_ce_history . Writing multiple routes \u00b6 Note Before writing multiple routes, consider the details of how jobs match to job routes If your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues), you will need to write multiple job routes where each route is enclosed by square brackets. The following routes takes incoming jobs that have a queue attribute set to \"analy\" and routes them to the site's HTCondor batch system. Any other jobs will be sent to that site's PBS batch system. JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; Requirements = (TARGET.queue =?= \"analy\"); ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Route jobs to PBS\"; Requirements = (TARGET.queue =!= \"analy\"); ] @jre Writing comments \u00b6 To write comments you can use # to comment a line: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"# comments\"; # This is a comment ] @jre Setting attributes for all routes \u00b6 To set an attribute that will be applied to all routes, you will need to ensure that MERGE_JOB_ROUTER_DEFAULT_ADS is set to True (check the value with condor_ce_config_val ) and use the set_ function in the JOB_ROUTER_DEFAULTS . The following configuration sets the Periodic_Hold attribute for all routes: # Use the defaults generated by the condor_ce_router_defaults script. To add # additional defaults, add additional lines of the form: # # JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;] # MERGE_JOB_ROUTER_DEFAULT_ADS=True JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1;] Filtering jobs based on\u2026 \u00b6 To filter jobs, use the Requirements attribute. Jobs will evaluate against the ClassAd expression set in the Requirements and if the expression evaluates to TRUE , the route will match. More information on the syntax of ClassAd's can be found in the HTCondor manual . For an example on how incoming jobs interact with filtering in job routes, consult this document . When setting requirements, you need to prefix job attributes that you are filtering with TARGET. so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a queue = \"analy\" attribute, then the following job route will not match: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by queue\"; queue = \"not-analy\"; Requirements = (queue =?= \"analy\"); ] @jre This is because when evaluating the route requirement, the job route will compare its own queue attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the HTCondor manual . Note If you have an HTCondor batch system, note the difference with set_requirements . Note Before writing multiple routes, consider the details of how jobs match to job routes . Glidein queue \u00b6 To filter jobs based on their glidein queue attribute, your routes will need a Requirements expression using the incoming job's queue attribute. The following entry routes jobs to HTCondor if the incoming job (specified by TARGET ) is an analy (Analysis) glidein: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by queue\"; Requirements = (TARGET.queue =?= \"analy\"); ] @jre Job submitter \u00b6 To filter jobs based on who submitted it, your routes will need a Requirements expression using the incoming job's Owner attribute. The following entry routes jobs to the HTCondor batch system if the submitter is usatlas2 : JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by job submitter\"; Requirements = (TARGET.Owner =?= \"usatlas2\"); ] @jre Alternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system if the submitter's name begins with usatlas : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Filtering by job submitter (regular expression)\"; Requirements = regexp(\"^usatlas\", TARGET.Owner); ] @jre VOMS attribute \u00b6 To filter jobs based on the subject of the job's proxy, your routes will need a Requirements expression using the incoming job's x509UserProxyFirstFQAN attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains /cms/Role=Pilot : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Filtering by VOMS attribute (regex)\"; Requirements = regexp(\"\\/cms\\/Role\\=pilot\", TARGET.x509UserProxyFirstFQAN); ] @jre Setting a default\u2026 \u00b6 This section outlines how to set default job limits, memory, cores, and maximum walltime. For an example on how users can override these defaults, consult this document . Maximum number of jobs \u00b6 To set a default limit to the maximum number of jobs per route, you can edit the configuration variable CONDORCE_MAX_JOBS in /etc/condor-ce/config.d/01-ce-router.conf : CONDORCE_MAX_JOBS = 10000 Note The above configuration is to be placed directly into the HTCondor-CE configuration, not into a job route. Maximum memory \u00b6 To set a default maximum memory (in MB) for routed jobs, set the attribute default_maxMemory : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Request memory\"; # Set the requested memory to 1 GB set_default_maxMemory = 1000; ] @jre Number of cores to request \u00b6 To set a default number of cores for routed jobs, set the attribute default_xcount : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Request CPU\"; # Set the requested cores to 8 set_default_xcount = 8; ] @jre Maximum walltime \u00b6 To set a default maximum walltime (in minutes) for routed jobs, set the attribute default_maxWallTime : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting WallTime\"; # Set the max walltime to 1 hr set_default_maxWallTime = 60; ] @jre Setting job environments \u00b6 HTCondor-CE offers two different methods for setting environment variables of routed jobs: CONDORCE_PILOT_JOB_ENV configuration, which should be used for setting environment variables for all routed jobs to static strings. set_default_pilot_job_env job route configuration, which should be used for setting environment variables: Per job route To values based on incoming job attributes Using ClassAd functions Both of these methods use the new HTCondor format of the environment command , which is described by environment variable/value pairs separated by whitespace and enclosed in double-quotes. For example, the following HTCondor-CE configuration would result in the following environment for all routed jobs: ``` tab=\"HTCondor-CE Configuration\" CONDORCE_PILOT_JOB_ENV = \"WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu\" ```bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu Contents of CONDORCE_PILOT_JOB_ENV can reference other HTCondor-CE configuration using HTCondor's configuration $() macro expansion . For example, the following HTCondor-CE configuration would result in the following environment for all routed jobs: ``` tab=\"HTCondor-CE Configuration\" LOCAL_PROXY = proxy.wisc.edu CONDORCE_PILOT_JOB_ENV = \"WN_SCRATCH_DIR=/nobackup/ http_proxy=$(LOCAL_PROXY)\" ```bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu To set environment variables per job route, based on incoming job attributes, or using ClassAd functions, add set_default_pilot_job_env to your job route configuration. For example, the following HTCondor-CE configuration would result in this environment for a job with these attributes: ``` tab=\"HTCondor-CE Configuration\" hl_lines=\"5 6 7\" JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Local_Condor\"; set_default_pilot_job_env = strcat(\"WN_SCRATCH_DIR=/nobackup\", \" PILOT_COLLECTOR=\", JOB_COLLECTOR, \" ACCOUNTING_GROUP=\", toLower(JOB_VO)); ] @jre ``` tab=\"Incoming Job Attributes\" JOB_COLLECTOR = \"collector.wisc.edu\" JOB_VO = \"GLOW\" ``` bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ PILOT_COLLECTOR=collector.wisc.edu ACCOUNTING_GROUP=glow !!!tip \"Debugging job route environment expressions\" While constructing `set_default_pilot_job_env` expressions, try wrapping your expression in [debug()](#debugging-routes) to help with any issues that may arise. Make sure to remove `debug()` after you're done! ### Editing attributes\u2026 The following functions are operations that affect job attributes and are evaluated in the following order: 1. `copy_*` 2. `delete_*` 3. `set_*` 4. `eval_set_*` After each job route\u2019s ClassAd is [constructed](#how-job-routes-are-constructed), the above operations are evaluated in order. For example, if the attribute `foo` is set using `eval_set_foo` in the `JOB_ROUTER_DEFAULTS`, you'll be unable to use `delete_foo` to remove it from your jobs since the attribute is set using `eval_set_foo` after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in `JOB_ROUTER_DEFAULTS` get overridden by the same operation in `JOB_ROUTER_ENTRIES`. So to 'delete' `foo`, we would add `eval_set_foo = \"\"` to the route in the `JOB_ROUTER_ENTRIES`, resulting in `foo` being absent from the routed job. More documentation can be found in the [HTCondor manual](http://research.cs.wisc.edu/htcondor/manual/v8.6/5_4HTCondor_Job.html#SECTION00644000000000000000). #### Copying attributes To copy the value of an attribute of the incoming job to an attribute of the routed job, use `copy_`. The following route copies the `environment` attribute of the incoming job and sets the attribute `Original_Environment` on the routed job to the same value: ```hl_lines=\"6\" JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Copying attributes\"; copy_environment = \"Original_Environment\"; ] @jre Removing attributes \u00b6 To remove an attribute of the incoming job from the routed job, use delete_ . The following route removes the environment attribute from the routed job: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Copying attributes\"; delete_environment = True; ] @jre Setting attributes \u00b6 To set an attribute on the routed job, use set_ . The following route sets the Job's Rank attribute to 5: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting an attribute\"; set_Rank = 5; ] @jre Setting attributes with ClassAd expressions \u00b6 To set an attribute to a ClassAd expression to be evaluated, use eval_set . The following route sets the Experiment attribute to atlas.osguser if the Owner of the incoming job is osguser : Note If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting an attribute with a !ClassAd expression\"; eval_set_Experiment = strcat(\"atlas.\", Owner); ] @jre Limiting the number of jobs \u00b6 This section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route). Note If you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via user priorities and group accounting . Total jobs \u00b6 To set a limit on the number of jobs for a specific route, set the MaxJobs attribute: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Limit the total number of jobs to 100\"; MaxJobs = 100; ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Limit the total number of jobs to 75\"; MaxJobs = 75; ] @jre Idle jobs \u00b6 To set a limit on the number of idle jobs for a specific route, set the MaxIdleJobs attribute: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Limit the total number of idle jobs to 100\"; MaxIdleJobs = 100; ] [ TargetUniverse = 5; name = \"Limit the total number of idle jobs to 75\"; MaxIdleJobs = 75; ] @jre Debugging routes \u00b6 To help debug expressions in your routes, you can use the debug() function. First, set the debug mode for the JobRouter by editing a file in /etc/condor-ce/config.d/ to read JOB_ROUTER_DEBUG = D_ALWAYS:2 D_CAT Then wrap the problematic attribute in debug() : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Debugging a difficult !ClassAd expression\"; eval_set_Experiment = debug(strcat(\"atlas\", Name)); ] @jre You will find the debugging output in /var/log/condor-ce/JobRouterLog . Routes for HTCondor Batch Systems \u00b6 This section contains information about job routes that can be used if you are running an HTCondor batch system at your site. Setting periodic hold, release or remove \u00b6 To release, remove or put a job on hold if it meets certain criteria, use the PERIODIC_* family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting PERIODIC_EXPR_INTERVAL in your CE's configuration. In this example, we set the routed job on hold if the job is idle and has been started at least once or if the job has tried to start more than once. This will catch jobs which are starting and stopping multiple times. JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Setting periodic statements\"; # Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; # Remove routed jobs if their walltime is longer than 3 days and 5 minutes set_Periodic_Remove = ( RemoteWallClockTime > (3*24*60*60 + 5*60) ); # Release routed jobs if the condor_starter couldn't start the executable and 'VMGAHP_ERR_INTERNAL' is in the HoldReason set_Periodic_Release = HoldReasonCode == 6 && regexp(\"VMGAHP_ERR_INTERNAL\", HoldReason); ] @jre Setting routed job requirements \u00b6 If you need to set requirements on your routed job, you will need to use set_Requirements instead of Requirements . The Requirements attribute filters jobs coming into your CE into different job routes whereas set_Requirements will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the HTCondor manual . To ensure that your job lands on a Linux machine in your pool: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; set_Requirements = OpSys == \"LINUX\"; ] @jre Preserving original job requirements \u00b6 To preserve and include the original job requirements, rather than just setting new requirements, you can use copy_Requirements to store the current value of Requirements to another variable, which we'll call original_requirements . To do this, replace the above set_Requirements line with: copy_Requirements = \"original_requirements\"; set_Requirements = original_requirements && ...; Routes for non-HTCondor Batch Systems \u00b6 This section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site. Setting a default batch queue \u00b6 To set a default queue for routed jobs, set the attribute default_queue : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting batch system queues\"; set_default_queue = \"osg_queue\"; ] @jre Setting batch system directives \u00b6 To write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in /etc/blahp/ (e.g., if your local batch system is PBS, edit /etc/blahp/pbs_local_submit_attributes.sh ). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via set_default_CERequirements , which takes a comma-separated list of other attributes: set_foo = X; set_bar = \"Y\"; set_default_CERequirements = \"foo,bar\"; This sets foo to value X and bar to the string Y in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the x509UserProxyFirstFQAN attribute of the job submitted to a PBS batch system: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting job submit variables\"; set_Walltime = 3600; set_AccountingGroup = x509UserProxyFirstFQAN; set_default_CERequirements = \"WallTime,AccountingGroup\"; ] @jre With /etc/blahp/pbs_local_submit_attributes.sh containing: #!/bin/bash echo \"#PBS -l walltime=$Walltime\" echo \"#PBS -A $AccountingGroup\" This results in the following being appended to the script that gets submitted to your batch system: #PBS -l walltime=3600 #PBS -A <CE job's x509UserProxyFirstFQAN attribute> Getting Help \u00b6 If you have any questions or issues with configuring job routes, please contact us for assistance. Reference \u00b6 Here are some example HTCondor-CE job routes: AGLT2's job routes \u00b6 Atlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes. Setting various HTCondor-specific attributes like Rank , AccountingGroup , JobPrio and Periodic_Remove (see the HTCondor manual for more). Some of these are site-specific like LastandFrac , IdleMP8Pressure , localQue , IsAnalyJob and JobMemoryLimit . There is a difference between Requirements and set_requirements . The Requirements attribute matches jobs to specific routes while the set_requirements sets the Requirements attribute on the routed job, which confines which machines that the routed job can land on. Source: https://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content JOB_ROUTER_ENTRIES @=jre # Still to do on all routes, get job requirements and add them here # Route no 1 # Analysis queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue==\"analy\"; Name = \"Analysis Queue\"; TargetUniverse = 5; eval_set_IdleMP8Pressure = $(IdleMP8Pressure); eval_set_LastAndFrac = $(LastAndFrac); set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && (IfThenElse((Owner == \"atlasconnect\" || Owner == \"muoncal\"),IfThenElse(IdleMP8Pressure,(TARGET.PARTITIONED =!= TRUE),True),IfThenElse(LastAndFrac,(TARGET.PARTITIONED =!= TRUE),True))); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Analysis\"; set_IsAnalyJob = True; set_JobPrio = 5; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 2 # splitterNT queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"splitterNT\"; Name = \"Splitter ntuple queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = \"group_calibrate.muoncal\"; set_localQue = \"Splitter\"; set_IsAnalyJob = False; set_JobPrio = 10; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 3 # splitter queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"splitter\"; Name = \"Splitter queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = \"group_calibrate.muoncal\"; set_localQue = \"Splitter\"; set_IsAnalyJob = False; set_JobPrio = 15; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 4 # xrootd queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"xrootd\"; Name = \"Xrootd queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Analysis\"; set_IsAnalyJob = True; set_JobPrio = 35; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 5 # Tier3Test queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"Tier3Test\"; Name = \"Tier3 Test Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && ( IS_TIER3_TEST_QUEUE =?= True ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Tier3Test\"; set_IsTier3TestJob = True; set_IsAnalyJob = True; set_JobPrio = 20; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 6 # mp8 queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue==\"mp8\"; Name = \"MCORE Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && (( TARGET.Cpus == 8 && TARGET.CPU_TYPE =?= \"mp8\" ) || TARGET.PARTITIONED =?= True ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.mcore.\",Owner); set_localQue = \"MP8\"; set_IsAnalyJob = False; set_JobPrio = 25; set_Rank = 0.0; eval_set_RequestCpus = 8; set_JobMemoryLimit = 33552000; set_Slot_Type = \"mp8\"; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 7 # Installation queue, triggered by usatlas2 user [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && target.Owner == \"usatlas2\"; Name = \"Install Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && ( TARGET.IS_INSTALL_QUE =?= True ) && (TARGET.AGLT2_SITE == \"UM\" ); eval_set_AccountingGroup = strcat(\"group_gatekpr.other.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_IsInstallJob = True; set_JobPrio = 15; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 8 # Default queue for usatlas1 user [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && regexp(\"usatlas1\",target.Owner); Name = \"ATLAS Production Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.prod.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 9 # Default queue for any other usatlas account [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && (regexp(\"usatlas2\",target.Owner) || regexp(\"usatlas3\",target.Owner)); Name = \"Other ATLAS Production\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.other.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 10 # Anything else. Set queue as Default and assign to other VOs [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && ifThenElse(regexp(\"usatlas\",target.Owner),false,true); Name = \"Other Jobs\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_VOgener.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] @jre BNL's job routes \u00b6 ATLAS BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes: Setting various HTCondor-specific attributes like JobLeaseDuration , Requirements and Periodic_Hold (see the HTCondor manual ). Some of these are site-specific like RACF_Group , Experiment , Job_Type and VO . Jobs are split into different routes based on the GlideIn queue that they're in. There is a difference between Requirements and set_requirements . The Requirements attribute matches incoming jobs to specific routes while the set_requirements sets the Requirements attribute on the routed job, which confines which machines that the routed job can land on. JOB_ROUTER_ENTRIES @=jre [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_long\"; Requirements = target.queue==\"analysis.long\"; eval_set_RACF_Group = \"long\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_short\"; Requirements = target.queue==\"analysis.short\"; eval_set_RACF_Group = \"short\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_grid\"; Requirements = target.queue==\"grid\"; eval_set_RACF_Group = \"grid\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool\"; Requirements = target.queue is undefined; eval_set_RACF_Group = \"grid\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"rcf\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Experiment = \"atlas\"; set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] @jre","title":"Batch System Integration"},{"location":"batch-system-integration/#writing-routes-for-htcondor-ce","text":"The JobRouter is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in /etc/condor-ce/config.d/02-ce-*.conf that provide enough basic functionality for a small site. If you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems. Definitions Incoming Job : A job which was submitted to the CE from an external source. Routed Job : A job that has been transformed by the JobRouter.","title":"Writing Routes For HTCondor-CE"},{"location":"batch-system-integration/#quirks-and-pitfalls","text":"If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break. Make sure to run condor_ce_reconfig after changing your routes, otherwise they will not take effect. HTCondor batch system only: Local universe jobs are excluded from any routing.","title":"Quirks and Pitfalls"},{"location":"batch-system-integration/#how-job-routes-are-constructed","text":"Each job route\u2019s ClassAd is constructed by combining each entry from the JOB_ROUTER_ENTRIES with the JOB_ROUTER_DEFAULTS . Attributes that are set_* in JOB_ROUTER_ENTRIES will override those set_* in JOB_ROUTER_DEFAULTS","title":"How Job Routes are Constructed"},{"location":"batch-system-integration/#job_router_entries","text":"JOB_ROUTER_ENTRIES is a configuration variable whose default is set in /etc/condor-ce/config.d/02-ce-*.conf but may be overriden by the administrator in /etc/condor-ce/config.d/99-local.conf . This document outlines the many changes you can make to JOB_ROUTER_ENTRIES to fit your site\u2019s needs.","title":"JOB_ROUTER_ENTRIES"},{"location":"batch-system-integration/#job_router_defaults","text":"JOB_ROUTER_DEFAULTS is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents in a readable format, run the following command: user@host $ condor_ce_config_val JOB_ROUTER_DEFAULTS | sed 's/;/;\\n/g' Warning If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break. Warning Do not set the JOB_ROUTER_DEFAULTS configuration variable yourself. This will cause the CE to stop functioning.","title":"JOB_ROUTER_DEFAULTS"},{"location":"batch-system-integration/#how-jobs-match-to-job-routes","text":"The job router considers jobs in the queue ( condor_ce_q ) that meet the following constraints: The job has not already been considered by the job router The job is associated with an unexpired x509 proxy The job's universe is standard or vanilla If the job meets the above constraints, then the job's ClassAd is compared against each route's requirements . If the job only meets one route's requirements, the job is matched to that route. If the job meets the requirements of multiple routes, the route that is chosen depends on your version of HTCondor ( condor_version ): If your version of HTCondor is... Then the route is chosen by... < 8.7.1 Round-robin between all matching routes. In this case, we recommend making each route's requirements mutually exclusive. >= 8.7.1, < 8.9.5 First matching route where routes are considered in hash-table order. In this case, we recommend making each route's requirements mutually exclusive. >= 8.9.5 First matching route where routes are considered in the order specified by JOB_ROUTER_ROUTE_NAMES Job Route Order For HTCondor versions < 8.9.5 (as well as versions >= 8.7.1 and < 8.8.7) the order of job routes does not match the order in which they are configured. As a result, we recommend updating to at least HTCondor 8.9.5 (or 8.8.7) and specifying the names of your routes in JOB_ROUTER_ROUTE_NAMES in the order that you'd like them considered. If you are using HTCondor >= 8.7.1 and would like to use round-robin matching, add the following text to a file in /etc/condor-ce/config.d/ : JOB_ROUTER_ROUND_ROBIN_SELECTION = True","title":"How Jobs Match to Job Routes"},{"location":"batch-system-integration/#generic-routes","text":"This section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in /etc/condor-ce/config.d/99-local.conf , not the original 02-ce-*.conf .","title":"Generic Routes"},{"location":"batch-system-integration/#required-fields","text":"The minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in /usr/share/condor-ce/config.d/02-ce-<batch system>-defaults.conf , provided by the htcondor-ce-<batch system> packages.","title":"Required fields"},{"location":"batch-system-integration/#batch-system","text":"Each route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the TargetUniverse attribute needs to be set to 5 or \"vanilla\" . For all other batch systems, the TargetUniverse attribute needs to be set to 9 or \"grid\" and the GridResource attribute needs to be set to \"batch <batch system>\" (where <batch system> can be one of pbs , slurm , lsf , or sge ). JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Route jobs to PBS\"; ] @jre","title":"Batch system"},{"location":"batch-system-integration/#route-name","text":"To identify routes, you will need to assign a name to the route with the name attribute: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; ] @jre The name of the route will be useful in debugging since it shows up in the output of condor_ce_job_router_info , the JobRouterLog , and in the ClassAd of the routed job, which can be viewed with condor_ce_q or condor_ce_history .","title":"Route name"},{"location":"batch-system-integration/#writing-multiple-routes","text":"Note Before writing multiple routes, consider the details of how jobs match to job routes If your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues), you will need to write multiple job routes where each route is enclosed by square brackets. The following routes takes incoming jobs that have a queue attribute set to \"analy\" and routes them to the site's HTCondor batch system. Any other jobs will be sent to that site's PBS batch system. JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; Requirements = (TARGET.queue =?= \"analy\"); ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Route jobs to PBS\"; Requirements = (TARGET.queue =!= \"analy\"); ] @jre","title":"Writing multiple routes"},{"location":"batch-system-integration/#writing-comments","text":"To write comments you can use # to comment a line: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"# comments\"; # This is a comment ] @jre","title":"Writing comments"},{"location":"batch-system-integration/#setting-attributes-for-all-routes","text":"To set an attribute that will be applied to all routes, you will need to ensure that MERGE_JOB_ROUTER_DEFAULT_ADS is set to True (check the value with condor_ce_config_val ) and use the set_ function in the JOB_ROUTER_DEFAULTS . The following configuration sets the Periodic_Hold attribute for all routes: # Use the defaults generated by the condor_ce_router_defaults script. To add # additional defaults, add additional lines of the form: # # JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;] # MERGE_JOB_ROUTER_DEFAULT_ADS=True JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1;]","title":"Setting attributes for all routes"},{"location":"batch-system-integration/#filtering-jobs-based-on","text":"To filter jobs, use the Requirements attribute. Jobs will evaluate against the ClassAd expression set in the Requirements and if the expression evaluates to TRUE , the route will match. More information on the syntax of ClassAd's can be found in the HTCondor manual . For an example on how incoming jobs interact with filtering in job routes, consult this document . When setting requirements, you need to prefix job attributes that you are filtering with TARGET. so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a queue = \"analy\" attribute, then the following job route will not match: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by queue\"; queue = \"not-analy\"; Requirements = (queue =?= \"analy\"); ] @jre This is because when evaluating the route requirement, the job route will compare its own queue attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the HTCondor manual . Note If you have an HTCondor batch system, note the difference with set_requirements . Note Before writing multiple routes, consider the details of how jobs match to job routes .","title":"Filtering jobs based on\u2026"},{"location":"batch-system-integration/#glidein-queue","text":"To filter jobs based on their glidein queue attribute, your routes will need a Requirements expression using the incoming job's queue attribute. The following entry routes jobs to HTCondor if the incoming job (specified by TARGET ) is an analy (Analysis) glidein: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by queue\"; Requirements = (TARGET.queue =?= \"analy\"); ] @jre","title":"Glidein queue"},{"location":"batch-system-integration/#job-submitter","text":"To filter jobs based on who submitted it, your routes will need a Requirements expression using the incoming job's Owner attribute. The following entry routes jobs to the HTCondor batch system if the submitter is usatlas2 : JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by job submitter\"; Requirements = (TARGET.Owner =?= \"usatlas2\"); ] @jre Alternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system if the submitter's name begins with usatlas : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Filtering by job submitter (regular expression)\"; Requirements = regexp(\"^usatlas\", TARGET.Owner); ] @jre","title":"Job submitter"},{"location":"batch-system-integration/#voms-attribute","text":"To filter jobs based on the subject of the job's proxy, your routes will need a Requirements expression using the incoming job's x509UserProxyFirstFQAN attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains /cms/Role=Pilot : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Filtering by VOMS attribute (regex)\"; Requirements = regexp(\"\\/cms\\/Role\\=pilot\", TARGET.x509UserProxyFirstFQAN); ] @jre","title":"VOMS attribute"},{"location":"batch-system-integration/#setting-a-default","text":"This section outlines how to set default job limits, memory, cores, and maximum walltime. For an example on how users can override these defaults, consult this document .","title":"Setting a default\u2026"},{"location":"batch-system-integration/#maximum-number-of-jobs","text":"To set a default limit to the maximum number of jobs per route, you can edit the configuration variable CONDORCE_MAX_JOBS in /etc/condor-ce/config.d/01-ce-router.conf : CONDORCE_MAX_JOBS = 10000 Note The above configuration is to be placed directly into the HTCondor-CE configuration, not into a job route.","title":"Maximum number of jobs"},{"location":"batch-system-integration/#maximum-memory","text":"To set a default maximum memory (in MB) for routed jobs, set the attribute default_maxMemory : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Request memory\"; # Set the requested memory to 1 GB set_default_maxMemory = 1000; ] @jre","title":"Maximum memory"},{"location":"batch-system-integration/#number-of-cores-to-request","text":"To set a default number of cores for routed jobs, set the attribute default_xcount : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Request CPU\"; # Set the requested cores to 8 set_default_xcount = 8; ] @jre","title":"Number of cores to request"},{"location":"batch-system-integration/#maximum-walltime","text":"To set a default maximum walltime (in minutes) for routed jobs, set the attribute default_maxWallTime : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting WallTime\"; # Set the max walltime to 1 hr set_default_maxWallTime = 60; ] @jre","title":"Maximum walltime"},{"location":"batch-system-integration/#setting-job-environments","text":"HTCondor-CE offers two different methods for setting environment variables of routed jobs: CONDORCE_PILOT_JOB_ENV configuration, which should be used for setting environment variables for all routed jobs to static strings. set_default_pilot_job_env job route configuration, which should be used for setting environment variables: Per job route To values based on incoming job attributes Using ClassAd functions Both of these methods use the new HTCondor format of the environment command , which is described by environment variable/value pairs separated by whitespace and enclosed in double-quotes. For example, the following HTCondor-CE configuration would result in the following environment for all routed jobs: ``` tab=\"HTCondor-CE Configuration\" CONDORCE_PILOT_JOB_ENV = \"WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu\" ```bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu Contents of CONDORCE_PILOT_JOB_ENV can reference other HTCondor-CE configuration using HTCondor's configuration $() macro expansion . For example, the following HTCondor-CE configuration would result in the following environment for all routed jobs: ``` tab=\"HTCondor-CE Configuration\" LOCAL_PROXY = proxy.wisc.edu CONDORCE_PILOT_JOB_ENV = \"WN_SCRATCH_DIR=/nobackup/ http_proxy=$(LOCAL_PROXY)\" ```bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu To set environment variables per job route, based on incoming job attributes, or using ClassAd functions, add set_default_pilot_job_env to your job route configuration. For example, the following HTCondor-CE configuration would result in this environment for a job with these attributes: ``` tab=\"HTCondor-CE Configuration\" hl_lines=\"5 6 7\" JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Local_Condor\"; set_default_pilot_job_env = strcat(\"WN_SCRATCH_DIR=/nobackup\", \" PILOT_COLLECTOR=\", JOB_COLLECTOR, \" ACCOUNTING_GROUP=\", toLower(JOB_VO)); ] @jre ``` tab=\"Incoming Job Attributes\" JOB_COLLECTOR = \"collector.wisc.edu\" JOB_VO = \"GLOW\" ``` bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ PILOT_COLLECTOR=collector.wisc.edu ACCOUNTING_GROUP=glow !!!tip \"Debugging job route environment expressions\" While constructing `set_default_pilot_job_env` expressions, try wrapping your expression in [debug()](#debugging-routes) to help with any issues that may arise. Make sure to remove `debug()` after you're done! ### Editing attributes\u2026 The following functions are operations that affect job attributes and are evaluated in the following order: 1. `copy_*` 2. `delete_*` 3. `set_*` 4. `eval_set_*` After each job route\u2019s ClassAd is [constructed](#how-job-routes-are-constructed), the above operations are evaluated in order. For example, if the attribute `foo` is set using `eval_set_foo` in the `JOB_ROUTER_DEFAULTS`, you'll be unable to use `delete_foo` to remove it from your jobs since the attribute is set using `eval_set_foo` after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in `JOB_ROUTER_DEFAULTS` get overridden by the same operation in `JOB_ROUTER_ENTRIES`. So to 'delete' `foo`, we would add `eval_set_foo = \"\"` to the route in the `JOB_ROUTER_ENTRIES`, resulting in `foo` being absent from the routed job. More documentation can be found in the [HTCondor manual](http://research.cs.wisc.edu/htcondor/manual/v8.6/5_4HTCondor_Job.html#SECTION00644000000000000000). #### Copying attributes To copy the value of an attribute of the incoming job to an attribute of the routed job, use `copy_`. The following route copies the `environment` attribute of the incoming job and sets the attribute `Original_Environment` on the routed job to the same value: ```hl_lines=\"6\" JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Copying attributes\"; copy_environment = \"Original_Environment\"; ] @jre","title":"Setting job environments"},{"location":"batch-system-integration/#removing-attributes","text":"To remove an attribute of the incoming job from the routed job, use delete_ . The following route removes the environment attribute from the routed job: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Copying attributes\"; delete_environment = True; ] @jre","title":"Removing attributes"},{"location":"batch-system-integration/#setting-attributes","text":"To set an attribute on the routed job, use set_ . The following route sets the Job's Rank attribute to 5: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting an attribute\"; set_Rank = 5; ] @jre","title":"Setting attributes"},{"location":"batch-system-integration/#setting-attributes-with-classad-expressions","text":"To set an attribute to a ClassAd expression to be evaluated, use eval_set . The following route sets the Experiment attribute to atlas.osguser if the Owner of the incoming job is osguser : Note If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting an attribute with a !ClassAd expression\"; eval_set_Experiment = strcat(\"atlas.\", Owner); ] @jre","title":"Setting attributes with ClassAd expressions"},{"location":"batch-system-integration/#limiting-the-number-of-jobs","text":"This section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route). Note If you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via user priorities and group accounting .","title":"Limiting the number of jobs"},{"location":"batch-system-integration/#total-jobs","text":"To set a limit on the number of jobs for a specific route, set the MaxJobs attribute: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Limit the total number of jobs to 100\"; MaxJobs = 100; ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Limit the total number of jobs to 75\"; MaxJobs = 75; ] @jre","title":"Total jobs"},{"location":"batch-system-integration/#idle-jobs","text":"To set a limit on the number of idle jobs for a specific route, set the MaxIdleJobs attribute: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Limit the total number of idle jobs to 100\"; MaxIdleJobs = 100; ] [ TargetUniverse = 5; name = \"Limit the total number of idle jobs to 75\"; MaxIdleJobs = 75; ] @jre","title":"Idle jobs"},{"location":"batch-system-integration/#debugging-routes","text":"To help debug expressions in your routes, you can use the debug() function. First, set the debug mode for the JobRouter by editing a file in /etc/condor-ce/config.d/ to read JOB_ROUTER_DEBUG = D_ALWAYS:2 D_CAT Then wrap the problematic attribute in debug() : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Debugging a difficult !ClassAd expression\"; eval_set_Experiment = debug(strcat(\"atlas\", Name)); ] @jre You will find the debugging output in /var/log/condor-ce/JobRouterLog .","title":"Debugging routes"},{"location":"batch-system-integration/#routes-for-htcondor-batch-systems","text":"This section contains information about job routes that can be used if you are running an HTCondor batch system at your site.","title":"Routes for HTCondor Batch Systems"},{"location":"batch-system-integration/#setting-periodic-hold-release-or-remove","text":"To release, remove or put a job on hold if it meets certain criteria, use the PERIODIC_* family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting PERIODIC_EXPR_INTERVAL in your CE's configuration. In this example, we set the routed job on hold if the job is idle and has been started at least once or if the job has tried to start more than once. This will catch jobs which are starting and stopping multiple times. JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Setting periodic statements\"; # Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; # Remove routed jobs if their walltime is longer than 3 days and 5 minutes set_Periodic_Remove = ( RemoteWallClockTime > (3*24*60*60 + 5*60) ); # Release routed jobs if the condor_starter couldn't start the executable and 'VMGAHP_ERR_INTERNAL' is in the HoldReason set_Periodic_Release = HoldReasonCode == 6 && regexp(\"VMGAHP_ERR_INTERNAL\", HoldReason); ] @jre","title":"Setting periodic hold, release or remove"},{"location":"batch-system-integration/#setting-routed-job-requirements","text":"If you need to set requirements on your routed job, you will need to use set_Requirements instead of Requirements . The Requirements attribute filters jobs coming into your CE into different job routes whereas set_Requirements will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the HTCondor manual . To ensure that your job lands on a Linux machine in your pool: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; set_Requirements = OpSys == \"LINUX\"; ] @jre","title":"Setting routed job requirements"},{"location":"batch-system-integration/#preserving-original-job-requirements","text":"To preserve and include the original job requirements, rather than just setting new requirements, you can use copy_Requirements to store the current value of Requirements to another variable, which we'll call original_requirements . To do this, replace the above set_Requirements line with: copy_Requirements = \"original_requirements\"; set_Requirements = original_requirements && ...;","title":"Preserving original job requirements"},{"location":"batch-system-integration/#routes-for-non-htcondor-batch-systems","text":"This section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site.","title":"Routes for non-HTCondor Batch Systems"},{"location":"batch-system-integration/#setting-a-default-batch-queue","text":"To set a default queue for routed jobs, set the attribute default_queue : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting batch system queues\"; set_default_queue = \"osg_queue\"; ] @jre","title":"Setting a default batch queue"},{"location":"batch-system-integration/#setting-batch-system-directives","text":"To write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in /etc/blahp/ (e.g., if your local batch system is PBS, edit /etc/blahp/pbs_local_submit_attributes.sh ). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via set_default_CERequirements , which takes a comma-separated list of other attributes: set_foo = X; set_bar = \"Y\"; set_default_CERequirements = \"foo,bar\"; This sets foo to value X and bar to the string Y in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the x509UserProxyFirstFQAN attribute of the job submitted to a PBS batch system: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting job submit variables\"; set_Walltime = 3600; set_AccountingGroup = x509UserProxyFirstFQAN; set_default_CERequirements = \"WallTime,AccountingGroup\"; ] @jre With /etc/blahp/pbs_local_submit_attributes.sh containing: #!/bin/bash echo \"#PBS -l walltime=$Walltime\" echo \"#PBS -A $AccountingGroup\" This results in the following being appended to the script that gets submitted to your batch system: #PBS -l walltime=3600 #PBS -A <CE job's x509UserProxyFirstFQAN attribute>","title":"Setting batch system directives"},{"location":"batch-system-integration/#getting-help","text":"If you have any questions or issues with configuring job routes, please contact us for assistance.","title":"Getting Help"},{"location":"batch-system-integration/#reference","text":"Here are some example HTCondor-CE job routes:","title":"Reference"},{"location":"batch-system-integration/#aglt2s-job-routes","text":"Atlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes. Setting various HTCondor-specific attributes like Rank , AccountingGroup , JobPrio and Periodic_Remove (see the HTCondor manual for more). Some of these are site-specific like LastandFrac , IdleMP8Pressure , localQue , IsAnalyJob and JobMemoryLimit . There is a difference between Requirements and set_requirements . The Requirements attribute matches jobs to specific routes while the set_requirements sets the Requirements attribute on the routed job, which confines which machines that the routed job can land on. Source: https://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content JOB_ROUTER_ENTRIES @=jre # Still to do on all routes, get job requirements and add them here # Route no 1 # Analysis queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue==\"analy\"; Name = \"Analysis Queue\"; TargetUniverse = 5; eval_set_IdleMP8Pressure = $(IdleMP8Pressure); eval_set_LastAndFrac = $(LastAndFrac); set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && (IfThenElse((Owner == \"atlasconnect\" || Owner == \"muoncal\"),IfThenElse(IdleMP8Pressure,(TARGET.PARTITIONED =!= TRUE),True),IfThenElse(LastAndFrac,(TARGET.PARTITIONED =!= TRUE),True))); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Analysis\"; set_IsAnalyJob = True; set_JobPrio = 5; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 2 # splitterNT queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"splitterNT\"; Name = \"Splitter ntuple queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = \"group_calibrate.muoncal\"; set_localQue = \"Splitter\"; set_IsAnalyJob = False; set_JobPrio = 10; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 3 # splitter queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"splitter\"; Name = \"Splitter queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = \"group_calibrate.muoncal\"; set_localQue = \"Splitter\"; set_IsAnalyJob = False; set_JobPrio = 15; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 4 # xrootd queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"xrootd\"; Name = \"Xrootd queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Analysis\"; set_IsAnalyJob = True; set_JobPrio = 35; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 5 # Tier3Test queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"Tier3Test\"; Name = \"Tier3 Test Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && ( IS_TIER3_TEST_QUEUE =?= True ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Tier3Test\"; set_IsTier3TestJob = True; set_IsAnalyJob = True; set_JobPrio = 20; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 6 # mp8 queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue==\"mp8\"; Name = \"MCORE Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && (( TARGET.Cpus == 8 && TARGET.CPU_TYPE =?= \"mp8\" ) || TARGET.PARTITIONED =?= True ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.mcore.\",Owner); set_localQue = \"MP8\"; set_IsAnalyJob = False; set_JobPrio = 25; set_Rank = 0.0; eval_set_RequestCpus = 8; set_JobMemoryLimit = 33552000; set_Slot_Type = \"mp8\"; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 7 # Installation queue, triggered by usatlas2 user [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && target.Owner == \"usatlas2\"; Name = \"Install Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && ( TARGET.IS_INSTALL_QUE =?= True ) && (TARGET.AGLT2_SITE == \"UM\" ); eval_set_AccountingGroup = strcat(\"group_gatekpr.other.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_IsInstallJob = True; set_JobPrio = 15; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 8 # Default queue for usatlas1 user [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && regexp(\"usatlas1\",target.Owner); Name = \"ATLAS Production Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.prod.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 9 # Default queue for any other usatlas account [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && (regexp(\"usatlas2\",target.Owner) || regexp(\"usatlas3\",target.Owner)); Name = \"Other ATLAS Production\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.other.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 10 # Anything else. Set queue as Default and assign to other VOs [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && ifThenElse(regexp(\"usatlas\",target.Owner),false,true); Name = \"Other Jobs\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_VOgener.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] @jre","title":"AGLT2's job routes"},{"location":"batch-system-integration/#bnls-job-routes","text":"ATLAS BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes: Setting various HTCondor-specific attributes like JobLeaseDuration , Requirements and Periodic_Hold (see the HTCondor manual ). Some of these are site-specific like RACF_Group , Experiment , Job_Type and VO . Jobs are split into different routes based on the GlideIn queue that they're in. There is a difference between Requirements and set_requirements . The Requirements attribute matches incoming jobs to specific routes while the set_requirements sets the Requirements attribute on the routed job, which confines which machines that the routed job can land on. JOB_ROUTER_ENTRIES @=jre [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_long\"; Requirements = target.queue==\"analysis.long\"; eval_set_RACF_Group = \"long\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_short\"; Requirements = target.queue==\"analysis.short\"; eval_set_RACF_Group = \"short\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_grid\"; Requirements = target.queue==\"grid\"; eval_set_RACF_Group = \"grid\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool\"; Requirements = target.queue is undefined; eval_set_RACF_Group = \"grid\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"rcf\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Experiment = \"atlas\"; set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] @jre","title":"BNL's job routes"},{"location":"overview/","text":"Overview \u00b6 This document serves as an introduction to HTCondor-CE and how it works. Before continuing with the overview, make sure that you are familiar with the following concepts: What is a batch system and which one will you use ( HTCondor , Grid Engine , LSF , PBS Pro / Torque , and Slurm )? Pilot jobs and factories (e.g., GlideinWMS , PanDA , DIRAC ) What is a Compute Entrypoint? \u00b6 A Compute Entrypoint (CE) is the door for remote organizations to submit requests to temporarily allocate local compute resources. These resource allocation requests are submitted as pilot jobs that create an environment for end-user jobs to match and ultimately run within the pilot job. CEs are made up of a thin layer of software that you install on a machine that already has the ability to submit and manage jobs in your local batch system. Note The Compute Entrypoint was previously known as the \"Compute Element\". What is HTCondor-CE? \u00b6 HTCondor-CE is a special configuration of the HTCondor software designed as a Compute Entrypoint solution for computing grids (e.g. European Grid Infrastructure , Open Science Grid ). It is configured to use the Job Router daemon to delegate resource allocation requests by transforming and submitting them to the site\u2019s batch system. Benefits of running the HTCondor-CE: Scalability: HTCondor-CE is capable of supporting ~16k concurrent RARs Debugging tools: HTCondor-CE offers many tools to help troubleshoot issues with RARs Routing as configuration: HTCondor-CE\u2019s mechanism to transform and submit RARs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs How Jobs Run \u00b6 Once an incoming pilot job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the Job Router creates a transformed copy (called the routed job ) and submits the copy to the batch system (called the batch system job ). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original pilot job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the batch job job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter. On HTCondor batch systems \u00b6 For a site with an HTCondor batch system , the Job Router uses HTCondor protocols to place a transformed copy of the pilot job directly into the batch system\u2019s scheduler, meaning that the routed job is also the batch system job. Thus, there are three representations of your job, each with its own ID (see diagram below): Submitter: the HTCondor job ID in the original queue HTCondor-CE: the incoming pilot job\u2019s ID HTCondor batch system: the routed job\u2019s ID In an HTCondor-CE/HTCondor setup, file transfer is handled natively between the two sets of daemons by the underlying HTCondor software. If you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in /etc/condor/ and the other in /etc/condor-ce ) and will need to make sure to differentiate the two when modifying any configuration. On other batch systems \u00b6 For non-HTCondor batch systems, the Job Router transforms the pilot job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below): Submitter: the HTCondor job ID in the original queue HTCondor-CE: the incoming pilot job\u2019s ID and the routed job\u2019s ID Non-HTCondor batch system: the batch system\u2019s job ID Although the following figure specifies the PBS case, it applies to all non-HTCondor batch systems: With non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its \"spool\" directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes. Hosted CE over SSH \u00b6 The Hosted CE is designed to be an HTCondor-CE as a Service offered by a central grid operations team. Hosted CEs submit jobs to remote clusters over SSH, providing a simple starting point for opportunistic resource owners that want to start contributing to a computing grid with minimal effort. If your site intends to run over 10,000 concurrent pilot jobs, you will need to host your own HTCondor-CE because the Hosted CE has not yet been optimized for such loads. How the CE is Customized \u00b6 Aside from the basic configuration required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all): Deciding which Virtual Organizations (VOs) are allowed to run at your site: HTCondor-CE leverages HTCondor's built-in ability to authenticate incoming jobs based on their GSI or OAuth token credentials. Additionally, HTCondor may be configured to callout to external authentication services like Argus or LCMAPS. How to filter and transform the pilot jobs to be run on your batch system: Filtering and transforming pilot jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the batch system integration page. How Security Works \u00b6 In the grid, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certificates. When these clients and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust. In its default configuration, HTCondor-CE supports token-based and GSI-based authentication and authorization to the remote submitter's credentials. HTCondor-CE also supports callouts to external authorization technoligies such as LCMAPS or Argus. Getting Help \u00b6 If you have any questions about the architecture of HTCondor-CE, please contact us for assistance.","title":"Overview"},{"location":"overview/#overview","text":"This document serves as an introduction to HTCondor-CE and how it works. Before continuing with the overview, make sure that you are familiar with the following concepts: What is a batch system and which one will you use ( HTCondor , Grid Engine , LSF , PBS Pro / Torque , and Slurm )? Pilot jobs and factories (e.g., GlideinWMS , PanDA , DIRAC )","title":"Overview"},{"location":"overview/#what-is-a-compute-entrypoint","text":"A Compute Entrypoint (CE) is the door for remote organizations to submit requests to temporarily allocate local compute resources. These resource allocation requests are submitted as pilot jobs that create an environment for end-user jobs to match and ultimately run within the pilot job. CEs are made up of a thin layer of software that you install on a machine that already has the ability to submit and manage jobs in your local batch system. Note The Compute Entrypoint was previously known as the \"Compute Element\".","title":"What is a Compute Entrypoint?"},{"location":"overview/#what-is-htcondor-ce","text":"HTCondor-CE is a special configuration of the HTCondor software designed as a Compute Entrypoint solution for computing grids (e.g. European Grid Infrastructure , Open Science Grid ). It is configured to use the Job Router daemon to delegate resource allocation requests by transforming and submitting them to the site\u2019s batch system. Benefits of running the HTCondor-CE: Scalability: HTCondor-CE is capable of supporting ~16k concurrent RARs Debugging tools: HTCondor-CE offers many tools to help troubleshoot issues with RARs Routing as configuration: HTCondor-CE\u2019s mechanism to transform and submit RARs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs","title":"What is HTCondor-CE?"},{"location":"overview/#how-jobs-run","text":"Once an incoming pilot job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the Job Router creates a transformed copy (called the routed job ) and submits the copy to the batch system (called the batch system job ). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original pilot job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the batch job job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter.","title":"How Jobs Run"},{"location":"overview/#on-htcondor-batch-systems","text":"For a site with an HTCondor batch system , the Job Router uses HTCondor protocols to place a transformed copy of the pilot job directly into the batch system\u2019s scheduler, meaning that the routed job is also the batch system job. Thus, there are three representations of your job, each with its own ID (see diagram below): Submitter: the HTCondor job ID in the original queue HTCondor-CE: the incoming pilot job\u2019s ID HTCondor batch system: the routed job\u2019s ID In an HTCondor-CE/HTCondor setup, file transfer is handled natively between the two sets of daemons by the underlying HTCondor software. If you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in /etc/condor/ and the other in /etc/condor-ce ) and will need to make sure to differentiate the two when modifying any configuration.","title":"On HTCondor batch systems"},{"location":"overview/#on-other-batch-systems","text":"For non-HTCondor batch systems, the Job Router transforms the pilot job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below): Submitter: the HTCondor job ID in the original queue HTCondor-CE: the incoming pilot job\u2019s ID and the routed job\u2019s ID Non-HTCondor batch system: the batch system\u2019s job ID Although the following figure specifies the PBS case, it applies to all non-HTCondor batch systems: With non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its \"spool\" directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes.","title":"On other batch systems"},{"location":"overview/#hosted-ce-over-ssh","text":"The Hosted CE is designed to be an HTCondor-CE as a Service offered by a central grid operations team. Hosted CEs submit jobs to remote clusters over SSH, providing a simple starting point for opportunistic resource owners that want to start contributing to a computing grid with minimal effort. If your site intends to run over 10,000 concurrent pilot jobs, you will need to host your own HTCondor-CE because the Hosted CE has not yet been optimized for such loads.","title":"Hosted CE over SSH"},{"location":"overview/#how-the-ce-is-customized","text":"Aside from the basic configuration required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all): Deciding which Virtual Organizations (VOs) are allowed to run at your site: HTCondor-CE leverages HTCondor's built-in ability to authenticate incoming jobs based on their GSI or OAuth token credentials. Additionally, HTCondor may be configured to callout to external authentication services like Argus or LCMAPS. How to filter and transform the pilot jobs to be run on your batch system: Filtering and transforming pilot jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the batch system integration page.","title":"How the CE is Customized"},{"location":"overview/#how-security-works","text":"In the grid, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certificates. When these clients and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust. In its default configuration, HTCondor-CE supports token-based and GSI-based authentication and authorization to the remote submitter's credentials. HTCondor-CE also supports callouts to external authorization technoligies such as LCMAPS or Argus.","title":"How Security Works"},{"location":"overview/#getting-help","text":"If you have any questions about the architecture of HTCondor-CE, please contact us for assistance.","title":"Getting Help"},{"location":"reference/","text":"Reference \u00b6 Configuration \u00b6 The following directories contain the configuration for HTCondor-CE. The directories are parsed in the order presented and thus configuration within the final directory will override configuration specified in the previous directories. Location Comment /usr/share/condor-ce/config.d/ Configuration defaults (overwritten on package updates) /etc/condor-ce/config.d/ Files in this directory are parsed in alphanumeric order (i.e., 99-local.conf will override values in 01-ce-auth.conf ) For a detailed order of the way configuration files are parsed, run the following command: user@host $ condor_ce_config_val -config Users \u00b6 The following users are needed by HTCondor-CE at all sites: User Comment condor The HTCondor-CE will be run as root, but perform most of its operations as the condor user. Certificates \u00b6 File User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem Host key root /grid-security/hostkey.pem Networking \u00b6 Service Name Protocol Port Number Inbound Outbound Comment Htcondor-CE tcp 9619 X HTCondor-CE shared port Allow inbound and outbound network connection to all internal site servers, such as the batch system head-node only ephemeral outgoing ports are necessary.","title":"Reference"},{"location":"reference/#reference","text":"","title":"Reference"},{"location":"reference/#configuration","text":"The following directories contain the configuration for HTCondor-CE. The directories are parsed in the order presented and thus configuration within the final directory will override configuration specified in the previous directories. Location Comment /usr/share/condor-ce/config.d/ Configuration defaults (overwritten on package updates) /etc/condor-ce/config.d/ Files in this directory are parsed in alphanumeric order (i.e., 99-local.conf will override values in 01-ce-auth.conf ) For a detailed order of the way configuration files are parsed, run the following command: user@host $ condor_ce_config_val -config","title":"Configuration"},{"location":"reference/#users","text":"The following users are needed by HTCondor-CE at all sites: User Comment condor The HTCondor-CE will be run as root, but perform most of its operations as the condor user.","title":"Users"},{"location":"reference/#certificates","text":"File User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem Host key root /grid-security/hostkey.pem","title":"Certificates"},{"location":"reference/#networking","text":"Service Name Protocol Port Number Inbound Outbound Comment Htcondor-CE tcp 9619 X HTCondor-CE shared port Allow inbound and outbound network connection to all internal site servers, such as the batch system head-node only ephemeral outgoing ports are necessary.","title":"Networking"},{"location":"releases/","text":"Releases \u00b6 HTCondor-CE 4 is distributed via RPM and is available from the following Yum repositories: HTCondor development Open Science Grid Updating to HTCondor-CE 4 \u00b6 HTCondor-CE 4 is a major release that adds many features and overhauls the default configuration. As such, upgrades from older versions of HTCondor-CE may require manual intervention: Disabled job retries by default since most jobs submitted through HTCondor-CEs are actually resource requests (i.e. pilot jobs) instead of jobs containing user payloads. Therefore, it's preferred to prevent these jobs from retrying and instead wait for additional resource requests to be submitted. To re-enable job retries, set the following in your configuration: ENABLE_JOB_RETRIES = True Simplified remote CE requirements format: remote CE requirements are a way to specify batch system directives that aren't directly supported in the job router for non-HTCondor batch systems. In the past, specifying these directives were often quite complicated. For example, a snippet from an example job route: set_default_remote_cerequirements = strcat(\"Walltime == 3600 && AccountingGroup ==\"\", x509UserProxyFirstFQAN, \"\\\"\"); The HTCondor 8.8 series allows users to specify the same logic using a simplified format within job routes. The same expression above can be written as the following snippet: set_WallTime = 3600; set_AccountingGroup = x509UserProxyFirstFQAN; set_default_CERequirements = \"Walltime,AccountingGroup\"; Reorganized HTCondor-CE configuration: configuration that admins are expected to change is in /etc/condor-ce/config.d/ , other configuration is in /usr . Watch out for *.rpmnew files and merge changes into your existing configuration, especially /etc/condor-ce/condor_mapfile.rpmnew . OSG domain changes: OSG builds of HTCondor-CE now use the standard htcondor.org domain for mapped principles. For example, UID_DOMAIN is now users.htcondor.org instead of users.opensciencegrid.org . If you've made changes to the default HTCondor-CE security configuration (check with condor_ce_config_val -dump ), you may need to update any configuration to use *.htcondor.org instead of *.opensciencegrid.org . HTCondor-CE 4 Version History \u00b6 This section contains release notes for each version of HTCondor-CE 4. Full HTCondor-CE version history can be found on GitHub . 4.4.1 \u00b6 This release includes the following bug-fixes: Fixed a stacktrace with the BDII provider when HTCONDORCE_SPEC isn't defined in the local HTCondor configuration Fixed a race condition that could result in removed jobs being put on hold Improved performance of the HTCondor-CE View 4.4.0 \u00b6 This release includes the following new features: Add plug-in interface to HTCondor-CE View and separate out OSG-specific code and configuration Add configuration option ( COMPLETED_JOB_EXPIRATION ) for how many days completed jobs may stay in the queue This release also includes the following bug-fixes: Replace APEL uploader SchedD cron with init and systemd services Fix HTCondor-CE View SchedD query that caused \"Info\" tables to be blank 4.3.0 \u00b6 This release includes the following new features: Add the CE registry web application to the Central Collector. The registry provides an interface to OSG site administrators of HTCondor-CEs to retrieve an HTCondor IDTOKEN for authenticating pilot job submissions. Identify broken job routes upon startup Add benchmarking parameters to the BDII provider via HTCONDORCE_SPEC in the configuration. See /etc/condor-ce/config.d/99-ce-bdii.conf This release also includes the following bug-fixes: Fix handling of unmapped GSI users in the Central Collector Fix reference to old BDII configuration values 4.2.1 \u00b6 This release includes the following bug fixes: Drop vestigial Central Collector configuration generator script and service Fix unmapped GSI/SSL regular expressions and allow unmapped entities to advertise to the Central Collector 4.2.0 \u00b6 This release includes the following new features: Add SSL support for reporting to Central Collectors GLUE2 validation improvements for the BDII provider 4.1.0 \u00b6 This release includes the following new features: Added the ability to configure the environment of routed jobs: Administrators may now add or override environment variables for resultant batch system jobs. Simplified APEL configuration : HTCondor-CE provides appropriate default configuration for its APEL scripts so administrators only need to configure their HTCondor worker nodes as well as the APEL parser, client, and SSM. Details can be found in the documentation . This release also includes the following bug-fixes: Fixed the ability to specify grid certificate locations for SSL authentication Refined the APEL record filter to ignore jobs that have not yet started Fixed an issue where condor_ce_q required authentication Re-enabled the ability for local users to submit jobs to the CE queue Fixed an issue where some jobs were capped at 72 minutes instead of 72 hours Improved BDII provider error handling 4.0.1 \u00b6 This release fixes a stacktrace that can occur on condor-ce service startup when the required QUEUE_SUPER_USER_MAY_IMPERSONATE = .* configuration is not set for HTCondor batch systems ( #245 ). 4.0.0 \u00b6 This release includes the following new features: SciTokens support if using an HTCondor version that supports SciTokens (e.g. the OSG-distributed HTCondor 8.9.2). Disabled job retries by default since most jobs submitted through HTCondor-CEs are actually resource requests (i.e. pilot jobs) instead of jobs containing user payloads. Therefore, it's preferred to prevent these jobs from retrying and instead wait for additional resource requests to be submitted. To re-enable job retries, set the following in your configuration: ENABLE_JOB_RETRIES = True Simplified daemon authentication: HTCondor-CE now uses FS authentication between its own daemons instead of GSI. Simplified remote CE requirements format: Remote CE requirements are a way to specify batch system directives that aren't directly supported in the job router for non-HTCondor batch systems. In the past, specifying these directives were often quite complicated. For example, a snippet from an example job route: set_default_remote_cerequirements = strcat(\"Walltime == 3600 && AccountingGroup ==\"\", x509UserProxyFirstFQAN, \"\\\"\"); The HTCondor 8.8 series allows users to specify the same logic using a simplified format within job routes. The same expression above can be written as the following snippet: set_WallTime = 3600; set_AccountingGroup = x509UserProxyFirstFQAN; set_default_CERequirements = \"Walltime,AccountingGroup\"; Reorganized HTCondor-CE configuration: configuration that admins are expected to change is in /etc/condor-ce/config.d/ , other configuration is in /usr . Watch out for *.rpmnew files, particularly for /etc/condor-ce/condor_mapfile.rpmnew and /etc/condor-ce/config.d/*.conf.rpmnew , and merge changes into your existing configuration. OSG domain changes: OSG builds of HTCondor-CE now use the standard htcondor.org domain for mapped principles. For example, UID_DOMAIN is now users.htcondor.org instead of users.opensciencegrid.org . If you've made changes to the default HTCondor-CE configuration, you may need to update any configuration to use *.htcondor.org . Moved most OSG-specific configuration into the OSG CE metapackage ( SOFTWARE-3813 ) Increased the default maximum walltime to 72 hours Getting Help \u00b6 If you have any questions about the release process or run into issues with an upgrade, please contact us for assistance.","title":"Releases"},{"location":"releases/#releases","text":"HTCondor-CE 4 is distributed via RPM and is available from the following Yum repositories: HTCondor development Open Science Grid","title":"Releases"},{"location":"releases/#updating-to-htcondor-ce-4","text":"HTCondor-CE 4 is a major release that adds many features and overhauls the default configuration. As such, upgrades from older versions of HTCondor-CE may require manual intervention: Disabled job retries by default since most jobs submitted through HTCondor-CEs are actually resource requests (i.e. pilot jobs) instead of jobs containing user payloads. Therefore, it's preferred to prevent these jobs from retrying and instead wait for additional resource requests to be submitted. To re-enable job retries, set the following in your configuration: ENABLE_JOB_RETRIES = True Simplified remote CE requirements format: remote CE requirements are a way to specify batch system directives that aren't directly supported in the job router for non-HTCondor batch systems. In the past, specifying these directives were often quite complicated. For example, a snippet from an example job route: set_default_remote_cerequirements = strcat(\"Walltime == 3600 && AccountingGroup ==\"\", x509UserProxyFirstFQAN, \"\\\"\"); The HTCondor 8.8 series allows users to specify the same logic using a simplified format within job routes. The same expression above can be written as the following snippet: set_WallTime = 3600; set_AccountingGroup = x509UserProxyFirstFQAN; set_default_CERequirements = \"Walltime,AccountingGroup\"; Reorganized HTCondor-CE configuration: configuration that admins are expected to change is in /etc/condor-ce/config.d/ , other configuration is in /usr . Watch out for *.rpmnew files and merge changes into your existing configuration, especially /etc/condor-ce/condor_mapfile.rpmnew . OSG domain changes: OSG builds of HTCondor-CE now use the standard htcondor.org domain for mapped principles. For example, UID_DOMAIN is now users.htcondor.org instead of users.opensciencegrid.org . If you've made changes to the default HTCondor-CE security configuration (check with condor_ce_config_val -dump ), you may need to update any configuration to use *.htcondor.org instead of *.opensciencegrid.org .","title":"Updating to HTCondor-CE 4"},{"location":"releases/#htcondor-ce-4-version-history","text":"This section contains release notes for each version of HTCondor-CE 4. Full HTCondor-CE version history can be found on GitHub .","title":"HTCondor-CE 4 Version History"},{"location":"releases/#441","text":"This release includes the following bug-fixes: Fixed a stacktrace with the BDII provider when HTCONDORCE_SPEC isn't defined in the local HTCondor configuration Fixed a race condition that could result in removed jobs being put on hold Improved performance of the HTCondor-CE View","title":"4.4.1"},{"location":"releases/#440","text":"This release includes the following new features: Add plug-in interface to HTCondor-CE View and separate out OSG-specific code and configuration Add configuration option ( COMPLETED_JOB_EXPIRATION ) for how many days completed jobs may stay in the queue This release also includes the following bug-fixes: Replace APEL uploader SchedD cron with init and systemd services Fix HTCondor-CE View SchedD query that caused \"Info\" tables to be blank","title":"4.4.0"},{"location":"releases/#430","text":"This release includes the following new features: Add the CE registry web application to the Central Collector. The registry provides an interface to OSG site administrators of HTCondor-CEs to retrieve an HTCondor IDTOKEN for authenticating pilot job submissions. Identify broken job routes upon startup Add benchmarking parameters to the BDII provider via HTCONDORCE_SPEC in the configuration. See /etc/condor-ce/config.d/99-ce-bdii.conf This release also includes the following bug-fixes: Fix handling of unmapped GSI users in the Central Collector Fix reference to old BDII configuration values","title":"4.3.0"},{"location":"releases/#421","text":"This release includes the following bug fixes: Drop vestigial Central Collector configuration generator script and service Fix unmapped GSI/SSL regular expressions and allow unmapped entities to advertise to the Central Collector","title":"4.2.1"},{"location":"releases/#420","text":"This release includes the following new features: Add SSL support for reporting to Central Collectors GLUE2 validation improvements for the BDII provider","title":"4.2.0"},{"location":"releases/#410","text":"This release includes the following new features: Added the ability to configure the environment of routed jobs: Administrators may now add or override environment variables for resultant batch system jobs. Simplified APEL configuration : HTCondor-CE provides appropriate default configuration for its APEL scripts so administrators only need to configure their HTCondor worker nodes as well as the APEL parser, client, and SSM. Details can be found in the documentation . This release also includes the following bug-fixes: Fixed the ability to specify grid certificate locations for SSL authentication Refined the APEL record filter to ignore jobs that have not yet started Fixed an issue where condor_ce_q required authentication Re-enabled the ability for local users to submit jobs to the CE queue Fixed an issue where some jobs were capped at 72 minutes instead of 72 hours Improved BDII provider error handling","title":"4.1.0"},{"location":"releases/#401","text":"This release fixes a stacktrace that can occur on condor-ce service startup when the required QUEUE_SUPER_USER_MAY_IMPERSONATE = .* configuration is not set for HTCondor batch systems ( #245 ).","title":"4.0.1"},{"location":"releases/#400","text":"This release includes the following new features: SciTokens support if using an HTCondor version that supports SciTokens (e.g. the OSG-distributed HTCondor 8.9.2). Disabled job retries by default since most jobs submitted through HTCondor-CEs are actually resource requests (i.e. pilot jobs) instead of jobs containing user payloads. Therefore, it's preferred to prevent these jobs from retrying and instead wait for additional resource requests to be submitted. To re-enable job retries, set the following in your configuration: ENABLE_JOB_RETRIES = True Simplified daemon authentication: HTCondor-CE now uses FS authentication between its own daemons instead of GSI. Simplified remote CE requirements format: Remote CE requirements are a way to specify batch system directives that aren't directly supported in the job router for non-HTCondor batch systems. In the past, specifying these directives were often quite complicated. For example, a snippet from an example job route: set_default_remote_cerequirements = strcat(\"Walltime == 3600 && AccountingGroup ==\"\", x509UserProxyFirstFQAN, \"\\\"\"); The HTCondor 8.8 series allows users to specify the same logic using a simplified format within job routes. The same expression above can be written as the following snippet: set_WallTime = 3600; set_AccountingGroup = x509UserProxyFirstFQAN; set_default_CERequirements = \"Walltime,AccountingGroup\"; Reorganized HTCondor-CE configuration: configuration that admins are expected to change is in /etc/condor-ce/config.d/ , other configuration is in /usr . Watch out for *.rpmnew files, particularly for /etc/condor-ce/condor_mapfile.rpmnew and /etc/condor-ce/config.d/*.conf.rpmnew , and merge changes into your existing configuration. OSG domain changes: OSG builds of HTCondor-CE now use the standard htcondor.org domain for mapped principles. For example, UID_DOMAIN is now users.htcondor.org instead of users.opensciencegrid.org . If you've made changes to the default HTCondor-CE configuration, you may need to update any configuration to use *.htcondor.org . Moved most OSG-specific configuration into the OSG CE metapackage ( SOFTWARE-3813 ) Increased the default maximum walltime to 72 hours","title":"4.0.0"},{"location":"releases/#getting-help","text":"If you have any questions about the release process or run into issues with an upgrade, please contact us for assistance.","title":"Getting Help"},{"location":"remote-job-submission/","text":"Submitting Jobs Remotely to an HTCondor-CE \u00b6 This document outlines how to submit jobs to an HTCondor-CE from a remote client using two different methods: With dedicated tools for quickly verifying end-to-end job submission, and From an existing HTCondor submit host, useful for developing pilot submission infrastructure If you are the administrator of an HTCondor-CE, consider verifying your HTCondor-CE using the administrator-focused documentation . Before Starting \u00b6 Before attempting to submit jobs to an HTCondor-CE as documented below, ensure the following: The HTCondor-CE administrator has independently verified their HTCondor-CE The HTCondor-CE administrator has added your credential information (e.g. SciToken or grid proxy) to the HTCondor-CE authentication configuration Your credentials are valid and unexpired Submission with Debugging Tools \u00b6 The HTCondor-CE client contains debugging tools designed to quickly test an HTCondor-CE. To use these tools, install the RPM package from the relevant Yum repository : root@host # yum install htcondor-ce-client Verify end-to-end submission \u00b6 The HTCondor-CE client package includes a debugging tool that perform tests of end-to-end job submission called condor_ce_trace . To submit a diagnostic job with condor_ce_trace , run the following command: user@host $ condor_ce_trace --debug <CE HOST> Replacing <CE HOST> with the hostname of the CE you wish to test. On success, you will see Job status: Completed and the job's environment on the worker node where it ran. If you do not see the expected output, refer to the troubleshooting guide . CONDOR_CE_TRACE_ATTEMPTS For a busy site cluster, it may take longer than the default 5 minutes to test end-to-end submission. To extend the length of time that condor_ce_trace waits for the job to complete, prepend the command with _condor_CONDOR_CE_TRACE_ATTEMPTS=<TIME IN SECONDS> . (Optional) Requesting resources \u00b6 condor_ce_trace doesn't make any specific resource requests so its jobs are only given the default resources as configured by the HTCondor-CE you are debugging. To request specific resources (or other job attributes), you can specify the --attribute option on the command line: user@host $ condor_ce_trace --debug \\ --attribute = '+resource1=value1' ... \\ --attribute = '+resourceN=valueN' \\ ce.htcondor.org For example, the following command submits a test job requesting 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command: user@host $ condor_ce_trace --debug \\ --attribute = '+xcount=4' \\ --attribute = '+maxMemory=4000' \\ --attribute = '+maxWallTime=120' \\ --attribute = '+remote_queue=osg' \\ ce.htcondor.org For a list of other attributes that can be set with the --attribute option, consult the submit file commands section. Note Non-HTCondor batch systems may need additional HTCondor-CE configuration to support these job attributes. See the batch system integration for details on how to support them. Submission with HTCondor Submit \u00b6 If you need to submit more complicated jobs than a trace job as described above (e.g. for developing piilot job submission infrastructures) and have access to an HTCondor submit host, you can use standard HTCondor submission tools. Submit the job \u00b6 To submit jobs to a remote HTCondor-CE (or any other externally facing HTCondor SchedD) from an HTCondor submit host, you need to construct an HTCondor submit file describing an HTCondor-C job : Write a submit file, ce_test.sub : # Required for remote HTCondor-CE submission universe = grid use_x509userproxy = true grid_resource = condor ce.htcondor.org ce.htcondor.org:9619 # Files executable = ce_test.sh output = ce_test.out error = ce_test.err log = ce_test.log # File transfer behavior ShouldTransferFiles = YES WhenToTransferOutput = ON_EXIT # Optional resource requests #+xcount = 4 # Request 4 cores #+maxMemory = 4000 # Request 4GB of RAM #+maxWallTime = 120 # Request 2 hrs of wall clock time #+remote_queue = \"osg\" # Request the OSG queue # Submit a single job queue Replacing ce_test.sh with the path to the executable you wish to run and ce.htcondor.org with the hostname of the CE you wish to test. Note The grid_resource line should start with condor and is not related to which batch system you are using. Submit the job: user@host $ condor_submit ce_test.sub Tracking job progress \u00b6 You can track job progress by by querying the local queue: user@host $ condor__q As well as the remote HTCondor-CE queue: user@host $ condor__q -name <CE HOST> -pool <CE HOST>:9619 Replacing <CE HOST> with the FQDN of the HTCondor-CE. For reference, condor_q -help status will provide details of job status codes. user@host $ condor_q -help status | tail JobStatus codes: 1 I IDLE 2 R RUNNING 3 X REMOVED 4 C COMPLETED 5 H HELD 6 > TRANSFERRING_OUTPUT 7 S SUSPENDED Troubleshooting \u00b6 All interactions between condor_submit and the HTCondor-CE will be recorded in the file specified by the log command in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion: 000 (786.000.000) 12/09 16:49:55 Job submitted from host: <131.225.154.68:53134> ... 027 (786.000.000) 12/09 16:50:09 Job submitted to grid resource GridResource: condor ce.htcondor.org ce.htcondor.org:9619 GridJobId: condor ce.htcondor.org ce.htcondor.org:9619 796.0 ... 005 (786.000.000) 12/09 16:52:19 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 0 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 0 - Total Bytes Received By Job If there are issues contacting the HTCondor-CE, you will see error messages about a Down Globus Resource : 020 (788.000.000) 12/09 16:56:17 Detected Down Globus Resource RM-Contact: ce.htcondor.org ... 026 (788.000.000) 12/09 16:56:17 Detected Down Grid Resource GridResource: condor ce.htcondor.org ce.htcondor.org:9619 This indicates a communication issue with the HTCondor-CE that may be diagnosed with condor_ce_ping . Submit File Commands \u00b6 The following table is a reference of commands that are commonly included in HTCondor submit files used for HTCondor-CE resource allocation requests. A more comprehensive list of submit file commands specific to HTCondor can be found in the HTCondor manual . HTCondor string values If you are setting an attribute to a string value, make sure enclose the string in double-quotes ( \" ) Command Description arguments Arguments that will be provided to the executable for the resource allocation request. error Path to the file on the client host that stores stderr from the resource allocation request. executable Path to the file on the client host that the resource allocation request will execute. input Path to the file on the client host that stores input to be piped into the stdin of the resource allocation request. +maxMemory The amount of memory in MB that you wish to allocate to the resource allocation request. +maxWallTime The maximum walltime (in minutes) the resource allocation request is allowed to run before it is removed. output Path to the file on the client host that stores stdout from the resource allocation request. +remote_queue Assign resource allocation request to the target queue in the scheduler. transfer_input_files A comma-delimited list of all the files and directories to be transferred into the working directory for the resource allocation request, before the resource allocation request is started. transfer_output_files A comma-delimited list of all the files and directories to be transferred back to the client, after the resource allocation request completes. +WantWholeNode When set to True , request entire node for the resource allocation request (HTCondor batch systems only) +xcount The number of cores to allocate for the resource allocation request. Getting Help \u00b6 If you have any questions or issues with job submission, please contact us for assistance.","title":"Submit Jobs Remotely"},{"location":"remote-job-submission/#submitting-jobs-remotely-to-an-htcondor-ce","text":"This document outlines how to submit jobs to an HTCondor-CE from a remote client using two different methods: With dedicated tools for quickly verifying end-to-end job submission, and From an existing HTCondor submit host, useful for developing pilot submission infrastructure If you are the administrator of an HTCondor-CE, consider verifying your HTCondor-CE using the administrator-focused documentation .","title":"Submitting Jobs Remotely to an HTCondor-CE"},{"location":"remote-job-submission/#before-starting","text":"Before attempting to submit jobs to an HTCondor-CE as documented below, ensure the following: The HTCondor-CE administrator has independently verified their HTCondor-CE The HTCondor-CE administrator has added your credential information (e.g. SciToken or grid proxy) to the HTCondor-CE authentication configuration Your credentials are valid and unexpired","title":"Before Starting"},{"location":"remote-job-submission/#submission-with-debugging-tools","text":"The HTCondor-CE client contains debugging tools designed to quickly test an HTCondor-CE. To use these tools, install the RPM package from the relevant Yum repository : root@host # yum install htcondor-ce-client","title":"Submission with Debugging Tools"},{"location":"remote-job-submission/#verify-end-to-end-submission","text":"The HTCondor-CE client package includes a debugging tool that perform tests of end-to-end job submission called condor_ce_trace . To submit a diagnostic job with condor_ce_trace , run the following command: user@host $ condor_ce_trace --debug <CE HOST> Replacing <CE HOST> with the hostname of the CE you wish to test. On success, you will see Job status: Completed and the job's environment on the worker node where it ran. If you do not see the expected output, refer to the troubleshooting guide . CONDOR_CE_TRACE_ATTEMPTS For a busy site cluster, it may take longer than the default 5 minutes to test end-to-end submission. To extend the length of time that condor_ce_trace waits for the job to complete, prepend the command with _condor_CONDOR_CE_TRACE_ATTEMPTS=<TIME IN SECONDS> .","title":"Verify end-to-end submission"},{"location":"remote-job-submission/#optional-requesting-resources","text":"condor_ce_trace doesn't make any specific resource requests so its jobs are only given the default resources as configured by the HTCondor-CE you are debugging. To request specific resources (or other job attributes), you can specify the --attribute option on the command line: user@host $ condor_ce_trace --debug \\ --attribute = '+resource1=value1' ... \\ --attribute = '+resourceN=valueN' \\ ce.htcondor.org For example, the following command submits a test job requesting 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command: user@host $ condor_ce_trace --debug \\ --attribute = '+xcount=4' \\ --attribute = '+maxMemory=4000' \\ --attribute = '+maxWallTime=120' \\ --attribute = '+remote_queue=osg' \\ ce.htcondor.org For a list of other attributes that can be set with the --attribute option, consult the submit file commands section. Note Non-HTCondor batch systems may need additional HTCondor-CE configuration to support these job attributes. See the batch system integration for details on how to support them.","title":"(Optional) Requesting resources"},{"location":"remote-job-submission/#submission-with-htcondor-submit","text":"If you need to submit more complicated jobs than a trace job as described above (e.g. for developing piilot job submission infrastructures) and have access to an HTCondor submit host, you can use standard HTCondor submission tools.","title":"Submission with HTCondor Submit"},{"location":"remote-job-submission/#submit-the-job","text":"To submit jobs to a remote HTCondor-CE (or any other externally facing HTCondor SchedD) from an HTCondor submit host, you need to construct an HTCondor submit file describing an HTCondor-C job : Write a submit file, ce_test.sub : # Required for remote HTCondor-CE submission universe = grid use_x509userproxy = true grid_resource = condor ce.htcondor.org ce.htcondor.org:9619 # Files executable = ce_test.sh output = ce_test.out error = ce_test.err log = ce_test.log # File transfer behavior ShouldTransferFiles = YES WhenToTransferOutput = ON_EXIT # Optional resource requests #+xcount = 4 # Request 4 cores #+maxMemory = 4000 # Request 4GB of RAM #+maxWallTime = 120 # Request 2 hrs of wall clock time #+remote_queue = \"osg\" # Request the OSG queue # Submit a single job queue Replacing ce_test.sh with the path to the executable you wish to run and ce.htcondor.org with the hostname of the CE you wish to test. Note The grid_resource line should start with condor and is not related to which batch system you are using. Submit the job: user@host $ condor_submit ce_test.sub","title":"Submit the job"},{"location":"remote-job-submission/#tracking-job-progress","text":"You can track job progress by by querying the local queue: user@host $ condor__q As well as the remote HTCondor-CE queue: user@host $ condor__q -name <CE HOST> -pool <CE HOST>:9619 Replacing <CE HOST> with the FQDN of the HTCondor-CE. For reference, condor_q -help status will provide details of job status codes. user@host $ condor_q -help status | tail JobStatus codes: 1 I IDLE 2 R RUNNING 3 X REMOVED 4 C COMPLETED 5 H HELD 6 > TRANSFERRING_OUTPUT 7 S SUSPENDED","title":"Tracking job progress"},{"location":"remote-job-submission/#troubleshooting","text":"All interactions between condor_submit and the HTCondor-CE will be recorded in the file specified by the log command in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion: 000 (786.000.000) 12/09 16:49:55 Job submitted from host: <131.225.154.68:53134> ... 027 (786.000.000) 12/09 16:50:09 Job submitted to grid resource GridResource: condor ce.htcondor.org ce.htcondor.org:9619 GridJobId: condor ce.htcondor.org ce.htcondor.org:9619 796.0 ... 005 (786.000.000) 12/09 16:52:19 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 0 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 0 - Total Bytes Received By Job If there are issues contacting the HTCondor-CE, you will see error messages about a Down Globus Resource : 020 (788.000.000) 12/09 16:56:17 Detected Down Globus Resource RM-Contact: ce.htcondor.org ... 026 (788.000.000) 12/09 16:56:17 Detected Down Grid Resource GridResource: condor ce.htcondor.org ce.htcondor.org:9619 This indicates a communication issue with the HTCondor-CE that may be diagnosed with condor_ce_ping .","title":"Troubleshooting"},{"location":"remote-job-submission/#submit-file-commands","text":"The following table is a reference of commands that are commonly included in HTCondor submit files used for HTCondor-CE resource allocation requests. A more comprehensive list of submit file commands specific to HTCondor can be found in the HTCondor manual . HTCondor string values If you are setting an attribute to a string value, make sure enclose the string in double-quotes ( \" ) Command Description arguments Arguments that will be provided to the executable for the resource allocation request. error Path to the file on the client host that stores stderr from the resource allocation request. executable Path to the file on the client host that the resource allocation request will execute. input Path to the file on the client host that stores input to be piped into the stdin of the resource allocation request. +maxMemory The amount of memory in MB that you wish to allocate to the resource allocation request. +maxWallTime The maximum walltime (in minutes) the resource allocation request is allowed to run before it is removed. output Path to the file on the client host that stores stdout from the resource allocation request. +remote_queue Assign resource allocation request to the target queue in the scheduler. transfer_input_files A comma-delimited list of all the files and directories to be transferred into the working directory for the resource allocation request, before the resource allocation request is started. transfer_output_files A comma-delimited list of all the files and directories to be transferred back to the client, after the resource allocation request completes. +WantWholeNode When set to True , request entire node for the resource allocation request (HTCondor batch systems only) +xcount The number of cores to allocate for the resource allocation request.","title":"Submit File Commands"},{"location":"remote-job-submission/#getting-help","text":"If you have any questions or issues with job submission, please contact us for assistance.","title":"Getting Help"},{"location":"verification/","text":"Verifying an HTCondor-CE \u00b6 To verify that you have a working installation of HTCondor-CE, ensure that all the relevant services are started and enabled then perform the validation steps below. Managing HTCondor-CE services \u00b6 In addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Fetch CRL fetch-crl-boot and fetch-crl-cron Your batch system condor or pbs_server or \u2026 HTCondor-CE condor-ce (Optional) APEL uploader condor-ce-apel and condor-ce-apel.timer Start and enable the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> Validating HTCondor-CE \u00b6 To validate an HTCondor-CE, perform the following steps: Verify that local job submissions complete successfully from the CE host. For example, if you have a Slurm cluster, run sbatch from the CE and verify that it runs and completes with scontrol and sacct . Verify that all the necessary daemons are running with condor_ce_status -any . Verify the CE's network configuration using condor_ce_host_network_check . Verify that jobs can complete successfully using condor_ce_trace . Getting Help \u00b6 If any of the above validation steps fail, consult the troubleshooting guide . If that still doesn't resolve your issue, please contact us for assistance.","title":"Verification"},{"location":"verification/#verifying-an-htcondor-ce","text":"To verify that you have a working installation of HTCondor-CE, ensure that all the relevant services are started and enabled then perform the validation steps below.","title":"Verifying an HTCondor-CE"},{"location":"verification/#managing-htcondor-ce-services","text":"In addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Fetch CRL fetch-crl-boot and fetch-crl-cron Your batch system condor or pbs_server or \u2026 HTCondor-CE condor-ce (Optional) APEL uploader condor-ce-apel and condor-ce-apel.timer Start and enable the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME>","title":"Managing HTCondor-CE services"},{"location":"verification/#validating-htcondor-ce","text":"To validate an HTCondor-CE, perform the following steps: Verify that local job submissions complete successfully from the CE host. For example, if you have a Slurm cluster, run sbatch from the CE and verify that it runs and completes with scontrol and sacct . Verify that all the necessary daemons are running with condor_ce_status -any . Verify the CE's network configuration using condor_ce_host_network_check . Verify that jobs can complete successfully using condor_ce_trace .","title":"Validating HTCondor-CE"},{"location":"verification/#getting-help","text":"If any of the above validation steps fail, consult the troubleshooting guide . If that still doesn't resolve your issue, please contact us for assistance.","title":"Getting Help"},{"location":"installation/central-collector/","text":"Installing an HTCondor-CE Central Collector \u00b6 The HTCondor-CE Central Collector is an information service designed to provide a an overview and descriptions of grid services. Based on the HTCondorView Server , the Central Collector accepts ClassAds from site HTCondor-CEs by default but may accept from other services using the HTCondor Python Bindings . By distributing configuration to each member site, a central grid team can coordinate the information that site HTCondor-CEs should advertise. Additionally, the the HTCondor-CE View web server may be installed alongside a Central Collector to display pilot job statistics across its grid, as well as information for each site HTCondor-CE. For example, the OSG Central Collector can be viewed at https://collector.opensciencegrid.org . Use this page to learn how to install, configure, and run an HTCondor-CE Central Collector as part of your central operations. Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the reference page as necessary): User IDs: If they do not exist already, the installation will create the condor Linux user (UID 4716) SSL certificate: The HTCondor-CE Central Collector service uses a host certificate and key for SSL and GSI authentication DNS entries: Forward and reverse DNS must resolve for the HTCondor-CE Central Collector host Network ports: Site HTCondor-CEs must be able to contact the Central Collector on port 9619 (TCP). Additionally, the optional HTCondor-CE View web server should be accessible on port 80 (TCP). There are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system (Red Hat Enterprise Linux variant 7) Obtain root access to the host Prepare the EPEL and HTCondor Yum repositories Install CA certificates and VO data into /etc/grid-security/certificates and /etc/grid-security/vomsdir , respectively Installing a Central Collector \u00b6 Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the fetch-crl package, available from the EPEL repositories. root@host # yum install fetch-crl Install the Central Collector software: root@host # yum install htcondor-ce-collector Configuring a Central Collector \u00b6 Like a site HTCondor-CE, the Central Collector uses X.509 host certificates and certificate authorities (CAs) when authenticating SSL and GSI connections. By default, the Central Collector uses the default system locations to locate CAs and host certificate when authenticating SSL connections, i.e. for SSL authentication methods. But traditionally, the Central Collector and HTCondor-CEs have authenticated with each other using specialized grid certificates (e.g. certificates issued by IGTF CAs ) located in /etc/grid-security/ . Choose one of the following options to configure your Central Collector to use grid or system certificates for authentication: If your site HTCondor-CEs will be advertising to your Central Collector using grid certificates or you are using a grid certificate for your Central Collector's host certificate: Set the following configuration in /etc/condor-ce/config.d/01-ce-auth.conf : AUTH_SSL_SERVER_CERTFILE = /etc/grid-security/hostcert.pem AUTH_SSL_SERVER_KEYFILE = /etc/grid-security/hostkey.pem AUTH_SSL_SERVER_CADIR = /etc/grid-security/certificates AUTH_SSL_SERVER_CAFILE = AUTH_SSL_CLIENT_CERTFILE = /etc/grid-security/hostcert.pem AUTH_SSL_CLIENT_KEYFILE = /etc/grid-security/hostkey.pem AUTH_SSL_CLIENT_CADIR = /etc/grid-security/certificates AUTH_SSL_CLIENT_CAFILE = Install your host certificate and key into /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem , respectively Set the ownership and Unix permissions of the host certificate and key root@host # chown root:root /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem root@host # chmod 644 /etc/grid-security/hostcert.pem root@host # chmod 600 /etc/grid-security/hostkey.pem Otherwise, use the default system locations: Install your host certificate and key into /etc/pki/tls/certs/localhost.crt and /etc/pki/tls/private/localhost.key , respectively Set the ownership and Unix permissions of the host certificate and key root@host # chown root:root /etc/pki/tls/certs/localhost.crt /etc/pki/tls/private/localhost.key root@host # chmod 644 /etc/pki/tls/certs/localhost.crt root@host # chmod 600 /etc/pki/tls/private/localhost.key Optional configuration \u00b6 The following configuration steps are optional and will not be required for all Central Collectors. If you do not need any of the following special configurations, skip to the section on next steps . Banning HTCondor-CEs \u00b6 By default, Central Collectors accept ClassAds from all HTCondor-CEs with a valid and accepted certificate. If you want to stop accepting ClassAds from a particular HTCondor-CE, add its hostname to DENY_ADVERTISE_SCHEDD in /etc/condor-ce/config.d/01-ce-collector.conf . For example: DENY_ADVERTISE_SCHEDD = $(DENY_ADVERTISE_SCHEDD), misbehaving-ce-1.bad-domain.com, misbehaving-ce-2.bad-domain.com Configuring HTCondor-CE View \u00b6 The HTCondor-CE View is an optional web interface to the status of all HTCondor-CEs advertising to your Central Collector. To run the HTCondor-CE View, install the appropriate package and set the relevant configuration. Begin by installing the htcondor-ce-view package: root@host # yum install htcondor-ce-view Restart the condor-ce-collector service Verify the service by entering your Central Collector's hostname into your web browser The website is served on port 80 by default. To change this default, edit the value of HTCONDORCE_VIEW_PORT in /etc/condor-ce/config.d/05-ce-view.conf . Distributing Configuration to Site HTCondor-CEs \u00b6 To make the Central Collector truly useful, each site HTCondor-CE in your organization will need to configure their HTCondor-CEs to advertise to your Central Collector(s) along with any custom information that may be of interest. For example, the OSG provides default configuration to OSG sites through an osg-ce metapackage and configuration tools. Following the Filesystem Hierarchy Standard , the following configuration should be set by HTCondor-CE administrators in /etc/condor-ce/config.d/ or by packagers in /usr/share/condor-ce/config.d/ : Set CONDOR_VIEW_HOST to a comma-separated list of Central Collectors: CONDOR_VIEW_HOST = collector.htcondor.org:9619, collector1.htcondor.org:9619, collector2.htcondor.org:9619 Append arbitrary attributes to SCHEDD_ATTRS containing custom information in any number of arbitrarily configuration attributes: ATTR_NAME_1 = value1 ATTR_NAME_2 = value2 SCHEDD_ATTRS = $(SCHEDD_ATTRS) ATTR_NAME_1 ATTR_NAME_2 For example, OSG sites advertise information describing their OSG Topology registrations, local batch system, and local resourcess: OSG_Resource = \"local\" OSG_ResourceGroup = \"\" OSG_BatchSystems = \"condor\" OSG_ResourceCatalog = { \\ [ \\ AllowedVOs = { \"osg\" }; \\ CPUs = 2; \\ MaxWallTime = 1440; \\ Memory = 10000; \\ Name = \"test\"; \\ Requirements = TARGET.RequestCPUs <= CPUs && TARGET.RequestMemory <= Memory && member(TARGET.VO, AllowedVOs); \\ Transform = [ set_MaxMemory = RequestMemory; set_xcount = RequestCPUs; ]; \\ ] \\ } SCHEDD_ATTRS = $(SCHEDD_ATTRS) OSG_Resource OSG_ResourceGroup OSG_BatchSystems OSG_ResourceCatalog Verifying a Central Collector \u00b6 To verify that you have a working installation of a Central Collector, ensure that all the relevant services are started and enabled then perform the validation steps below. Managing Central Collector services \u00b6 In addition to the Central Collector service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Fetch CRL fetch-crl-boot and fetch-crl-cron HTCondor-CE condor-ce-collector Start and enable the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME> Validating a Central Collector \u00b6 Getting Help \u00b6 If you have any questions or issues with the installation process, please contact us for assistance.","title":"Install a Central Collector"},{"location":"installation/central-collector/#installing-an-htcondor-ce-central-collector","text":"The HTCondor-CE Central Collector is an information service designed to provide a an overview and descriptions of grid services. Based on the HTCondorView Server , the Central Collector accepts ClassAds from site HTCondor-CEs by default but may accept from other services using the HTCondor Python Bindings . By distributing configuration to each member site, a central grid team can coordinate the information that site HTCondor-CEs should advertise. Additionally, the the HTCondor-CE View web server may be installed alongside a Central Collector to display pilot job statistics across its grid, as well as information for each site HTCondor-CE. For example, the OSG Central Collector can be viewed at https://collector.opensciencegrid.org . Use this page to learn how to install, configure, and run an HTCondor-CE Central Collector as part of your central operations.","title":"Installing an HTCondor-CE Central Collector"},{"location":"installation/central-collector/#before-starting","text":"Before starting the installation process, consider the following points (consulting the reference page as necessary): User IDs: If they do not exist already, the installation will create the condor Linux user (UID 4716) SSL certificate: The HTCondor-CE Central Collector service uses a host certificate and key for SSL and GSI authentication DNS entries: Forward and reverse DNS must resolve for the HTCondor-CE Central Collector host Network ports: Site HTCondor-CEs must be able to contact the Central Collector on port 9619 (TCP). Additionally, the optional HTCondor-CE View web server should be accessible on port 80 (TCP). There are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system (Red Hat Enterprise Linux variant 7) Obtain root access to the host Prepare the EPEL and HTCondor Yum repositories Install CA certificates and VO data into /etc/grid-security/certificates and /etc/grid-security/vomsdir , respectively","title":"Before Starting"},{"location":"installation/central-collector/#installing-a-central-collector","text":"Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the fetch-crl package, available from the EPEL repositories. root@host # yum install fetch-crl Install the Central Collector software: root@host # yum install htcondor-ce-collector","title":"Installing a Central Collector"},{"location":"installation/central-collector/#configuring-a-central-collector","text":"Like a site HTCondor-CE, the Central Collector uses X.509 host certificates and certificate authorities (CAs) when authenticating SSL and GSI connections. By default, the Central Collector uses the default system locations to locate CAs and host certificate when authenticating SSL connections, i.e. for SSL authentication methods. But traditionally, the Central Collector and HTCondor-CEs have authenticated with each other using specialized grid certificates (e.g. certificates issued by IGTF CAs ) located in /etc/grid-security/ . Choose one of the following options to configure your Central Collector to use grid or system certificates for authentication: If your site HTCondor-CEs will be advertising to your Central Collector using grid certificates or you are using a grid certificate for your Central Collector's host certificate: Set the following configuration in /etc/condor-ce/config.d/01-ce-auth.conf : AUTH_SSL_SERVER_CERTFILE = /etc/grid-security/hostcert.pem AUTH_SSL_SERVER_KEYFILE = /etc/grid-security/hostkey.pem AUTH_SSL_SERVER_CADIR = /etc/grid-security/certificates AUTH_SSL_SERVER_CAFILE = AUTH_SSL_CLIENT_CERTFILE = /etc/grid-security/hostcert.pem AUTH_SSL_CLIENT_KEYFILE = /etc/grid-security/hostkey.pem AUTH_SSL_CLIENT_CADIR = /etc/grid-security/certificates AUTH_SSL_CLIENT_CAFILE = Install your host certificate and key into /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem , respectively Set the ownership and Unix permissions of the host certificate and key root@host # chown root:root /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem root@host # chmod 644 /etc/grid-security/hostcert.pem root@host # chmod 600 /etc/grid-security/hostkey.pem Otherwise, use the default system locations: Install your host certificate and key into /etc/pki/tls/certs/localhost.crt and /etc/pki/tls/private/localhost.key , respectively Set the ownership and Unix permissions of the host certificate and key root@host # chown root:root /etc/pki/tls/certs/localhost.crt /etc/pki/tls/private/localhost.key root@host # chmod 644 /etc/pki/tls/certs/localhost.crt root@host # chmod 600 /etc/pki/tls/private/localhost.key","title":"Configuring a Central Collector"},{"location":"installation/central-collector/#optional-configuration","text":"The following configuration steps are optional and will not be required for all Central Collectors. If you do not need any of the following special configurations, skip to the section on next steps .","title":"Optional configuration"},{"location":"installation/central-collector/#banning-htcondor-ces","text":"By default, Central Collectors accept ClassAds from all HTCondor-CEs with a valid and accepted certificate. If you want to stop accepting ClassAds from a particular HTCondor-CE, add its hostname to DENY_ADVERTISE_SCHEDD in /etc/condor-ce/config.d/01-ce-collector.conf . For example: DENY_ADVERTISE_SCHEDD = $(DENY_ADVERTISE_SCHEDD), misbehaving-ce-1.bad-domain.com, misbehaving-ce-2.bad-domain.com","title":"Banning HTCondor-CEs"},{"location":"installation/central-collector/#configuring-htcondor-ce-view","text":"The HTCondor-CE View is an optional web interface to the status of all HTCondor-CEs advertising to your Central Collector. To run the HTCondor-CE View, install the appropriate package and set the relevant configuration. Begin by installing the htcondor-ce-view package: root@host # yum install htcondor-ce-view Restart the condor-ce-collector service Verify the service by entering your Central Collector's hostname into your web browser The website is served on port 80 by default. To change this default, edit the value of HTCONDORCE_VIEW_PORT in /etc/condor-ce/config.d/05-ce-view.conf .","title":"Configuring HTCondor-CE View"},{"location":"installation/central-collector/#distributing-configuration-to-site-htcondor-ces","text":"To make the Central Collector truly useful, each site HTCondor-CE in your organization will need to configure their HTCondor-CEs to advertise to your Central Collector(s) along with any custom information that may be of interest. For example, the OSG provides default configuration to OSG sites through an osg-ce metapackage and configuration tools. Following the Filesystem Hierarchy Standard , the following configuration should be set by HTCondor-CE administrators in /etc/condor-ce/config.d/ or by packagers in /usr/share/condor-ce/config.d/ : Set CONDOR_VIEW_HOST to a comma-separated list of Central Collectors: CONDOR_VIEW_HOST = collector.htcondor.org:9619, collector1.htcondor.org:9619, collector2.htcondor.org:9619 Append arbitrary attributes to SCHEDD_ATTRS containing custom information in any number of arbitrarily configuration attributes: ATTR_NAME_1 = value1 ATTR_NAME_2 = value2 SCHEDD_ATTRS = $(SCHEDD_ATTRS) ATTR_NAME_1 ATTR_NAME_2 For example, OSG sites advertise information describing their OSG Topology registrations, local batch system, and local resourcess: OSG_Resource = \"local\" OSG_ResourceGroup = \"\" OSG_BatchSystems = \"condor\" OSG_ResourceCatalog = { \\ [ \\ AllowedVOs = { \"osg\" }; \\ CPUs = 2; \\ MaxWallTime = 1440; \\ Memory = 10000; \\ Name = \"test\"; \\ Requirements = TARGET.RequestCPUs <= CPUs && TARGET.RequestMemory <= Memory && member(TARGET.VO, AllowedVOs); \\ Transform = [ set_MaxMemory = RequestMemory; set_xcount = RequestCPUs; ]; \\ ] \\ } SCHEDD_ATTRS = $(SCHEDD_ATTRS) OSG_Resource OSG_ResourceGroup OSG_BatchSystems OSG_ResourceCatalog","title":"Distributing Configuration to Site HTCondor-CEs"},{"location":"installation/central-collector/#verifying-a-central-collector","text":"To verify that you have a working installation of a Central Collector, ensure that all the relevant services are started and enabled then perform the validation steps below.","title":"Verifying a Central Collector"},{"location":"installation/central-collector/#managing-central-collector-services","text":"In addition to the Central Collector service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Fetch CRL fetch-crl-boot and fetch-crl-cron HTCondor-CE condor-ce-collector Start and enable the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... On EL7, run the command... Start a service systemctl start <SERVICE-NAME> Stop a service systemctl stop <SERVICE-NAME> Enable a service to start on boot systemctl enable <SERVICE-NAME> Disable a service from starting on boot systemctl disable <SERVICE-NAME>","title":"Managing Central Collector services"},{"location":"installation/central-collector/#validating-a-central-collector","text":"","title":"Validating a Central Collector"},{"location":"installation/central-collector/#getting-help","text":"If you have any questions or issues with the installation process, please contact us for assistance.","title":"Getting Help"},{"location":"installation/hosted-ce/","text":"Installing a Hosted CE \u00b6","title":"Hosted ce"},{"location":"installation/hosted-ce/#installing-a-hosted-ce","text":"","title":"Installing a Hosted CE"},{"location":"installation/htcondor-ce/","text":"Installing an HTCondor-CE \u00b6 Note If you are installing an HTCondor-CE for the Open Science Grid (OSG), consult the OSG-specific documentation . HTCondor-CE is a special configuration of the HTCondor software designed as a Compute Entrypoint solution for computing grids (e.g. European Grid Infrastructure , Open Science Grid ). It is configured to use the Job Router daemon to delegate resource allocation requests by transforming and submitting them to the site\u2019s batch system. See the overview page for more details on the features and architecture of HTCondor-CE. Use this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the HTCondor Yum repositories . Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the reference page as necessary): User IDs: If they do not exist already, the installation will create the condor Linux user (UID 4716) SSL certificate: The HTCondor-CE service uses a host certificate and key for SSL and GSI authentication DNS entries: Forward and reverse DNS must resolve for the HTCondor-CE host Network ports: The pilot factories must be able to contact your HTCondor-CE service on port 9619 (TCP) Submit host: HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster running supported batch system software (Grid Engine, HTCondor, LSF, PBS/Torque, Slurm) File Systems : Non-HTCondor batch systems require a shared file system between the HTCondor-CE host and the batch system worker nodes. There are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system (Red Hat Enterprise Linux variant 7) Obtain root access to the host Prepare the EPEL and HTCondor Development Yum repositories Install CA certificates and VO data into /etc/grid-security/certificates and /etc/grid-security/vomsdir , respectively Installing HTCondor-CE \u00b6 Important HTCondor-CE must be installed on a host that is configured to submit jobs to your batch system. The details of this setup is site-specific by nature and therefore beyond the scope of this document. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the fetch-crl package, available from the EPEL repositories. root@host # yum install fetch-crl Select the appropriate convenience RPM: If your batch system is... Then use the following package... Grid Engine htcondor-ce-sge HTCondor htcondor-ce-condor LSF htcondor-ce-lsf PBS/Torque htcondor-ce-pbs SLURM htcondor-ce-slurm Install the CE software: root@host # yum install <PACKAGE> Where <PACKAGE> is the package you selected in the above step. Configuring HTCondor-CE \u00b6 There are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on optional configurations . Configuring authentication \u00b6 To authenticate job submission from external users and VOs, HTCondor-CE can be configured to use a built-in mapfile or to make Globus callouts to an external service like Argus or LCMAPS. The former option is simpler but the latter option may be preferred if your grid supports it or your site already runs such a service. Additionally, the HTCondor-CE service uses X.509 certificates for SSL and GSI authentication. Built-in mapfile \u00b6 The built-in mapfile is a unified HTCondor mapfile located at /etc/condor-ce/condor_mapfile . This file is parsed in line-by-line order and HTCondor-CE will use the first line that matches. Therefore, mappings should be added to the top of the file. Warning condor_mapfile.rpmnew files may be generated upon HTCondor-CE version updates and they should be merged into condor_mapfile . To configure authorization for users submitting jobs with an X.509 proxy certificate to your HTCondor-CE, add lines of the following format: GSI \"^<DISTINGUISHED NAME>$\" <USERNAME> Replacing <DISTINGUISHED NAME > (escaping any '/' with '\\/') and <USERNAME > with the distinguished name of the incoming certificate and the unix account under which the job should run, respectively. VOMS attributes of incoming X.509 proxy certificates can also be used for mapping: GSI \"<DISTINGUISHED NAME>,<VOMS FQAN 1>,<VOMS FQAN 2>,...,<VOMSFQAN N>\" <USERNAME> Replacing <DISTINGUISHED NAME > (escaping any '/' with '\\/'), <VOMSFQAN > fields, and <USERNAME > with the distinguished name of the incoming certificate, the VOMS roles and groups, and the unix account under which the job should run, respectively. Additionally, you can use regular expressions for mapping certificate and VOMS attribute credentials. For example, to map any certificate from the GLOW VO with the htpc role to the glow user, add the following line: GSI \".*,\\/GLOW\\/Role=htpc.*\" glow Warning You should only add mappings to the mapfile. Do not remove any of the default mappings: GSI \"(/CN=[-.A-Za-z0-9/= ]+)\" \\1@unmapped.htcondor.org CLAIMTOBE .* anonymous@claimtobe FS (.*) \\1 Globus callout \u00b6 To use a Globus callout to a service like LCMAPS or Argus, you will need to have the relevant library installed as well as the following HTCondor-CE configuration: Add the following line to the top of /etc/condor-ce/condor_mapfile : GSI (.*) GSS_ASSIST_GRIDMAP Create /etc/grid-security/gsi-authz.conf with the following content: For LCMAPS: globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout For Argus: globus_mapping /usr/lib64/libgsi_pep_callout.so argus_pep_callout Configuring certificates \u00b6 HTCondor-CE uses X.509 host certificates and certificate authorities (CAs) when authenticating SSL and GSI connections. By default, HTCondor-CE uses the default system locations to locate CAs and host certificate when authenticating SSL connections, i.e. for SciTokens or SSL authentication methods. But traditionally, CEs and their clients have authenticated with each other using specialized grid certificates (e.g. certificates issued by IGTF CAs ) located in /etc/grid-security/ . Choose one of the following options to configure your HTCondor-CE to use grid or system certificates for authentication: If your SSL or SciTokens clients will be interacting with your CE using grid certificates or you are using a grid certificate as your host certificate: Set the following configuration in /etc/condor-ce/config.d/01-ce-auth.conf : AUTH_SSL_SERVER_CERTFILE = /etc/grid-security/hostcert.pem AUTH_SSL_SERVER_KEYFILE = /etc/grid-security/hostkey.pem AUTH_SSL_SERVER_CADIR = /etc/grid-security/certificates AUTH_SSL_SERVER_CAFILE = AUTH_SSL_CLIENT_CERTFILE = /etc/grid-security/hostcert.pem AUTH_SSL_CLIENT_KEYFILE = /etc/grid-security/hostkey.pem AUTH_SSL_CLIENT_CADIR = /etc/grid-security/certificates AUTH_SSL_CLIENT_CAFILE = Install your host certificate and key into /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem , respectively Set the ownership and Unix permissions of the host certificate and key root@host # chown root:root /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem root@host # chmod 644 /etc/grid-security/hostcert.pem root@host # chmod 600 /etc/grid-security/hostkey.pem Otherwise, use the default system locations: Install your host certificate and key into /etc/pki/tls/certs/localhost.crt and /etc/pki/tls/private/localhost.key , respectively Set the ownership and Unix permissions of the host certificate and key root@host # chown root:root /etc/pki/tls/certs/localhost.crt /etc/pki/tls/private/localhost.key root@host # chmod 644 /etc/pki/tls/certs/localhost.crt root@host # chmod 600 /etc/pki/tls/private/localhost.key Configuring the batch system \u00b6 Before HTCondor-CE can submit jobs to your local batch system, it has to be configured to do so. The configuration will differ depending on if your local batch system is HTCondor or one of the other supported batch systems. Choose the section corresponding to your batch system below. HTCondor batch systems \u00b6 To configure HTCondor-CE for an HTCondor batch system, set JOB_ROUTER_SCHEDD2_POOL to your site's central manager host and port: JOB_ROUTER_SCHEDD2_POOL = cm.chtc.wisc.edu:9618 Additionally, set JOB_ROUTER_SCHEDD2_SPOOL to the location of the local batch SPOOL directory on the CE host if it is different than the default location ( /var/lib/condor/spool ). Non-HTCondor batch systems \u00b6 Configuring the BLAHP \u00b6 HTCondor-CE uses the Batch Language ASCII Helper Protocol (BLAHP) to submit and track jobs to non-HTCondor batch systems. To work with the HTCondor-CE, modify /usr/libexec/condor/glite/etc/batch_gahp.config using the following steps: Disable BLAHP handling of certificate proxies: blah_disable_wn_proxy_renewal=yes blah_delegate_renewed_proxies=no blah_disable_limited_proxy=yes (Optional) If your batch system tools are installed in a non-standard location (i.e., outside of /usr/bin/ ), set the corresponding *_binpath variable to the directory containing your batch system tools: If your batch system is... Then change the following configuration variable... LSF lsf_binpath PBS/Torque pbs_binpath SGE sge_binpath Slurm slurm_binpath For example, if your Slurm binaries (e.g. sbatch ) exist in /opt/slurm/bin , you would set the following: slurm_binpath=/opt/slurm/bin/ Sharing the SPOOL directory \u00b6 Non-HTCondor batch systems require a shared file system configuration to support file transfer from the HTCondor-CE to your site's worker nodes. The current recommendation is to run a dedicated NFS server on the CE host . In this setup, HTCondor-CE writes to the local spool directory, the NFS server shares the directory, and each worker node mounts the directory in the same location as on the CE. For example, if your spool directory is /var/lib/condor-ce (the default), you must mount the shared directory to /var/lib/condor-ce on the worker nodes. Note If you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE daemons can write to the spool directory. You can control the value of the spool directory by setting SPOOL in /etc/condor-ce/config.d/99-local.conf (create this file if it doesn't exist). For example, the following sets the SPOOL directory to /home/condor : SPOOL = /home/condor Note The shared spool directory must be readable and writeable by the condor user for HTCondor-CE to function correctly. Optional Configuration \u00b6 The following configuration steps are optional and will not be required for all sites. If you do not need any of the following special configurations, skip to the section on next steps . Configuring for multiple network interfaces \u00b6 If you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname and interface to use when communicating with each other. Set NETWORK_HOSTNAME and NETWORK_INTERFACE to the hostname and IP address of your public interface, respectively, in /etc/condor-ce/config.d/99-local.conf directory with the line: NETWORK_HOSTNAME = condorce.example.com NETWORK_INTERFACE = 127.0.0.1 Replacing condorce.example.com text with your public interface\u2019s hostname and 127.0.0.1 with your public interface\u2019s IP address. Limiting or disabling locally running jobs on the CE \u00b6 If you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and scheduler universes. Local and scheduler universes allow jobs to be run on the CE itself, mainly for remote troubleshooting. Pilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another worker node. The two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be configuring them in unison. To change the default limit on the number of locally run jobs (the current default is 20), add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning < <JOB-LIMIT> START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Where <JOB-LIMIT> is the maximum number of jobs allowed to run locally To only allow a specific user to start locally run jobs, add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = target.Owner =?= \"<USERNAME>\" START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Change <USERNAME> for the username allowed to run jobs locally To disable locally run jobs, add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = False START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Enabling the monitoring web interface \u00b6 The HTCondor-CE View is an optional web interface to the status of your CE. To run the HTCondor-CE View, install the appropriate package and set the relevant configuration. Begin by installing the htcondor-ce-view package: root@host # yum install htcondor-ce-view Restart the condor-ce service Verify the service by entering your CE's hostname into your web browser The website is served on port 80 by default. To change this default, edit the value of HTCONDORCE_VIEW_PORT in /etc/condor-ce/config.d/05-ce-view.conf . Uploading accounting records to APEL \u00b6 Batch System Support HTCondor-CE only supports generation of APEL accounting records for HTCondor batch systems. For sites outside of the OSG that need to upload the APEL accounting records, HTCondor-CE supports uploading batch and blah APEL records for HTCondor batch systems: Install the HTCondor-CE APEL package on your CE host: root@host # yum install htcondor-ce-apel On each worker node, set the appropriate scaling factor in the HTCondor configuration (i.e. /etc/condor/config.d/ ) and advertise it in the startd ad: ApelScaling = <SCALING FACTOR> # For example, 1.062 STARTD_ATTRS = $(STARTD_ATTRS) ApelScaling Configure the APEL parser, client, and SSM Records are written to APEL_OUTPUT_DIR in the HTCondor-CE configuration (default: /var/lib/condor-ce/apel/ ) Batch and blah record filenames are prefixed batch- and blah- , respectively Start and enable the condor-ce-apel and condor-ce-apel.timer services Enabling BDII integration \u00b6 Batch System Support HTCondor-CE only supports reporting BDII information for HTCondor batch systems. HTCondor-CE supports reporting BDII information for all HTCondor-CE endpoints and batch information for an HTCondor batch system. To make this information available, perform the following instructions on your site BDII host. Install the HTCondor-CE BDII package: root@host # yum install htcondor-ce-bdii Configure HTCondor ( /etc/condor/config.d/ ) on your site BDII host to point to your central manager: CONDOR_HOST = <CENTRAL MANAGER> Replacing <CENTRAL MANAGER> with the hostname of your HTCondor central manager Configure BDII static information by modifying /etc/condor/config.d/99-ce-bdii.conf Additionally, install the HTCondor-CE BDII package on each of your HTCondor-CE hosts: root@host # yum install htcondor-ce-bdii Next Steps \u00b6 At this point, you should have an installation of HTCondor-CE that will forward pilot jobs into your site's batch system unchanged. If you need to transform incoming pilot jobs (e.g. by setting a partition, queue, or accounting group), configure the HTCondor-CE Job Router . Otherwise, continue to the this document to start the relevant services and verify your installation. Getting Help \u00b6 If you have any questions or issues with the installation process, please contact us for assistance.","title":"Installation"},{"location":"installation/htcondor-ce/#installing-an-htcondor-ce","text":"Note If you are installing an HTCondor-CE for the Open Science Grid (OSG), consult the OSG-specific documentation . HTCondor-CE is a special configuration of the HTCondor software designed as a Compute Entrypoint solution for computing grids (e.g. European Grid Infrastructure , Open Science Grid ). It is configured to use the Job Router daemon to delegate resource allocation requests by transforming and submitting them to the site\u2019s batch system. See the overview page for more details on the features and architecture of HTCondor-CE. Use this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the HTCondor Yum repositories .","title":"Installing an HTCondor-CE"},{"location":"installation/htcondor-ce/#before-starting","text":"Before starting the installation process, consider the following points (consulting the reference page as necessary): User IDs: If they do not exist already, the installation will create the condor Linux user (UID 4716) SSL certificate: The HTCondor-CE service uses a host certificate and key for SSL and GSI authentication DNS entries: Forward and reverse DNS must resolve for the HTCondor-CE host Network ports: The pilot factories must be able to contact your HTCondor-CE service on port 9619 (TCP) Submit host: HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster running supported batch system software (Grid Engine, HTCondor, LSF, PBS/Torque, Slurm) File Systems : Non-HTCondor batch systems require a shared file system between the HTCondor-CE host and the batch system worker nodes. There are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system (Red Hat Enterprise Linux variant 7) Obtain root access to the host Prepare the EPEL and HTCondor Development Yum repositories Install CA certificates and VO data into /etc/grid-security/certificates and /etc/grid-security/vomsdir , respectively","title":"Before Starting"},{"location":"installation/htcondor-ce/#installing-htcondor-ce","text":"Important HTCondor-CE must be installed on a host that is configured to submit jobs to your batch system. The details of this setup is site-specific by nature and therefore beyond the scope of this document. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the fetch-crl package, available from the EPEL repositories. root@host # yum install fetch-crl Select the appropriate convenience RPM: If your batch system is... Then use the following package... Grid Engine htcondor-ce-sge HTCondor htcondor-ce-condor LSF htcondor-ce-lsf PBS/Torque htcondor-ce-pbs SLURM htcondor-ce-slurm Install the CE software: root@host # yum install <PACKAGE> Where <PACKAGE> is the package you selected in the above step.","title":"Installing HTCondor-CE"},{"location":"installation/htcondor-ce/#configuring-htcondor-ce","text":"There are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on optional configurations .","title":"Configuring HTCondor-CE"},{"location":"installation/htcondor-ce/#configuring-authentication","text":"To authenticate job submission from external users and VOs, HTCondor-CE can be configured to use a built-in mapfile or to make Globus callouts to an external service like Argus or LCMAPS. The former option is simpler but the latter option may be preferred if your grid supports it or your site already runs such a service. Additionally, the HTCondor-CE service uses X.509 certificates for SSL and GSI authentication.","title":"Configuring authentication"},{"location":"installation/htcondor-ce/#built-in-mapfile","text":"The built-in mapfile is a unified HTCondor mapfile located at /etc/condor-ce/condor_mapfile . This file is parsed in line-by-line order and HTCondor-CE will use the first line that matches. Therefore, mappings should be added to the top of the file. Warning condor_mapfile.rpmnew files may be generated upon HTCondor-CE version updates and they should be merged into condor_mapfile . To configure authorization for users submitting jobs with an X.509 proxy certificate to your HTCondor-CE, add lines of the following format: GSI \"^<DISTINGUISHED NAME>$\" <USERNAME> Replacing <DISTINGUISHED NAME > (escaping any '/' with '\\/') and <USERNAME > with the distinguished name of the incoming certificate and the unix account under which the job should run, respectively. VOMS attributes of incoming X.509 proxy certificates can also be used for mapping: GSI \"<DISTINGUISHED NAME>,<VOMS FQAN 1>,<VOMS FQAN 2>,...,<VOMSFQAN N>\" <USERNAME> Replacing <DISTINGUISHED NAME > (escaping any '/' with '\\/'), <VOMSFQAN > fields, and <USERNAME > with the distinguished name of the incoming certificate, the VOMS roles and groups, and the unix account under which the job should run, respectively. Additionally, you can use regular expressions for mapping certificate and VOMS attribute credentials. For example, to map any certificate from the GLOW VO with the htpc role to the glow user, add the following line: GSI \".*,\\/GLOW\\/Role=htpc.*\" glow Warning You should only add mappings to the mapfile. Do not remove any of the default mappings: GSI \"(/CN=[-.A-Za-z0-9/= ]+)\" \\1@unmapped.htcondor.org CLAIMTOBE .* anonymous@claimtobe FS (.*) \\1","title":"Built-in mapfile"},{"location":"installation/htcondor-ce/#globus-callout","text":"To use a Globus callout to a service like LCMAPS or Argus, you will need to have the relevant library installed as well as the following HTCondor-CE configuration: Add the following line to the top of /etc/condor-ce/condor_mapfile : GSI (.*) GSS_ASSIST_GRIDMAP Create /etc/grid-security/gsi-authz.conf with the following content: For LCMAPS: globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout For Argus: globus_mapping /usr/lib64/libgsi_pep_callout.so argus_pep_callout","title":"Globus callout"},{"location":"installation/htcondor-ce/#configuring-certificates","text":"HTCondor-CE uses X.509 host certificates and certificate authorities (CAs) when authenticating SSL and GSI connections. By default, HTCondor-CE uses the default system locations to locate CAs and host certificate when authenticating SSL connections, i.e. for SciTokens or SSL authentication methods. But traditionally, CEs and their clients have authenticated with each other using specialized grid certificates (e.g. certificates issued by IGTF CAs ) located in /etc/grid-security/ . Choose one of the following options to configure your HTCondor-CE to use grid or system certificates for authentication: If your SSL or SciTokens clients will be interacting with your CE using grid certificates or you are using a grid certificate as your host certificate: Set the following configuration in /etc/condor-ce/config.d/01-ce-auth.conf : AUTH_SSL_SERVER_CERTFILE = /etc/grid-security/hostcert.pem AUTH_SSL_SERVER_KEYFILE = /etc/grid-security/hostkey.pem AUTH_SSL_SERVER_CADIR = /etc/grid-security/certificates AUTH_SSL_SERVER_CAFILE = AUTH_SSL_CLIENT_CERTFILE = /etc/grid-security/hostcert.pem AUTH_SSL_CLIENT_KEYFILE = /etc/grid-security/hostkey.pem AUTH_SSL_CLIENT_CADIR = /etc/grid-security/certificates AUTH_SSL_CLIENT_CAFILE = Install your host certificate and key into /etc/grid-security/hostcert.pem and /etc/grid-security/hostkey.pem , respectively Set the ownership and Unix permissions of the host certificate and key root@host # chown root:root /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem root@host # chmod 644 /etc/grid-security/hostcert.pem root@host # chmod 600 /etc/grid-security/hostkey.pem Otherwise, use the default system locations: Install your host certificate and key into /etc/pki/tls/certs/localhost.crt and /etc/pki/tls/private/localhost.key , respectively Set the ownership and Unix permissions of the host certificate and key root@host # chown root:root /etc/pki/tls/certs/localhost.crt /etc/pki/tls/private/localhost.key root@host # chmod 644 /etc/pki/tls/certs/localhost.crt root@host # chmod 600 /etc/pki/tls/private/localhost.key","title":"Configuring certificates"},{"location":"installation/htcondor-ce/#configuring-the-batch-system","text":"Before HTCondor-CE can submit jobs to your local batch system, it has to be configured to do so. The configuration will differ depending on if your local batch system is HTCondor or one of the other supported batch systems. Choose the section corresponding to your batch system below.","title":"Configuring the batch system"},{"location":"installation/htcondor-ce/#htcondor-batch-systems","text":"To configure HTCondor-CE for an HTCondor batch system, set JOB_ROUTER_SCHEDD2_POOL to your site's central manager host and port: JOB_ROUTER_SCHEDD2_POOL = cm.chtc.wisc.edu:9618 Additionally, set JOB_ROUTER_SCHEDD2_SPOOL to the location of the local batch SPOOL directory on the CE host if it is different than the default location ( /var/lib/condor/spool ).","title":"HTCondor batch systems"},{"location":"installation/htcondor-ce/#non-htcondor-batch-systems","text":"","title":"Non-HTCondor batch systems"},{"location":"installation/htcondor-ce/#configuring-the-blahp","text":"HTCondor-CE uses the Batch Language ASCII Helper Protocol (BLAHP) to submit and track jobs to non-HTCondor batch systems. To work with the HTCondor-CE, modify /usr/libexec/condor/glite/etc/batch_gahp.config using the following steps: Disable BLAHP handling of certificate proxies: blah_disable_wn_proxy_renewal=yes blah_delegate_renewed_proxies=no blah_disable_limited_proxy=yes (Optional) If your batch system tools are installed in a non-standard location (i.e., outside of /usr/bin/ ), set the corresponding *_binpath variable to the directory containing your batch system tools: If your batch system is... Then change the following configuration variable... LSF lsf_binpath PBS/Torque pbs_binpath SGE sge_binpath Slurm slurm_binpath For example, if your Slurm binaries (e.g. sbatch ) exist in /opt/slurm/bin , you would set the following: slurm_binpath=/opt/slurm/bin/","title":"Configuring the BLAHP"},{"location":"installation/htcondor-ce/#sharing-the-spool-directory","text":"Non-HTCondor batch systems require a shared file system configuration to support file transfer from the HTCondor-CE to your site's worker nodes. The current recommendation is to run a dedicated NFS server on the CE host . In this setup, HTCondor-CE writes to the local spool directory, the NFS server shares the directory, and each worker node mounts the directory in the same location as on the CE. For example, if your spool directory is /var/lib/condor-ce (the default), you must mount the shared directory to /var/lib/condor-ce on the worker nodes. Note If you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE daemons can write to the spool directory. You can control the value of the spool directory by setting SPOOL in /etc/condor-ce/config.d/99-local.conf (create this file if it doesn't exist). For example, the following sets the SPOOL directory to /home/condor : SPOOL = /home/condor Note The shared spool directory must be readable and writeable by the condor user for HTCondor-CE to function correctly.","title":"Sharing the SPOOL directory"},{"location":"installation/htcondor-ce/#optional-configuration","text":"The following configuration steps are optional and will not be required for all sites. If you do not need any of the following special configurations, skip to the section on next steps .","title":"Optional Configuration"},{"location":"installation/htcondor-ce/#configuring-for-multiple-network-interfaces","text":"If you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname and interface to use when communicating with each other. Set NETWORK_HOSTNAME and NETWORK_INTERFACE to the hostname and IP address of your public interface, respectively, in /etc/condor-ce/config.d/99-local.conf directory with the line: NETWORK_HOSTNAME = condorce.example.com NETWORK_INTERFACE = 127.0.0.1 Replacing condorce.example.com text with your public interface\u2019s hostname and 127.0.0.1 with your public interface\u2019s IP address.","title":"Configuring for multiple network interfaces"},{"location":"installation/htcondor-ce/#limiting-or-disabling-locally-running-jobs-on-the-ce","text":"If you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and scheduler universes. Local and scheduler universes allow jobs to be run on the CE itself, mainly for remote troubleshooting. Pilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another worker node. The two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be configuring them in unison. To change the default limit on the number of locally run jobs (the current default is 20), add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning < <JOB-LIMIT> START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Where <JOB-LIMIT> is the maximum number of jobs allowed to run locally To only allow a specific user to start locally run jobs, add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = target.Owner =?= \"<USERNAME>\" START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Change <USERNAME> for the username allowed to run jobs locally To disable locally run jobs, add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = False START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)","title":"Limiting or disabling locally running jobs on the CE"},{"location":"installation/htcondor-ce/#enabling-the-monitoring-web-interface","text":"The HTCondor-CE View is an optional web interface to the status of your CE. To run the HTCondor-CE View, install the appropriate package and set the relevant configuration. Begin by installing the htcondor-ce-view package: root@host # yum install htcondor-ce-view Restart the condor-ce service Verify the service by entering your CE's hostname into your web browser The website is served on port 80 by default. To change this default, edit the value of HTCONDORCE_VIEW_PORT in /etc/condor-ce/config.d/05-ce-view.conf .","title":"Enabling the monitoring web interface"},{"location":"installation/htcondor-ce/#uploading-accounting-records-to-apel","text":"Batch System Support HTCondor-CE only supports generation of APEL accounting records for HTCondor batch systems. For sites outside of the OSG that need to upload the APEL accounting records, HTCondor-CE supports uploading batch and blah APEL records for HTCondor batch systems: Install the HTCondor-CE APEL package on your CE host: root@host # yum install htcondor-ce-apel On each worker node, set the appropriate scaling factor in the HTCondor configuration (i.e. /etc/condor/config.d/ ) and advertise it in the startd ad: ApelScaling = <SCALING FACTOR> # For example, 1.062 STARTD_ATTRS = $(STARTD_ATTRS) ApelScaling Configure the APEL parser, client, and SSM Records are written to APEL_OUTPUT_DIR in the HTCondor-CE configuration (default: /var/lib/condor-ce/apel/ ) Batch and blah record filenames are prefixed batch- and blah- , respectively Start and enable the condor-ce-apel and condor-ce-apel.timer services","title":"Uploading accounting records to APEL"},{"location":"installation/htcondor-ce/#enabling-bdii-integration","text":"Batch System Support HTCondor-CE only supports reporting BDII information for HTCondor batch systems. HTCondor-CE supports reporting BDII information for all HTCondor-CE endpoints and batch information for an HTCondor batch system. To make this information available, perform the following instructions on your site BDII host. Install the HTCondor-CE BDII package: root@host # yum install htcondor-ce-bdii Configure HTCondor ( /etc/condor/config.d/ ) on your site BDII host to point to your central manager: CONDOR_HOST = <CENTRAL MANAGER> Replacing <CENTRAL MANAGER> with the hostname of your HTCondor central manager Configure BDII static information by modifying /etc/condor/config.d/99-ce-bdii.conf Additionally, install the HTCondor-CE BDII package on each of your HTCondor-CE hosts: root@host # yum install htcondor-ce-bdii","title":"Enabling BDII integration"},{"location":"installation/htcondor-ce/#next-steps","text":"At this point, you should have an installation of HTCondor-CE that will forward pilot jobs into your site's batch system unchanged. If you need to transform incoming pilot jobs (e.g. by setting a partition, queue, or accounting group), configure the HTCondor-CE Job Router . Otherwise, continue to the this document to start the relevant services and verify your installation.","title":"Next Steps"},{"location":"installation/htcondor-ce/#getting-help","text":"If you have any questions or issues with the installation process, please contact us for assistance.","title":"Getting Help"},{"location":"troubleshooting/common-issues/","text":"Common Issues \u00b6","title":"Common issues"},{"location":"troubleshooting/common-issues/#common-issues","text":"","title":"Common Issues"},{"location":"troubleshooting/debugging-tools/","text":"Debugging Tools \u00b6","title":"Debugging tools"},{"location":"troubleshooting/debugging-tools/#debugging-tools","text":"","title":"Debugging Tools"},{"location":"troubleshooting/logs/","text":"Helpful Logs \u00b6","title":"Logs"},{"location":"troubleshooting/logs/#helpful-logs","text":"","title":"Helpful Logs"},{"location":"troubleshooting/remote-troubleshooting/","text":"Troubleshooting Remote HTCondor-CEs \u00b6 Since HTCondor-CE is built on top of HTCondor, it's possible to perform quite a bit of troubleshooting from a remote client with access to the HTCondor command-line tools. For testing end-to-end resource request submission or remote interactive-like access, a grid operator will need access to a host with the htcondor-ce-client installed. This document outlines the steps that a grid operator can perform in order to troubleshoot a remote HTCondor-CE. Verifying Network Connectivity \u00b6 Before performing any troubleshooting of the remote HTCondor-CE service, it's important to verify that the HTCondor-CE can be contacted on its HTCondor-CE port (default: 9619 ) at the specified fully qualified domain name (FQDN). Verifying DNS \u00b6 Reverse DNS and GSI authentication GSI authentication requires that the HTCondor-CE host has a reverse DNS record but that record is not required to match the forward DNS record! For example, if you have an A record htcondor-ce.chtc.wisc.edu -> 123.4.5.678 , a PTR of 123.4.5.678 -> chtc.wisc.edu would satisfy the GSI authentication requirement. As noted in the HTCondor-CE installation document , an HTCondor-CE must have forward and reverse DNS records. To verify DNS, use a tool like nslookup : $ nslookup htcondor-ce.chtc.wisc.edu Server: 144.92.254.254 Address: 144.92.254.254#53 Non-authoritative answer: Name: htcondor-ce.chtc.wisc.edu Address: 128.104.100.65 Name: htcondor-ce.chtc.wisc.edu Address: 2607:f388:107c:501:216:3eff:fe89:aa3 $ nslookup 128 .104.100.65 65.100.104.128.in-addr.arpa name = htcondor-ce.chtc.wisc.edu. Authoritative answers can be found from: 104.128.in-addr.arpa nameserver = dns2.itd.umich.edu. 104.128.in-addr.arpa nameserver = adns1.doit.wisc.edu. 104.128.in-addr.arpa nameserver = adns3.doit.wisc.edu. 104.128.in-addr.arpa nameserver = adns2.doit.wisc.edu. adns2.doit.wisc.edu internet address = 144.92.20.99 dns2.itd.umich.edu internet address = 192.12.80.222 adns3.doit.wisc.edu internet address = 144.92.104.21 adns1.doit.wisc.edu internet address = 144.92.9.21 adns2.doit.wisc.edu has AAAA address 2607:f388:d:2::1006 adns2.doit.wisc.edu has AAAA address 2607:f388::a53:2 adns3.doit.wisc.edu has AAAA address 2607:f388:2:2001::100b adns3.doit.wisc.edu has AAAA address 2607:f388::a53:3 adns1.doit.wisc.edu has AAAA address 2607:f388:2:2001::100a adns1.doit.wisc.edu has AAAA address 2607:f388::a53:1 If not, the HTCondor-CE administrator will have to register the appropriate DNS records. Verifying service connectivity \u00b6 After verifying DNS, check to see if the remote HTCondor-CE is listening on the appropriate port: $ telnet htcondor-ce.chtc.wisc.edu 9619 Trying 128.104.100.65... Connected to htcondor-ce.chtc.wisc.edu. Escape character is '^]'. If not, the HTCondor-CE administrator will have to ensure that the service is running and/or open up their firewall. Verifying Configuration \u00b6 Once you've verified network connectivity, you can start verifying the HTCondor-CE daemons. Inspecting daemons \u00b6 To inspect the running daemons of a remote HTCondor-CE, use condor_status : $ condor_status -any -pool htcondor-ce.chtc.wisc.edu:9619 MyType TargetType Name Collector None My Pool - htcondor-ce.chtc.wisc.edu@htcondor-ce.c Job_Router None htcondor-ce@htcondor-ce.chtc.wisc.edu Scheduler None htcondor-ce.chtc.wisc.edu DaemonMaster None htcondor-ce.chtc.wisc.edu Submitter None nu_lhcb@users.htcondor.org If you don't see the appropriate daemons, ask the administrator to following these troubleshooting steps . Submitter ad When querying daemons for an HTCondor-CE, you may see Submitter ads for each user with jobs in the queue. These ads are used to collect per-user stats that are available to the HTCondor-CE administrator. You can inspect the details of a specific daemon with per-daemon and -long options: $ condor_status -pool htcondor-ce.chtc.wisc.edu:9619 -collector -long ActiveQueryWorkers = 0 ActiveQueryWorkersPeak = 1 AddressV1 = \"{[ p=\\\"primary\\\"; a=\\\"128.104.100.65\\\"; port=9619; n=\\\"Internet\\\"; alias=\\\"htcondor-ce.chtc.wisc.edu\\\"; spid=\\\"collector\\\"; noUDP=true; ], [ p=\\\"IPv4\\\"; a=\\\"128.104.100.65\\\"; port=9619; n=\\\"Internet\\\"; alias=\\\"htcondor-ce.chtc.wisc.edu\\\"; spid=\\\"collector\\\"; noUDP=true; ], [ p=\\\"IPv6\\\"; a=\\\"2607:f388:107c:501:216:3eff:fe89:aa3\\\"; port=9619; n=\\\"Internet\\\"; alias=\\\"htcondor-ce.chtc.wisc.edu\\\"; spid=\\\"collector\\\"; noUDP=true; ]}\" CollectorIpAddr = \"<128.104.100.65:9619?addrs=128.104.100.65-9619+[2607-f388-107c-501-216-3eff-fe89-aa3]-9619&alias=htcondor-ce.chtc.wisc.edu&noUDP&sock=collector>\" CondorAdmin = \"root@htcondor-ce.chtc.wisc.edu\" CondorPlatform = \"$CondorPlatform: x86_64_CentOS7 $\" CondorVersion = \"$CondorVersion: 8.9.8 Jun 29 2020 BuildID: 508520 PackageID: 8.9.8-0.508520 $\" CurrentJobsRunningAll = 0 CurrentJobsRunningGrid = 0 CurrentJobsRunningJava = 0 Inspecting resource requests \u00b6 To inspect resource requests submitted to a remote HTCondor-CE, use condor_q : $ condor_q -all -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -- Schedd: htcondor-ce.chtc.wisc.edu : <128.104.100.65:9619?... @ 10/29/20 15:31:45 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS nu_lhcb ID: 24631 10/18 12:06 33 _ _ _ 35 24631.5-15 nu_lhcb ID: 24632 10/18 12:23 7 _ _ _ 9 24632.2-4 nu_lhcb ID: 24635 10/18 14:23 3 _ _ _ 5 24635.0-1 nu_lhcb ID: 24636 10/18 14:40 5 _ _ _ 9 24636.0-6 nu_lhcb ID: 24637 10/18 14:58 7 _ _ _ 8 24637.2 nu_lhcb ID: 24638 10/18 15:15 7 _ _ _ 8 24638.1 You can inspect the details of a specific resource request with the -long option: $ condor_q -all -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -long 24631 .5 Arguments = \"\" BufferBlockSize = 32768 BufferSize = 524288 BytesRecvd = 58669.0 BytesSent = 184201.0 ClusterId = 24631 Cmd = \"DIRAC_Ullq7V_pilotwrapper.py\" CommittedSlotTime = 0 CommittedSuspensionTime = 0 CommittedTime = 0 Retrieving HTCondor-CE configuration \u00b6 To verify a remote HTCondor-CE configuration, use condor_config_val : $ condor_config_val -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -dump # Configuration from master on htcondor-ce.chtc.wisc.edu < 128 .104.100.65:9619?addrs = 128 .104.100.65-9619+ [ 2607 -f388-107c-501-216-3eff-fe89-aa3 ] -9619 & alias = htcondor-ce.chtc.wisc.edu & noUDP & sock = master_1744571_b0a0> ABORT_ON_EXCEPTION = false ACCOUNTANT_HOST = ACCOUNTANT_LOCAL_DOMAIN = ActivationTimer = ifThenElse(JobStart =!= UNDEFINED, (time() - JobStart), 0) ActivityTimer = (time() - EnteredCurrentActivity) ADD_WINDOWS_FIREWALL_EXCEPTION = $(CondorIsAdmin) ADVERTISE_IPV4_FIRST = $(PREFER_IPV4) ALL_DEBUG = D:CAT D_ALWAYS:2 ALLOW_ADMIN_COMMANDS = true If you know the name of the configuration variable, you can query for it directly: $ condor_config_val -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -verbose JOB_ROUTER_SCHEDD2_NAME JOB_ROUTER_SCHEDD2_NAME = htcondor-ce.chtc.wisc.edu # at: /etc/condor-ce/config.d/02-ce-condor.conf, line 20 # raw: JOB_ROUTER_SCHEDD2_NAME = $( FULL_HOSTNAME ) Verifying Resource Request Submission \u00b6 After verifying that all the remote HTCondor-CE daemons are up , you can start submitting resource requests! Verifying authentication \u00b6 Before submitting a successful resource request, you will want to verify that you have submit privileges. For this, you will need a credential such as a grid proxy: $ voms-proxy-info subject : /DC=org/DC=cilogon/C=US/O=University of Wisconsin-Madison/CN=Brian Lin A2266246/CN=41319870 issuer : /DC=org/DC=cilogon/C=US/O=University of Wisconsin-Madison/CN=Brian Lin A2266246 identity : /DC=org/DC=cilogon/C=US/O=University of Wisconsin-Madison/CN=Brian Lin A2266246 type : RFC compliant proxy strength : 1024 bits path : /tmp/x509up_u1000 timeleft : 3:55:22 After you have retrieved your credential, verify that you have the ability to submit requests to the remote HTCondor-CE (i.e., WRITE access) with condor_ping : $ export _condor_SEC_CLIENT_AUTHENTICATION_METHODS = SCITOKENS,GSI $ export _condor_SEC_TOOL_DEBUG = D_SECURITY:2 # Extremely verbose debugging for troubleshooting authentication issues $ condor_ping -name htcondor-ce.chtc.wisc.edu \\ -pool htcondor-ce.chtc.wisc.edu:9619 \\ -verbose \\ -debug \\ WRITE [...] Remote Version: $CondorVersion: 8.9.8 Jun 29 2020 BuildID: 508520 PackageID: 8.9.8-0.508520 $ Local Version: $CondorVersion: 8.9.9 Aug 26 2020 BuildID: 515894 PackageID: 8.9.9-0.515894 PRE-RELEASE-UWCS $ Session ID: htcondor-ce:2980451:1604006441:0 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: FS,GSI Remote Mapping: blin@users.htcondor.org Authorized: TRUE If condor_ping fails, ask the administrator to follow this troubleshooting section , set SCHEDD_DEBUG = $(SCHEDD_DEBUG) D_SECURITY:2 , and cross-check the HTCondor-CE SchedLog for authentication issues. Submitting a trace request \u00b6 HTCondor-CE client This section requires an installation of the htcondor-ce-client . The easiest way to troubleshoot end-to-end resource request submission is condor_ce_trace , available in the htcondor-ce-client . Follow this documentation for detailed instructions for installing and using condor_ce_trace . Advanced Troubleshooting \u00b6 HTCondor-CE client This section requires an installation of the htcondor-ce-client . If the issue at hand is complicated or the communication turnaround time with the administrator is too long, it is often more expedient to grant the operator direct access to the HTCondor-CE host. Instead of direct login access, HTCondor-CE has the ability to allow a remote operator to run commands on the host as an unprivileged user. This requires permission to submit resource requests as well as an HTCondor-CE that is configured to run local universe jobs: $ condor_config_val -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -verbose START_LOCAL_UNIVERSE START_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning < 20 # at: /etc/condor-ce/config.d/03-managed-fork.conf, line 11 # raw: START_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning < 20 # default: TotalLocalJobsRunning < 200 After verifying that you can submit resource requests and that the HTCondor-CE supports local universe, use condor_ce_run to run commands on the remote HTCondor-CE host: $ condor_ce_run -lr htcondor-ce.chtc.wisc.edu /bin/sh -c 'condor_q -all' -- Schedd: htcondor-ce.chtc.wisc.edu : <128.104.100.65:9618?... @ 10/29/20 17:42:27 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS nu_lhcb ID: 530251 7/13 22:20 _ _ _ _ 1 530251.0 nu_lhcb ID: 586158 10/28 05:15 _ _ _ 1 1 586158.0 nu_lhcb ID: 586213 10/28 06:23 _ 1 _ _ 1 586213.0 nu_lhcb ID: 586228 10/28 06:23 _ 1 _ _ 1 586228.0 nu_lhcb ID: 586254 10/28 06:23 _ 1 _ _ 1 586254.0 nu_lhcb ID: 586289 10/28 07:58 _ _ _ 1 1 586289.0 Getting Help \u00b6 If you have any questions or issues about troubleshooting remote HTCondor-CEs, please contact us for assistance.","title":"Remote Troubleshooting"},{"location":"troubleshooting/remote-troubleshooting/#troubleshooting-remote-htcondor-ces","text":"Since HTCondor-CE is built on top of HTCondor, it's possible to perform quite a bit of troubleshooting from a remote client with access to the HTCondor command-line tools. For testing end-to-end resource request submission or remote interactive-like access, a grid operator will need access to a host with the htcondor-ce-client installed. This document outlines the steps that a grid operator can perform in order to troubleshoot a remote HTCondor-CE.","title":"Troubleshooting Remote HTCondor-CEs"},{"location":"troubleshooting/remote-troubleshooting/#verifying-network-connectivity","text":"Before performing any troubleshooting of the remote HTCondor-CE service, it's important to verify that the HTCondor-CE can be contacted on its HTCondor-CE port (default: 9619 ) at the specified fully qualified domain name (FQDN).","title":"Verifying Network Connectivity"},{"location":"troubleshooting/remote-troubleshooting/#verifying-dns","text":"Reverse DNS and GSI authentication GSI authentication requires that the HTCondor-CE host has a reverse DNS record but that record is not required to match the forward DNS record! For example, if you have an A record htcondor-ce.chtc.wisc.edu -> 123.4.5.678 , a PTR of 123.4.5.678 -> chtc.wisc.edu would satisfy the GSI authentication requirement. As noted in the HTCondor-CE installation document , an HTCondor-CE must have forward and reverse DNS records. To verify DNS, use a tool like nslookup : $ nslookup htcondor-ce.chtc.wisc.edu Server: 144.92.254.254 Address: 144.92.254.254#53 Non-authoritative answer: Name: htcondor-ce.chtc.wisc.edu Address: 128.104.100.65 Name: htcondor-ce.chtc.wisc.edu Address: 2607:f388:107c:501:216:3eff:fe89:aa3 $ nslookup 128 .104.100.65 65.100.104.128.in-addr.arpa name = htcondor-ce.chtc.wisc.edu. Authoritative answers can be found from: 104.128.in-addr.arpa nameserver = dns2.itd.umich.edu. 104.128.in-addr.arpa nameserver = adns1.doit.wisc.edu. 104.128.in-addr.arpa nameserver = adns3.doit.wisc.edu. 104.128.in-addr.arpa nameserver = adns2.doit.wisc.edu. adns2.doit.wisc.edu internet address = 144.92.20.99 dns2.itd.umich.edu internet address = 192.12.80.222 adns3.doit.wisc.edu internet address = 144.92.104.21 adns1.doit.wisc.edu internet address = 144.92.9.21 adns2.doit.wisc.edu has AAAA address 2607:f388:d:2::1006 adns2.doit.wisc.edu has AAAA address 2607:f388::a53:2 adns3.doit.wisc.edu has AAAA address 2607:f388:2:2001::100b adns3.doit.wisc.edu has AAAA address 2607:f388::a53:3 adns1.doit.wisc.edu has AAAA address 2607:f388:2:2001::100a adns1.doit.wisc.edu has AAAA address 2607:f388::a53:1 If not, the HTCondor-CE administrator will have to register the appropriate DNS records.","title":"Verifying DNS"},{"location":"troubleshooting/remote-troubleshooting/#verifying-service-connectivity","text":"After verifying DNS, check to see if the remote HTCondor-CE is listening on the appropriate port: $ telnet htcondor-ce.chtc.wisc.edu 9619 Trying 128.104.100.65... Connected to htcondor-ce.chtc.wisc.edu. Escape character is '^]'. If not, the HTCondor-CE administrator will have to ensure that the service is running and/or open up their firewall.","title":"Verifying service connectivity"},{"location":"troubleshooting/remote-troubleshooting/#verifying-configuration","text":"Once you've verified network connectivity, you can start verifying the HTCondor-CE daemons.","title":"Verifying Configuration"},{"location":"troubleshooting/remote-troubleshooting/#inspecting-daemons","text":"To inspect the running daemons of a remote HTCondor-CE, use condor_status : $ condor_status -any -pool htcondor-ce.chtc.wisc.edu:9619 MyType TargetType Name Collector None My Pool - htcondor-ce.chtc.wisc.edu@htcondor-ce.c Job_Router None htcondor-ce@htcondor-ce.chtc.wisc.edu Scheduler None htcondor-ce.chtc.wisc.edu DaemonMaster None htcondor-ce.chtc.wisc.edu Submitter None nu_lhcb@users.htcondor.org If you don't see the appropriate daemons, ask the administrator to following these troubleshooting steps . Submitter ad When querying daemons for an HTCondor-CE, you may see Submitter ads for each user with jobs in the queue. These ads are used to collect per-user stats that are available to the HTCondor-CE administrator. You can inspect the details of a specific daemon with per-daemon and -long options: $ condor_status -pool htcondor-ce.chtc.wisc.edu:9619 -collector -long ActiveQueryWorkers = 0 ActiveQueryWorkersPeak = 1 AddressV1 = \"{[ p=\\\"primary\\\"; a=\\\"128.104.100.65\\\"; port=9619; n=\\\"Internet\\\"; alias=\\\"htcondor-ce.chtc.wisc.edu\\\"; spid=\\\"collector\\\"; noUDP=true; ], [ p=\\\"IPv4\\\"; a=\\\"128.104.100.65\\\"; port=9619; n=\\\"Internet\\\"; alias=\\\"htcondor-ce.chtc.wisc.edu\\\"; spid=\\\"collector\\\"; noUDP=true; ], [ p=\\\"IPv6\\\"; a=\\\"2607:f388:107c:501:216:3eff:fe89:aa3\\\"; port=9619; n=\\\"Internet\\\"; alias=\\\"htcondor-ce.chtc.wisc.edu\\\"; spid=\\\"collector\\\"; noUDP=true; ]}\" CollectorIpAddr = \"<128.104.100.65:9619?addrs=128.104.100.65-9619+[2607-f388-107c-501-216-3eff-fe89-aa3]-9619&alias=htcondor-ce.chtc.wisc.edu&noUDP&sock=collector>\" CondorAdmin = \"root@htcondor-ce.chtc.wisc.edu\" CondorPlatform = \"$CondorPlatform: x86_64_CentOS7 $\" CondorVersion = \"$CondorVersion: 8.9.8 Jun 29 2020 BuildID: 508520 PackageID: 8.9.8-0.508520 $\" CurrentJobsRunningAll = 0 CurrentJobsRunningGrid = 0 CurrentJobsRunningJava = 0","title":"Inspecting daemons"},{"location":"troubleshooting/remote-troubleshooting/#inspecting-resource-requests","text":"To inspect resource requests submitted to a remote HTCondor-CE, use condor_q : $ condor_q -all -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -- Schedd: htcondor-ce.chtc.wisc.edu : <128.104.100.65:9619?... @ 10/29/20 15:31:45 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS nu_lhcb ID: 24631 10/18 12:06 33 _ _ _ 35 24631.5-15 nu_lhcb ID: 24632 10/18 12:23 7 _ _ _ 9 24632.2-4 nu_lhcb ID: 24635 10/18 14:23 3 _ _ _ 5 24635.0-1 nu_lhcb ID: 24636 10/18 14:40 5 _ _ _ 9 24636.0-6 nu_lhcb ID: 24637 10/18 14:58 7 _ _ _ 8 24637.2 nu_lhcb ID: 24638 10/18 15:15 7 _ _ _ 8 24638.1 You can inspect the details of a specific resource request with the -long option: $ condor_q -all -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -long 24631 .5 Arguments = \"\" BufferBlockSize = 32768 BufferSize = 524288 BytesRecvd = 58669.0 BytesSent = 184201.0 ClusterId = 24631 Cmd = \"DIRAC_Ullq7V_pilotwrapper.py\" CommittedSlotTime = 0 CommittedSuspensionTime = 0 CommittedTime = 0","title":"Inspecting resource requests"},{"location":"troubleshooting/remote-troubleshooting/#retrieving-htcondor-ce-configuration","text":"To verify a remote HTCondor-CE configuration, use condor_config_val : $ condor_config_val -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -dump # Configuration from master on htcondor-ce.chtc.wisc.edu < 128 .104.100.65:9619?addrs = 128 .104.100.65-9619+ [ 2607 -f388-107c-501-216-3eff-fe89-aa3 ] -9619 & alias = htcondor-ce.chtc.wisc.edu & noUDP & sock = master_1744571_b0a0> ABORT_ON_EXCEPTION = false ACCOUNTANT_HOST = ACCOUNTANT_LOCAL_DOMAIN = ActivationTimer = ifThenElse(JobStart =!= UNDEFINED, (time() - JobStart), 0) ActivityTimer = (time() - EnteredCurrentActivity) ADD_WINDOWS_FIREWALL_EXCEPTION = $(CondorIsAdmin) ADVERTISE_IPV4_FIRST = $(PREFER_IPV4) ALL_DEBUG = D:CAT D_ALWAYS:2 ALLOW_ADMIN_COMMANDS = true If you know the name of the configuration variable, you can query for it directly: $ condor_config_val -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -verbose JOB_ROUTER_SCHEDD2_NAME JOB_ROUTER_SCHEDD2_NAME = htcondor-ce.chtc.wisc.edu # at: /etc/condor-ce/config.d/02-ce-condor.conf, line 20 # raw: JOB_ROUTER_SCHEDD2_NAME = $( FULL_HOSTNAME )","title":"Retrieving HTCondor-CE configuration"},{"location":"troubleshooting/remote-troubleshooting/#verifying-resource-request-submission","text":"After verifying that all the remote HTCondor-CE daemons are up , you can start submitting resource requests!","title":"Verifying Resource Request Submission"},{"location":"troubleshooting/remote-troubleshooting/#verifying-authentication","text":"Before submitting a successful resource request, you will want to verify that you have submit privileges. For this, you will need a credential such as a grid proxy: $ voms-proxy-info subject : /DC=org/DC=cilogon/C=US/O=University of Wisconsin-Madison/CN=Brian Lin A2266246/CN=41319870 issuer : /DC=org/DC=cilogon/C=US/O=University of Wisconsin-Madison/CN=Brian Lin A2266246 identity : /DC=org/DC=cilogon/C=US/O=University of Wisconsin-Madison/CN=Brian Lin A2266246 type : RFC compliant proxy strength : 1024 bits path : /tmp/x509up_u1000 timeleft : 3:55:22 After you have retrieved your credential, verify that you have the ability to submit requests to the remote HTCondor-CE (i.e., WRITE access) with condor_ping : $ export _condor_SEC_CLIENT_AUTHENTICATION_METHODS = SCITOKENS,GSI $ export _condor_SEC_TOOL_DEBUG = D_SECURITY:2 # Extremely verbose debugging for troubleshooting authentication issues $ condor_ping -name htcondor-ce.chtc.wisc.edu \\ -pool htcondor-ce.chtc.wisc.edu:9619 \\ -verbose \\ -debug \\ WRITE [...] Remote Version: $CondorVersion: 8.9.8 Jun 29 2020 BuildID: 508520 PackageID: 8.9.8-0.508520 $ Local Version: $CondorVersion: 8.9.9 Aug 26 2020 BuildID: 515894 PackageID: 8.9.9-0.515894 PRE-RELEASE-UWCS $ Session ID: htcondor-ce:2980451:1604006441:0 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: FS,GSI Remote Mapping: blin@users.htcondor.org Authorized: TRUE If condor_ping fails, ask the administrator to follow this troubleshooting section , set SCHEDD_DEBUG = $(SCHEDD_DEBUG) D_SECURITY:2 , and cross-check the HTCondor-CE SchedLog for authentication issues.","title":"Verifying authentication"},{"location":"troubleshooting/remote-troubleshooting/#submitting-a-trace-request","text":"HTCondor-CE client This section requires an installation of the htcondor-ce-client . The easiest way to troubleshoot end-to-end resource request submission is condor_ce_trace , available in the htcondor-ce-client . Follow this documentation for detailed instructions for installing and using condor_ce_trace .","title":"Submitting a trace request"},{"location":"troubleshooting/remote-troubleshooting/#advanced-troubleshooting","text":"HTCondor-CE client This section requires an installation of the htcondor-ce-client . If the issue at hand is complicated or the communication turnaround time with the administrator is too long, it is often more expedient to grant the operator direct access to the HTCondor-CE host. Instead of direct login access, HTCondor-CE has the ability to allow a remote operator to run commands on the host as an unprivileged user. This requires permission to submit resource requests as well as an HTCondor-CE that is configured to run local universe jobs: $ condor_config_val -name htcondor-ce.chtc.wisc.edu -pool htcondor-ce.chtc.wisc.edu:9619 -verbose START_LOCAL_UNIVERSE START_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning < 20 # at: /etc/condor-ce/config.d/03-managed-fork.conf, line 11 # raw: START_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning < 20 # default: TotalLocalJobsRunning < 200 After verifying that you can submit resource requests and that the HTCondor-CE supports local universe, use condor_ce_run to run commands on the remote HTCondor-CE host: $ condor_ce_run -lr htcondor-ce.chtc.wisc.edu /bin/sh -c 'condor_q -all' -- Schedd: htcondor-ce.chtc.wisc.edu : <128.104.100.65:9618?... @ 10/29/20 17:42:27 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE HOLD TOTAL JOB_IDS nu_lhcb ID: 530251 7/13 22:20 _ _ _ _ 1 530251.0 nu_lhcb ID: 586158 10/28 05:15 _ _ _ 1 1 586158.0 nu_lhcb ID: 586213 10/28 06:23 _ 1 _ _ 1 586213.0 nu_lhcb ID: 586228 10/28 06:23 _ 1 _ _ 1 586228.0 nu_lhcb ID: 586254 10/28 06:23 _ 1 _ _ 1 586254.0 nu_lhcb ID: 586289 10/28 07:58 _ _ _ 1 1 586289.0","title":"Advanced Troubleshooting"},{"location":"troubleshooting/remote-troubleshooting/#getting-help","text":"If you have any questions or issues about troubleshooting remote HTCondor-CEs, please contact us for assistance.","title":"Getting Help"},{"location":"troubleshooting/troubleshooting/","text":"HTCondor-CE Troubleshooting Guide \u00b6 In this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of common issues with suggested troubleshooting steps. Known Issues \u00b6 SUBMIT_ATTRS are not applied to jobs on the local HTCondor \u00b6 If you are adding attributes to jobs submitted to your HTCondor pool with SUBMIT_ATTRS , these will not be applied to jobs that are entering your pool from the HTCondor-CE. To get around this, you will want to add the attributes to your job routes . If the CE is the only entry point for jobs into your pool, you can get rid of SUBMIT_ATTRS on your backend. Otherwise, you will have to maintain your list of attributes both in your list of routes and in your SUBMIT_ATTRS . General Troubleshooting Items \u00b6 Making sure packages are up-to-date \u00b6 It is important to make sure that the HTCondor-CE and related RPMs are up-to-date. root@host # yum update \"htcondor-ce*\" blahp condor If you just want to see the packages to update, but do not want to perform the update now, answer N at the prompt. Verify package contents \u00b6 If the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the contents of your packages (ignoring changes to configuration files): user@host $ rpm -q --verify htcondor-ce htcondor-ce-client blahp | grep -v '/var/' | awk '$2 != \"c\" {print $0}' If the verification command returns output, this means that your packages have been changed. To fix this, you can reinstall the packages: user@host $ yum reinstall htcondor-ce htcondor-ce-client blahp Note The reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an .rpmnew suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration. Verify clocks are synchronized \u00b6 Like all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is synchronized using a utility such as ntpd . Additionally, HTCondor itself is sensitive to time skews on the NFS server. If you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew. Verify host cerificates and CRLs are valid \u00b6 An expired host certificate or CRLs will cause various issues with GSI authentication. Verify that your host certificate is valid by running: root@host # openssl x509 -in /etc/grid-security/hostcert.pem -noout -dates Likewise, run the fetch-crl script to update your CRLs: root@host # fetch-crl If updating CRLs fix your issues, make sure that the fetch-crl-cron and fetch-crl-boot services are enabled and running. HTCondor-CE Troubleshooting Items \u00b6 This section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. Before troubleshooting, we recommend increasing the log level: Write the following into /etc/condor-ce/config.d/99-local.conf to increase the log level for all daemons: ALL_DEBUG = D_ALWAYS:2 D_CAT Ensure that the configuration is in place: root@host # condor_ce_reconfig Reproduce the issue Note Before spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running condor_ce_reconfig . Daemons fail to start \u00b6 If there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to startup. Check the following subsections in order: Symptoms Daemon startup failure may manifest in many ways, the following are few symptoms of the problem. The service fails to start: root@host # service condor-ce start Starting Condor-CE daemons: [ FAIL ] condor_ce_q fails with a lengthy error message: user@host $ condor_ce_q Error: Extra Info: You probably saw this error because the condor_schedd is not running on the machine you are trying to query. If the condor_schedd is not running, the Condor system will not be able to find an address and port to connect to and satisfy this request. Please make sure the Condor daemons are running and try again. Extra Info: If the condor_schedd is running on the machine you are trying to query and you still see the error, the most likely cause is that you have setup a personal Condor, you have not defined SCHEDD_NAME in your condor_config file, and something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define either or both of those settings in your config file, or you must use the -name option to condor_q. Please see the Condor manual for details on SCHEDD_NAME and SCHEDD_ADDRESS_FILE. Next actions If the MasterLog is filled with ERROR:SECMAN...TCP connection to collector...failed : This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in this section of the install guide. If the MasterLog is filled with DC_AUTHENTICATE errors: The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in /etc/condor-ce/condor_mapfile . If the SchedLog is filled with Can\u2019t find address for negotiator : You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one. Jobs fail to submit to the CE \u00b6 If a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the culprit, then you may have encountered an authentication or authorization issue. You may see error messages like the following in your SchedLog : 08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa) 08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189 08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done! Next actions Check voms-mapfile or grid-mapfile and ensure that the user's DN or VOMS attributes are known to your authentication method , and that the mapped users exist on your CE and cluster. Check for lcmaps errors in /var/log/messages If you do not see helpful error messages in /var/log/messages , adjust the debug level by adding export LCMAPS_DEBUG_LEVEL=5 to /etc/sysconfig/condor-ce , restarting the condor-ce service, and checking /var/log/messages for errors again. Jobs stay idle on the CE \u00b6 Check the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is busy. Idle jobs on CE: Make sure the underlying batch system can run jobs \u00b6 HTCondor-CE delegates jobs to your batch system, which is then responsible for matching jobs to worker nodes. If you cannot manually submit jobs (e.g., condor_submit , qsub ) on the CE host to your batch system, then HTCondor-CE won't be able to either. Procedure Manually create and submit a simple job (e.g., one that runs sleep ) Check for errors in the submission itself Watch the job in the batch system queue (e.g., condor_q , qstat ) If the job does not run, check for errors on the batch system Next actions Consult troubleshooting documentation or support avenues for your batch system. Once you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again. Idle jobs on CE: Is the job router handling the incoming job? \u00b6 Jobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things if you suspect that the jobs are not being matched. Check if the JobRouter sees a job before that by looking at the job router log and looking for the text src=<JOB-ID>\u2026claimed job . Next actions Use condor_ce_job_router_info to see why your idle job does not match any routes Idle jobs on CE: Verify correct operation between the CE and your local batch system \u00b6 For HTCondor batch systems \u00b6 HTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch system interaction will appear in the JobRouterLog . Next actions Check the JobRouterLog for failures. Verify that the local HTCondor is functional. Use condor_ce_config_val to verify that the JOB_ROUTER_SCHEDD2_NAME , JOB_ROUTER_SCHEDD2_POOL , and JOB_ROUTER_SCHEDD2_SPOOL configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively. Use condor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE and verify that it is set to .* . For non-HTCondor batch systems \u00b6 HTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch system interaction will appear in the GridmanagerLog . Look for gm state change\u2026 lines to figure out where the issures are occuring. Next actions If you see failures in the GridmanagerLog during job submission: Save the submit files by adding the appropriate entry to blah.config and submit it manually to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the <batch system>_binpath in /etc/blah.config . If you see failures in the GridmanagerLog during queries for job status: Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in /usr/libexec/blahp/<batch system>_status.sh (e.g., /usr/libexec/blahp/lsf_status.sh ) that take the argument batch system/YYYMMDD/job ID (e.g., lsf/20141008/65053 ). Run the appropriate status script for your batch system and upon success, you should see the following output: root@host # /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053 [ BatchjobId = \"894862\"; JobStatus = 4; ExitCode = 0; WorkerNode = \"atl-prod08\" ] If the script fails, request help from the OSG. Idle jobs on CE: Verify ability to change permissions on key files \u00b6 HTCondor-CE needs the ability to write and chown files in its spool directory and if it cannot, jobs will not run at all. Spool permission errors can appear in the SchedLog and the JobRouterLog . Symptoms 09/17/14 14:45:42 Error: Unable to chown '/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env' from 12345 to 54321 Next actions As root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions. Verify that there aren't any underlying file system issues in the specified location Jobs stay idle on a remote host submitting to the CE \u00b6 If you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not see a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not authorized to run there. Note that jobs may take several minutes or longer if the CE is busy. Remote idle jobs: Can you contact the CE? \u00b6 To check basic connectivity to a CE, use condor_ce_ping : Symptoms user@host $ condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE ERROR: couldn't locate condorce.example.com! Next actions Make sure that the HTCondor-CE daemons are running with condor_ce_status . Verify that your CE is reachable from your submit host, replacing condorce.example.com with the hostname of your CE: user@host $ ping condorce.example.com Remote idle jobs: Are you authorized to run jobs on the CE? \u00b6 The CE will only run jobs from users that authenticate through the HTCondor-CE configuration . You can use condor_ce_ping to check if you are authorized and what user your proxy is being mapped to. Symptoms user@host $ condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE Remote Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Local Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Session ID: condorce:3343:1412790611:0 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: GSI Remote Mapping: gsi@unmapped Authorized: FALSE Notice the failures in the above message: Remote Mapping: gsi@unmapped and Authorized: FALSE Next actions Verify that an authentication method is set up on the CE Verify that your user DN is mapped to an existing system user Jobs go on hold \u00b6 Jobs will be put on held with a HoldReason attribute that can be inspected with condor_ce_q : user@host $ condor_ce_q -l <JOB-ID> -attr HoldReason HoldReason = \"CE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to no matching routes, route job limit, or route failure threshold.\" Held jobs: no matching routes, route job limit, or route failure threshold \u00b6 Jobs on the CE will be put on hold if they are not claimed by the job router within 30 minutes. The most common cases for this behavior are as follows: The job does not match any job routes: use condor_ce_job_router_info to see why your idle job does not match any routes . The route(s) that the job matches to are full: See limiting the number of jobs . The job router is throttling submission to your batch system due to submission failures: See the HTCondor manual for FailureRateThreshold . Check for errors in the JobRouterLog or GridmanagerLog for HTCondor and non-HTCondor batch systems, respectively. Held jobs: Missing/expired user proxy \u00b6 HTCondor-CE requires a valid user proxy for each job that is submitted. You can check the status of your proxy with the following user@host $ voms-proxy-info -all Next actions Ensure that the owner of the job generates their proxy with voms-proxy-init . Held jobs: Invalid job universe \u00b6 The HTCondor-CE only accepts jobs that have universe in their submit files set to vanilla , standard , local , or scheduler . These universes also have corresponding integer values that can be found in the HTCondor manual . Next actions Ensure jobs submitted locally, from the CE host, are submitted with universe = vanilla Ensure jobs submitted from a remote submit point are submitted with: universe = grid grid_resource = condor condorce.example.com condorce.example.com:9619 replacing condorce.example.com with the hostname of the CE. Identifying the corresponding job ID on the local batch system \u00b6 When troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID and the resultant job ID on the batch system. The methods for finding the resultant job ID differs between batch systems. HTCondor batch systems \u00b6 To inspect the CE\u2019s job ad, use condor_ce_q or condor_ce_history : Use condor_ce_q if the job is still in the CE\u2019s queue: user@host $ condor_ce_q <JOB-ID> -af RoutedToJobId Use condor_ce_history if the job has left the CE\u2019s queue: user@host $ condor_ce_history <JOB-ID> -af RoutedToJobId Parse the JobRouterLog for the CE\u2019s job ID. Non-HTCondor batch systems \u00b6 When HTCondor-CE records the corresponding batch system job ID, it is written in the form <BATCH-SYSTEM>/<DATE>/<JOB ID> : lsf/20141206/482046 To inspect the CE\u2019s job ad, use condor_ce_q : user@host $ condor_ce_q <JOB-ID> -af GridJobId Parse the GridmanagerLog for the CE\u2019s job ID. Jobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only) \u00b6 By design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. Therefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps: Identify the misbehaving job ID in your batch system queue Find the job's corresponding CE job ID: user@host $ condor_q <JOB-ID> -af RoutedFromJobId Use condor_ce_rm to remove the CE job from the queue Missing HTCondor tools \u00b6 Most of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. If you are trying to use HTCondor-CE tools and you see the following error: user@host $ condor_ce_job_router_info /usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found This means that the condor_job_router_info (note this is not the CE version), is not in your PATH . Next Actions Either the condor RPM is missing or there are some other issues with it (try rpm --verify condor ). You have installed HTCondor in a non-standard location that is not in your PATH . The condor_job_router_info tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming). HTCondor-CE Troubleshooting Tools \u00b6 HTCondor-CE has its own separate set of of the HTCondor tools with ce in the name (i.e., condor_ce_submit vs condor_submit ). Some of the the commands are only for the CE (e.g., condor_ce_run and condor_ce_trace ) but many of them are just HTCondor commands configured to interact with the CE (e.g., condor_ce_q , condor_ce_status ). It is important to differentiate the two: condor_ce_config_val will provide configuration values for your HTCondor-CE while condor_config_val will provide configuration values for your HTCondor batch system. If you are not running an HTCondor batch system, the non-CE commands will return errors. condor_ce_trace \u00b6 Usage \u00b6 condor_ce_trace is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job. Note You must have generated a proxy (e.g., voms-proxy-init ) and your DN must be added to your chosen authentication method . user@host $ condor_ce_trace condorce.example.com Replacing the condorce.example.com with the hostname of the CE. If you are familiar with the output of condor commands, the command also takes a --debug option that displays verbose condor output. Troubleshooting \u00b6 If the command fails with \u201cFailed ping\u2026\u201d: Make sure that the HTCondor-CE daemons are running on the CE If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line: Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our installation document . If the job submits but does not complete: Look at the status of the job and perform the relevant troubleshooting steps . condor_ce_host_network_check \u00b6 Usage \u00b6 condor_ce_host_network_check is a tool for testing an HTCondor-CE's networking configuration: root@host # condor_ce_host_network_check Starting analysis of host networking for HTCondor-CE System hostname: fermicloud360.fnal.gov FQDN matches hostname Forward resolution of hostname fermicloud360.fnal.gov is 131.225.155.96. Backward resolution of IPv4 131.225.155.96 is fermicloud360.fnal.gov. Forward and backward resolution match! HTCondor is considering all network interfaces and addresses. HTCondor would pick address of 131.225.155.96 as primary address. HTCondor primary address 131.225.155.96 matches system preferred address. Host network configuration should work with HTCondor-CE Troubleshooting \u00b6 If the tool reports that Host network configuration not expected to work with HTCondor-CE , ensure that forward and reverse DNS resolution return the public IP and hostname. condor_ce_run \u00b6 Usage \u00b6 Similar to globus-job-run , condor_ce_run is a tool that submits a simple job to your CE, so it is useful for quickly submitting jobs through your CE. To submit a job to the CE and run the env command on the remote batch system: Note You must have generated a proxy (e.g., voms-proxy-init ) and your DN must be added to your chosen authentication method . user@host $ condor_ce_run -r condorce.example.com:9619 /bin/env Replacing the condorce.example.com with the hostname of the CE. If you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you can run commands locally on the CE with condor_ce_run with the -l option. The following example outputs the JobRouterLog of the CE in question: user@host $ condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog Replacing the condorce.example.com text with the hostname of the CE. To disable this feature on your CE, consult this section of the install documentation. Troubleshooting \u00b6 If you do not see any results: condor_ce_run does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use condor_ce_q in a separate terminal to track the job on the CE. If you never see any results, use condor_ce_trace to pinpoint errors. If you see an error message that begins with \u201cFailed to\u2026\u201d: Check connectivity to the CE with condor_ce_trace or condor_ce_ping condor_ce_submit \u00b6 See this documentation for details condor_ce_ping \u00b6 Usage \u00b6 Use the following condor_ce_ping command to test your ability to submit jobs to an HTCondor-CE, replacing condorce.example.com with the hostname of your CE: user@host $ condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE The following shows successful output where I am able to submit jobs ( Authorized: TRUE ) as the glow user ( Remote Mapping: glow@users.opensciencegrid.org ): Remote Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Local Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Session ID: condorce:27407:1412286981:3 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: GSI Remote Mapping: glow@users.opensciencegrid.org Authorized: TRUE Note If you run the condor_ce_ping command on the CE that you are testing, omit the -name and -pool options. condor_ce_ping takes the same arguments as condor_ping and is documented in the HTCondor manual . Troubleshooting \u00b6 If you see \u201cERROR: couldn\u2019t locate (null)\u201d , that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE: MASTER_DEBUG = D_ALWAYS:2 D_CAT SCHEDD_DEBUG = D_ALWAYS:2 D_CAT Then look in the MasterLog and SchedLog for any errors. If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line , this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our installation document . condor_ce_q \u00b6 Usage \u00b6 condor_ce_q can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 To inspect the full ClassAd for a specific job, specify the -l flag and the job ID: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l <JOB-ID> Note If you run the condor_ce_q command on the CE that you are testing, omit the -name and -pool options. condor_ce_q takes the same arguments as condor_q and is documented in the HTCondor manual . Troubleshooting \u00b6 If the jobs that you are submiting to a CE are not completing, condor_ce_q can tell you the status of your jobs. If the schedd is not running: You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with: MASTER_DEBUG = D_ALWAYS:2 D_CAT SCHEDD_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig Then look in the MasterLog and SchedLog on the CE for any errors. If there are issues with contacting the collector: You will see the following message: user@host $ condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu -- Failed to fetch ads from: <129.59.197.223:9620?sock`33630_8b33_4> : ce1.accre.vanderbilt.edu This may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the ALLOW_READ configuration value is not set: user@host $ condor_ce_config_val -v ALLOW_READ Not defined: ALLOW_READ If it is defined, remove it from the file that is returned in the output. If a job is held: There should be an accompanying HoldReason that will tell you why it is being held. The HoldReason is in the job\u2019s ClassAd, so you can use the long form of condor_ce_q to extract its value: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l <Job ID> | grep HoldReason If a job is idle: The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the condor_ce_job_router_info . condor_ce_history \u00b6 Usage \u00b6 condor_ce_history can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE: user@host $ condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 To inspect the full ClassAd for a specific job, specify the -l flag and the job ID: user@host $ condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l <Job ID> Note If you run the condor_ce_history command on the CE that you are testing, omit the -name and -pool options. condor_ce_history takes the same arguments as condor_history and is documented in the HTCondor manual . condor_ce_job_router_info \u00b6 Usage \u00b6 Use the condor_ce_job_router_info command to help troubleshoot your routes and how jobs will match to them. To see all of your routes (the output is long because it combines your routes with the JOB_ROUTER_DEFAULTS configuration variable): root@host # condor_ce_job_router_info -config To see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of condor_ce_q (replace the <JOB-ID> with the job ID that you are interested in): root@host # condor_ce_q -l <JOB-ID> | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - To inspect a job that has already left the queue, use condor_ce_history instead of condor_ce_q : root@host # condor_ce_history -l <JOB-ID> | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - Note If the proxy for the job has expired, the job will not match any routes. To work around this constraint: root@host # condor_ce_history -l <JOB-ID> | sed \"s/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date '+1 sec'`/\" | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - Alternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file: root@host # condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads <JOBAD-FILE> Troubleshooting \u00b6 If the job does not match any route: You can identify this case when you see 0 candidate jobs found in the condor_job_router_info output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to true . When troubleshooting, look at all of the expressions prior to the target.ProcId >= 0 expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again. If your job matches more than one route: the tool will tell you by showing all matching routes after the job ID: Checking Job src=162,0 against all routes Route Matches: Local_PBS Route Matches: Condor_Test To troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is highlighted below: Umbrella constraint: ((target.x509userproxysubject =!= UNDEFINED) && (target.x509UserProxyExpiration =!= UNDEFINED) && (time() < target.x509UserProxyExpiration) && (target.JobUniverse =?= 5 || target.JobUniverse =?= 1)) && ( (target.osgTestPBS is true) || (true) ) && (target.ProcId >= 0 && target.JobStatus == 1 && (target.StageInStart is undefined || target.StageInFinish isnt undefined) && target.Managed isnt \"ScheddDone\" && target.Managed isnt \"Extenal\" && target.Owner isnt Undefined && target.RoutedBy isnt \"htcondor-ce\") Both routes evaluate to true for the job\u2019s ClassAd because it contained osgTestPBS = true . Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the job route configuration page for more details. If it is unclear why jobs are matching a route: wrap the route's requirements expression in debug() and check the JobRouterLog for more information. condor_ce_router_q \u00b6 Usage \u00b6 If you have multiple job routes and many jobs, condor_ce_router_q is a useful tool to see how jobs are being routed and their statuses: user@host $ condor_ce_router_q condor_ce_router_q takes the same options as condor_router_q and condor_q and is documented in the HTCondor manual condor_ce_status \u00b6 Usage \u00b6 To see the daemons running on a CE, run the following command: user@host $ condor_ce_status -any condor_ce_status takes the same arguments as condor_status , which are documented in the HTCondor manual . \"Missing\" Worker Nodes An HTCondor-CE will not show any worker nodes (e.g. Machine entries in the condor_ce_status -any output) if it does not have any running GlideinWMS pilot jobs. This is expected since HTCondor-CE only forwards incoming pilot jobs to your batch system and does not match jobs to worker nodes. Troubleshooting \u00b6 If the output of condor_ce_status -any does not show at least the following daemons: Collector Scheduler DaemonMaster Job_Router Increase the debug level and consult the HTCondor-CE logs for errors. condor_ce_config_val \u00b6 Usage \u00b6 To see the value of configuration variables and where they are set, use condor_ce_config_val . Primarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. To see the value of a single variable and where it is set: user@host $ condor_ce_config_val -v <CONFIGURATION-VARIABLE> To see a list of all configuration variables and their values: user@host $ condor_ce_config_val -dump To see a list of all the files that are used to create your configuration and the order that they are parsed, use the following command: user@host $ condor_ce_config_val -config condor_ce_config_val takes the same arguments as condor_config_val and is documented in the HTCondor manual . condor_ce_reconfig \u00b6 Usage \u00b6 To ensure that your configuration changes have taken effect, run condor_ce_reconfig . user@host $ condor_ce_reconfig condor_ce_{on,off,restart} \u00b6 Usage \u00b6 To turn on/off/restart HTCondor-CE daemons, use the following commands: root@host # condor_ce_on root@host # condor_ce_off root@host # condor_ce_restart The HTCondor-CE service uses the previous commands with default values. Using these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart: If you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command: root@host # condor_ce_restart -fast This will cause HTCondor-CE to restart and quickly reconnect to all running jobs. If you need to stop running new jobs, run the following: root@host # condor_ce_off -peaceful This will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down. HTCondor-CE Troubleshooting Data \u00b6 The following files are located on the CE host. MasterLog \u00b6 The HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if they fail to start. Location: /var/log/condor-ce/MasterLog Key contents: Start-up, shut-down, and communication with other HTCondor daemons Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: MASTER_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig What to look for: \u00b6 Successful daemon start-up. The following line shows that the Collector daemon started successfully: 10/07/14 14:20:27 Started DaemonCore process \"/usr/sbin/condor_collector -f -port 9619\", pid and pgroup = 7318 SchedLog \u00b6 The HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. It contains valuable information when trying to troubleshoot authentication issues. Location: /var/log/condor-ce/SchedLog Key contents: Every job submitted to the CE User authorization events Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: SCHEDD_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig What to look for \u00b6 Job is submitted to the CE queue: 10/07/14 16:52:17 Submitting new job 234.0 In this example, the ID of the submitted job is 234.0 . Job owner is authorized and mapped: 10/07/14 16:52:17 Command=QMGMT_WRITE_CMD, peer=<131.225.154.68:42262> 10/07/14 16:52:17 AuthMethod=GSI, AuthId=/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Lin 1047, /GLOW/Role=NULL/Capability=NULL, <CondorId=glow@users.opensciencegrid.org> In this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the glow user. User job submission fails due to improper authentication or authorization: 08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa) 08/30/16 16:53:12 PERMISSION DENIED to <gsi@unmapped> from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189 08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done Missing negotiator: 10/18/14 17:32:21 Can't find address for negotiator 10/18/14 17:32:21 Failed to send RESCHEDULE to unknown daemon: Since HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes: 06/23/15 11:15:03 Number of Active Workers 0 Corrupted job_queue.log : 02/07/17 10:55:49 WARNING: Encountered corrupt log record _654 (byte offset 5046225) 02/07/17 10:55:49 103 1354325.0 PeriodicRemove ( StageInFinish > 0 ) 105 02/07/17 10:55:49 Lines following corrupt log record _654 (up to 3): 02/07/17 10:55:49 103 1346101.0 RemoteWallClockTime 116668.000000 02/07/17 10:55:49 104 1346101.0 WallClockCheckpoint 02/07/17 10:55:49 104 1346101.0 ShadowBday 02/07/17 10:55:49 ERROR \"Error: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction, recovery failed\" at line 1080 in file /builddir/build/BUILD/condor-8.4.8/src/condor_utils/classad_log.cpp This means /var/lib/condor-ce/spool/job_queue.log has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the Lines following corrupt log record... line. The most common culprit of the corruption is that the disk containing the job_queue.log has filled up. To avoid this problem, you can change the location of job_queue.log by setting JOB_QUEUE_LOG in /etc/condor-ce/config.d/ to a path, preferably one on a large SSD. JobRouterLog \u00b6 The HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to troubleshoot issues with job routing. Location: /var/log/condor-ce/JobRouterLog Key contents: Every attempt to route a job Routing success messages Job attribute changes, based on chosen route Job submission errors to an HTCondor batch system Corresponding job IDs on an HTCondor batch system Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: JOB_ROUTER_DEBUG = D_ALWAYS:2 D_CAT Apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig Known Errors \u00b6 (HTCondor batch systems only) If you see the following error message: Can't find address of schedd This means that HTCondor-CE cannot communicate with your HTCondor batch system. Verify that the condor service is running on the HTCondor-CE host and is configured for your central manager. (HTCondor batch systems only) If you see the following error message: JobRouter failure (src=2810466.0,dest=47968.0,route=MWT2_UCORE): giving up, because submitted job is still not in job queue mirror (submitted 605 seconds ago). Perhaps it has been removed? Ensure that condor_config_val SPOOL and condor_ce_config_val JOB_ROUTER_SCHEDD2_SPOOL return the same value. If they don't, change the value of JOB_ROUTER_SCHEDD2_SPOOL in your HTCondor-CE configuration to match SPOOL from your HTCondor configuration. If you have D_ALWAYS:2 turned on for the job router, you will see errors like the following: 06/12/15 14:00:28 HOOK_UPDATE_JOB_INFO not configured. You can safely ignore these. What to look for \u00b6 Job is considered for routing: 09/17/14 15:00:56 JobRouter (src=86.0,route=Local_LSF): found candidate job In parentheses are the original HTCondor-CE job ID (e.g., 86.0 ) and the route (e.g., Local_LSF ). Job is successfully routed: 09/17/14 15:00:57 JobRouter (src=86.0,route=Local_LSF): claimed job Finding the corresponding job ID on your HTCondor batch system: 09/17/14 15:00:57 JobRouter (src=86.0,dest=205.0,route=Local_Condor): claimed job In parentheses are the original HTCondor-CE job ID (e.g., 86.0 ) and the resultant job ID on the HTCondor batch system (e.g., 205.0 ) If your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the condor_ce_job_router_info HTCondor batch systems only : The following error occurs when the job router daemon cannot submit the routed job: 10/19/14 13:09:15 Can't resolve collector condorce.example.com; skipping 10/19/14 13:09:15 ERROR (pool condorce.example.com) Can't find address of schedd 10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job GridmanagerLog \u00b6 The HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. It contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. Details on how to read the Gridmanager log can be found on the HTCondor Wiki . Location: /var/log/condor-ce/GridmanagerLog.<JOB-OWNER> Key contents: Every attempt to submit a job to a batch system or other grid resource Status updates of submitted jobs Corresponding job IDs on non-HTCondor batch systems Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: MAX_GRIDMANAGER_LOG = 6h MAX_NUM_GRIDMANAGER_LOG = 8 GRIDMANAGER_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig What to look for \u00b6 Job is submitted to the batch system: 09/17/14 09:51:34 [12997] (85.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED Every state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)). Job status being updated: 09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE 09/17/14 15:07:24 [25543] GAHP[25563] <- 'BLAH_JOB_STATUS 3 lsf/20140917/482046' 09/17/14 15:07:24 [25543] GAHP[25563] -> 'S' 09/17/14 15:07:25 [25543] GAHP[25563] <- 'RESULTS' 09/17/14 15:07:25 [25543] GAHP[25563] -> 'R' 09/17/14 15:07:25 [25543] GAHP[25563] -> 'S' '1' 09/17/14 15:07:25 [25543] GAHP[25563] -> '3' '0' 'No Error' '4' '[ BatchjobId = \"482046\"; JobStatus = 4; ExitCode = 0; WorkerNode = \"atl-prod08\" ]' The first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here. Finding the corresponding job ID on your non-HTCondor batch system: 09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE 09/17/14 15:07:24 [25543] GAHP[25563] <- 'BLAH_JOB_STATUS 3 lsf/20140917/482046' On the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses, (87.0) . At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes, lsf/20140917/482046 . Job completion on the batch system: 09/17/14 15:07:25 [25543] (87.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE SharedPortLog \u00b6 The HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the collector. This log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found here . Location: /var/log/condor-ce/SharedPortLog Key contents: Every attempt to connect to HTCondor-CE (except collector queries) Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: SHARED_PORT_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig Messages log \u00b6 The messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. If there are issues with the authentication setup , the errors may appear here. Location: /var/log/messages Key contents: User authentication What to look for \u00b6 A user is mapped: Oct 6 10:35:32 osgserv06 htondor-ce-llgt[12147]: Callout to \"LCMAPS\" returned local user (service condor): \"osgglow01\" BLAHP Configuration File \u00b6 HTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client tools. Location: /etc/blah.config Key contents: Locations of the batch system's client binaries and logs Location to save files that are submitted to the local batch system You can also tell the BLAHP to save the files that are being submitted to the local batch system to <DIR-NAME> by adding the following line: blah_debug_save_submit_info=<DIR_NAME> The BLAHP will then create a directory with the format bl_* for each submission to the local jobmanager with the submit file and proxy used. Note Whitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within <DIR_NAME> . Getting Help \u00b6 If you are still experiencing issues after using this document, please let us know! Gather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.) Gather system information: root@host # osg-system-profiler Start a support request using a web interface or by email to help@opensciencegrid.org Describe issue and expected or desired behavior Include basic HTCondor-CE and related information Attach the osg-system-profiler output Reference \u00b6 Here are some other HTCondor-CE documents that might be helpful: HTCondor-CE overview and architecture Installing HTCondor-CE Configuring HTCondor-CE job routes Submitting jobs to HTCondor-CE","title":"Troubleshooting"},{"location":"troubleshooting/troubleshooting/#htcondor-ce-troubleshooting-guide","text":"In this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of common issues with suggested troubleshooting steps.","title":"HTCondor-CE Troubleshooting Guide"},{"location":"troubleshooting/troubleshooting/#known-issues","text":"","title":"Known Issues"},{"location":"troubleshooting/troubleshooting/#submit_attrs-are-not-applied-to-jobs-on-the-local-htcondor","text":"If you are adding attributes to jobs submitted to your HTCondor pool with SUBMIT_ATTRS , these will not be applied to jobs that are entering your pool from the HTCondor-CE. To get around this, you will want to add the attributes to your job routes . If the CE is the only entry point for jobs into your pool, you can get rid of SUBMIT_ATTRS on your backend. Otherwise, you will have to maintain your list of attributes both in your list of routes and in your SUBMIT_ATTRS .","title":"SUBMIT_ATTRS are not applied to jobs on the local HTCondor"},{"location":"troubleshooting/troubleshooting/#general-troubleshooting-items","text":"","title":"General Troubleshooting Items"},{"location":"troubleshooting/troubleshooting/#making-sure-packages-are-up-to-date","text":"It is important to make sure that the HTCondor-CE and related RPMs are up-to-date. root@host # yum update \"htcondor-ce*\" blahp condor If you just want to see the packages to update, but do not want to perform the update now, answer N at the prompt.","title":"Making sure packages are up-to-date"},{"location":"troubleshooting/troubleshooting/#verify-package-contents","text":"If the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the contents of your packages (ignoring changes to configuration files): user@host $ rpm -q --verify htcondor-ce htcondor-ce-client blahp | grep -v '/var/' | awk '$2 != \"c\" {print $0}' If the verification command returns output, this means that your packages have been changed. To fix this, you can reinstall the packages: user@host $ yum reinstall htcondor-ce htcondor-ce-client blahp Note The reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an .rpmnew suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration.","title":"Verify package contents"},{"location":"troubleshooting/troubleshooting/#verify-clocks-are-synchronized","text":"Like all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is synchronized using a utility such as ntpd . Additionally, HTCondor itself is sensitive to time skews on the NFS server. If you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew.","title":"Verify clocks are synchronized"},{"location":"troubleshooting/troubleshooting/#verify-host-cerificates-and-crls-are-valid","text":"An expired host certificate or CRLs will cause various issues with GSI authentication. Verify that your host certificate is valid by running: root@host # openssl x509 -in /etc/grid-security/hostcert.pem -noout -dates Likewise, run the fetch-crl script to update your CRLs: root@host # fetch-crl If updating CRLs fix your issues, make sure that the fetch-crl-cron and fetch-crl-boot services are enabled and running.","title":"Verify host cerificates and CRLs are valid"},{"location":"troubleshooting/troubleshooting/#htcondor-ce-troubleshooting-items","text":"This section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. Before troubleshooting, we recommend increasing the log level: Write the following into /etc/condor-ce/config.d/99-local.conf to increase the log level for all daemons: ALL_DEBUG = D_ALWAYS:2 D_CAT Ensure that the configuration is in place: root@host # condor_ce_reconfig Reproduce the issue Note Before spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running condor_ce_reconfig .","title":"HTCondor-CE Troubleshooting Items"},{"location":"troubleshooting/troubleshooting/#daemons-fail-to-start","text":"If there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to startup. Check the following subsections in order: Symptoms Daemon startup failure may manifest in many ways, the following are few symptoms of the problem. The service fails to start: root@host # service condor-ce start Starting Condor-CE daemons: [ FAIL ] condor_ce_q fails with a lengthy error message: user@host $ condor_ce_q Error: Extra Info: You probably saw this error because the condor_schedd is not running on the machine you are trying to query. If the condor_schedd is not running, the Condor system will not be able to find an address and port to connect to and satisfy this request. Please make sure the Condor daemons are running and try again. Extra Info: If the condor_schedd is running on the machine you are trying to query and you still see the error, the most likely cause is that you have setup a personal Condor, you have not defined SCHEDD_NAME in your condor_config file, and something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define either or both of those settings in your config file, or you must use the -name option to condor_q. Please see the Condor manual for details on SCHEDD_NAME and SCHEDD_ADDRESS_FILE. Next actions If the MasterLog is filled with ERROR:SECMAN...TCP connection to collector...failed : This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in this section of the install guide. If the MasterLog is filled with DC_AUTHENTICATE errors: The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in /etc/condor-ce/condor_mapfile . If the SchedLog is filled with Can\u2019t find address for negotiator : You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one.","title":"Daemons fail to start"},{"location":"troubleshooting/troubleshooting/#jobs-fail-to-submit-to-the-ce","text":"If a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the culprit, then you may have encountered an authentication or authorization issue. You may see error messages like the following in your SchedLog : 08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa) 08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189 08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done! Next actions Check voms-mapfile or grid-mapfile and ensure that the user's DN or VOMS attributes are known to your authentication method , and that the mapped users exist on your CE and cluster. Check for lcmaps errors in /var/log/messages If you do not see helpful error messages in /var/log/messages , adjust the debug level by adding export LCMAPS_DEBUG_LEVEL=5 to /etc/sysconfig/condor-ce , restarting the condor-ce service, and checking /var/log/messages for errors again.","title":"Jobs fail to submit to the CE"},{"location":"troubleshooting/troubleshooting/#jobs-stay-idle-on-the-ce","text":"Check the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is busy.","title":"Jobs stay idle on the CE"},{"location":"troubleshooting/troubleshooting/#idle-jobs-on-ce-make-sure-the-underlying-batch-system-can-run-jobs","text":"HTCondor-CE delegates jobs to your batch system, which is then responsible for matching jobs to worker nodes. If you cannot manually submit jobs (e.g., condor_submit , qsub ) on the CE host to your batch system, then HTCondor-CE won't be able to either. Procedure Manually create and submit a simple job (e.g., one that runs sleep ) Check for errors in the submission itself Watch the job in the batch system queue (e.g., condor_q , qstat ) If the job does not run, check for errors on the batch system Next actions Consult troubleshooting documentation or support avenues for your batch system. Once you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again.","title":"Idle jobs on CE: Make sure the underlying batch system can run jobs"},{"location":"troubleshooting/troubleshooting/#idle-jobs-on-ce-is-the-job-router-handling-the-incoming-job","text":"Jobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things if you suspect that the jobs are not being matched. Check if the JobRouter sees a job before that by looking at the job router log and looking for the text src=<JOB-ID>\u2026claimed job . Next actions Use condor_ce_job_router_info to see why your idle job does not match any routes","title":"Idle jobs on CE: Is the job router handling the incoming job?"},{"location":"troubleshooting/troubleshooting/#idle-jobs-on-ce-verify-correct-operation-between-the-ce-and-your-local-batch-system","text":"","title":"Idle jobs on CE: Verify correct operation between the CE and your local batch system"},{"location":"troubleshooting/troubleshooting/#for-htcondor-batch-systems","text":"HTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch system interaction will appear in the JobRouterLog . Next actions Check the JobRouterLog for failures. Verify that the local HTCondor is functional. Use condor_ce_config_val to verify that the JOB_ROUTER_SCHEDD2_NAME , JOB_ROUTER_SCHEDD2_POOL , and JOB_ROUTER_SCHEDD2_SPOOL configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively. Use condor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE and verify that it is set to .* .","title":"For HTCondor batch systems"},{"location":"troubleshooting/troubleshooting/#for-non-htcondor-batch-systems","text":"HTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch system interaction will appear in the GridmanagerLog . Look for gm state change\u2026 lines to figure out where the issures are occuring. Next actions If you see failures in the GridmanagerLog during job submission: Save the submit files by adding the appropriate entry to blah.config and submit it manually to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the <batch system>_binpath in /etc/blah.config . If you see failures in the GridmanagerLog during queries for job status: Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in /usr/libexec/blahp/<batch system>_status.sh (e.g., /usr/libexec/blahp/lsf_status.sh ) that take the argument batch system/YYYMMDD/job ID (e.g., lsf/20141008/65053 ). Run the appropriate status script for your batch system and upon success, you should see the following output: root@host # /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053 [ BatchjobId = \"894862\"; JobStatus = 4; ExitCode = 0; WorkerNode = \"atl-prod08\" ] If the script fails, request help from the OSG.","title":"For non-HTCondor batch systems"},{"location":"troubleshooting/troubleshooting/#idle-jobs-on-ce-verify-ability-to-change-permissions-on-key-files","text":"HTCondor-CE needs the ability to write and chown files in its spool directory and if it cannot, jobs will not run at all. Spool permission errors can appear in the SchedLog and the JobRouterLog . Symptoms 09/17/14 14:45:42 Error: Unable to chown '/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env' from 12345 to 54321 Next actions As root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions. Verify that there aren't any underlying file system issues in the specified location","title":"Idle jobs on CE: Verify ability to change permissions on key files"},{"location":"troubleshooting/troubleshooting/#jobs-stay-idle-on-a-remote-host-submitting-to-the-ce","text":"If you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not see a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not authorized to run there. Note that jobs may take several minutes or longer if the CE is busy.","title":"Jobs stay idle on a remote host submitting to the CE"},{"location":"troubleshooting/troubleshooting/#remote-idle-jobs-can-you-contact-the-ce","text":"To check basic connectivity to a CE, use condor_ce_ping : Symptoms user@host $ condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE ERROR: couldn't locate condorce.example.com! Next actions Make sure that the HTCondor-CE daemons are running with condor_ce_status . Verify that your CE is reachable from your submit host, replacing condorce.example.com with the hostname of your CE: user@host $ ping condorce.example.com","title":"Remote idle jobs: Can you contact the CE?"},{"location":"troubleshooting/troubleshooting/#remote-idle-jobs-are-you-authorized-to-run-jobs-on-the-ce","text":"The CE will only run jobs from users that authenticate through the HTCondor-CE configuration . You can use condor_ce_ping to check if you are authorized and what user your proxy is being mapped to. Symptoms user@host $ condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE Remote Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Local Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Session ID: condorce:3343:1412790611:0 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: GSI Remote Mapping: gsi@unmapped Authorized: FALSE Notice the failures in the above message: Remote Mapping: gsi@unmapped and Authorized: FALSE Next actions Verify that an authentication method is set up on the CE Verify that your user DN is mapped to an existing system user","title":"Remote idle jobs: Are you authorized to run jobs on the CE?"},{"location":"troubleshooting/troubleshooting/#jobs-go-on-hold","text":"Jobs will be put on held with a HoldReason attribute that can be inspected with condor_ce_q : user@host $ condor_ce_q -l <JOB-ID> -attr HoldReason HoldReason = \"CE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to no matching routes, route job limit, or route failure threshold.\"","title":"Jobs go on hold"},{"location":"troubleshooting/troubleshooting/#held-jobs-no-matching-routes-route-job-limit-or-route-failure-threshold","text":"Jobs on the CE will be put on hold if they are not claimed by the job router within 30 minutes. The most common cases for this behavior are as follows: The job does not match any job routes: use condor_ce_job_router_info to see why your idle job does not match any routes . The route(s) that the job matches to are full: See limiting the number of jobs . The job router is throttling submission to your batch system due to submission failures: See the HTCondor manual for FailureRateThreshold . Check for errors in the JobRouterLog or GridmanagerLog for HTCondor and non-HTCondor batch systems, respectively.","title":"Held jobs: no matching routes, route job limit, or route failure threshold"},{"location":"troubleshooting/troubleshooting/#held-jobs-missingexpired-user-proxy","text":"HTCondor-CE requires a valid user proxy for each job that is submitted. You can check the status of your proxy with the following user@host $ voms-proxy-info -all Next actions Ensure that the owner of the job generates their proxy with voms-proxy-init .","title":"Held jobs: Missing/expired user proxy"},{"location":"troubleshooting/troubleshooting/#held-jobs-invalid-job-universe","text":"The HTCondor-CE only accepts jobs that have universe in their submit files set to vanilla , standard , local , or scheduler . These universes also have corresponding integer values that can be found in the HTCondor manual . Next actions Ensure jobs submitted locally, from the CE host, are submitted with universe = vanilla Ensure jobs submitted from a remote submit point are submitted with: universe = grid grid_resource = condor condorce.example.com condorce.example.com:9619 replacing condorce.example.com with the hostname of the CE.","title":"Held jobs: Invalid job universe"},{"location":"troubleshooting/troubleshooting/#identifying-the-corresponding-job-id-on-the-local-batch-system","text":"When troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID and the resultant job ID on the batch system. The methods for finding the resultant job ID differs between batch systems.","title":"Identifying the corresponding job ID on the local batch system"},{"location":"troubleshooting/troubleshooting/#htcondor-batch-systems","text":"To inspect the CE\u2019s job ad, use condor_ce_q or condor_ce_history : Use condor_ce_q if the job is still in the CE\u2019s queue: user@host $ condor_ce_q <JOB-ID> -af RoutedToJobId Use condor_ce_history if the job has left the CE\u2019s queue: user@host $ condor_ce_history <JOB-ID> -af RoutedToJobId Parse the JobRouterLog for the CE\u2019s job ID.","title":"HTCondor batch systems"},{"location":"troubleshooting/troubleshooting/#non-htcondor-batch-systems","text":"When HTCondor-CE records the corresponding batch system job ID, it is written in the form <BATCH-SYSTEM>/<DATE>/<JOB ID> : lsf/20141206/482046 To inspect the CE\u2019s job ad, use condor_ce_q : user@host $ condor_ce_q <JOB-ID> -af GridJobId Parse the GridmanagerLog for the CE\u2019s job ID.","title":"Non-HTCondor batch systems"},{"location":"troubleshooting/troubleshooting/#jobs-removed-from-the-local-htcondor-pool-become-resubmitted-htcondor-batch-systems-only","text":"By design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. Therefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps: Identify the misbehaving job ID in your batch system queue Find the job's corresponding CE job ID: user@host $ condor_q <JOB-ID> -af RoutedFromJobId Use condor_ce_rm to remove the CE job from the queue","title":"Jobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only)"},{"location":"troubleshooting/troubleshooting/#missing-htcondor-tools","text":"Most of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. If you are trying to use HTCondor-CE tools and you see the following error: user@host $ condor_ce_job_router_info /usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found This means that the condor_job_router_info (note this is not the CE version), is not in your PATH . Next Actions Either the condor RPM is missing or there are some other issues with it (try rpm --verify condor ). You have installed HTCondor in a non-standard location that is not in your PATH . The condor_job_router_info tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming).","title":"Missing HTCondor tools"},{"location":"troubleshooting/troubleshooting/#htcondor-ce-troubleshooting-tools","text":"HTCondor-CE has its own separate set of of the HTCondor tools with ce in the name (i.e., condor_ce_submit vs condor_submit ). Some of the the commands are only for the CE (e.g., condor_ce_run and condor_ce_trace ) but many of them are just HTCondor commands configured to interact with the CE (e.g., condor_ce_q , condor_ce_status ). It is important to differentiate the two: condor_ce_config_val will provide configuration values for your HTCondor-CE while condor_config_val will provide configuration values for your HTCondor batch system. If you are not running an HTCondor batch system, the non-CE commands will return errors.","title":"HTCondor-CE Troubleshooting Tools"},{"location":"troubleshooting/troubleshooting/#condor_ce_trace","text":"","title":"condor_ce_trace"},{"location":"troubleshooting/troubleshooting/#usage","text":"condor_ce_trace is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job. Note You must have generated a proxy (e.g., voms-proxy-init ) and your DN must be added to your chosen authentication method . user@host $ condor_ce_trace condorce.example.com Replacing the condorce.example.com with the hostname of the CE. If you are familiar with the output of condor commands, the command also takes a --debug option that displays verbose condor output.","title":"Usage"},{"location":"troubleshooting/troubleshooting/#troubleshooting","text":"If the command fails with \u201cFailed ping\u2026\u201d: Make sure that the HTCondor-CE daemons are running on the CE If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line: Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our installation document . If the job submits but does not complete: Look at the status of the job and perform the relevant troubleshooting steps .","title":"Troubleshooting"},{"location":"troubleshooting/troubleshooting/#condor_ce_host_network_check","text":"","title":"condor_ce_host_network_check"},{"location":"troubleshooting/troubleshooting/#usage_1","text":"condor_ce_host_network_check is a tool for testing an HTCondor-CE's networking configuration: root@host # condor_ce_host_network_check Starting analysis of host networking for HTCondor-CE System hostname: fermicloud360.fnal.gov FQDN matches hostname Forward resolution of hostname fermicloud360.fnal.gov is 131.225.155.96. Backward resolution of IPv4 131.225.155.96 is fermicloud360.fnal.gov. Forward and backward resolution match! HTCondor is considering all network interfaces and addresses. HTCondor would pick address of 131.225.155.96 as primary address. HTCondor primary address 131.225.155.96 matches system preferred address. Host network configuration should work with HTCondor-CE","title":"Usage"},{"location":"troubleshooting/troubleshooting/#troubleshooting_1","text":"If the tool reports that Host network configuration not expected to work with HTCondor-CE , ensure that forward and reverse DNS resolution return the public IP and hostname.","title":"Troubleshooting"},{"location":"troubleshooting/troubleshooting/#condor_ce_run","text":"","title":"condor_ce_run"},{"location":"troubleshooting/troubleshooting/#usage_2","text":"Similar to globus-job-run , condor_ce_run is a tool that submits a simple job to your CE, so it is useful for quickly submitting jobs through your CE. To submit a job to the CE and run the env command on the remote batch system: Note You must have generated a proxy (e.g., voms-proxy-init ) and your DN must be added to your chosen authentication method . user@host $ condor_ce_run -r condorce.example.com:9619 /bin/env Replacing the condorce.example.com with the hostname of the CE. If you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you can run commands locally on the CE with condor_ce_run with the -l option. The following example outputs the JobRouterLog of the CE in question: user@host $ condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog Replacing the condorce.example.com text with the hostname of the CE. To disable this feature on your CE, consult this section of the install documentation.","title":"Usage"},{"location":"troubleshooting/troubleshooting/#troubleshooting_2","text":"If you do not see any results: condor_ce_run does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use condor_ce_q in a separate terminal to track the job on the CE. If you never see any results, use condor_ce_trace to pinpoint errors. If you see an error message that begins with \u201cFailed to\u2026\u201d: Check connectivity to the CE with condor_ce_trace or condor_ce_ping","title":"Troubleshooting"},{"location":"troubleshooting/troubleshooting/#condor_ce_submit","text":"See this documentation for details","title":"condor_ce_submit"},{"location":"troubleshooting/troubleshooting/#condor_ce_ping","text":"","title":"condor_ce_ping"},{"location":"troubleshooting/troubleshooting/#usage_3","text":"Use the following condor_ce_ping command to test your ability to submit jobs to an HTCondor-CE, replacing condorce.example.com with the hostname of your CE: user@host $ condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE The following shows successful output where I am able to submit jobs ( Authorized: TRUE ) as the glow user ( Remote Mapping: glow@users.opensciencegrid.org ): Remote Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Local Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Session ID: condorce:27407:1412286981:3 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: GSI Remote Mapping: glow@users.opensciencegrid.org Authorized: TRUE Note If you run the condor_ce_ping command on the CE that you are testing, omit the -name and -pool options. condor_ce_ping takes the same arguments as condor_ping and is documented in the HTCondor manual .","title":"Usage"},{"location":"troubleshooting/troubleshooting/#troubleshooting_3","text":"If you see \u201cERROR: couldn\u2019t locate (null)\u201d , that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE: MASTER_DEBUG = D_ALWAYS:2 D_CAT SCHEDD_DEBUG = D_ALWAYS:2 D_CAT Then look in the MasterLog and SchedLog for any errors. If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line , this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our installation document .","title":"Troubleshooting"},{"location":"troubleshooting/troubleshooting/#condor_ce_q","text":"","title":"condor_ce_q"},{"location":"troubleshooting/troubleshooting/#usage_4","text":"condor_ce_q can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 To inspect the full ClassAd for a specific job, specify the -l flag and the job ID: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l <JOB-ID> Note If you run the condor_ce_q command on the CE that you are testing, omit the -name and -pool options. condor_ce_q takes the same arguments as condor_q and is documented in the HTCondor manual .","title":"Usage"},{"location":"troubleshooting/troubleshooting/#troubleshooting_4","text":"If the jobs that you are submiting to a CE are not completing, condor_ce_q can tell you the status of your jobs. If the schedd is not running: You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with: MASTER_DEBUG = D_ALWAYS:2 D_CAT SCHEDD_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig Then look in the MasterLog and SchedLog on the CE for any errors. If there are issues with contacting the collector: You will see the following message: user@host $ condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu -- Failed to fetch ads from: <129.59.197.223:9620?sock`33630_8b33_4> : ce1.accre.vanderbilt.edu This may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the ALLOW_READ configuration value is not set: user@host $ condor_ce_config_val -v ALLOW_READ Not defined: ALLOW_READ If it is defined, remove it from the file that is returned in the output. If a job is held: There should be an accompanying HoldReason that will tell you why it is being held. The HoldReason is in the job\u2019s ClassAd, so you can use the long form of condor_ce_q to extract its value: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l <Job ID> | grep HoldReason If a job is idle: The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the condor_ce_job_router_info .","title":"Troubleshooting"},{"location":"troubleshooting/troubleshooting/#condor_ce_history","text":"","title":"condor_ce_history"},{"location":"troubleshooting/troubleshooting/#usage_5","text":"condor_ce_history can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE: user@host $ condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 To inspect the full ClassAd for a specific job, specify the -l flag and the job ID: user@host $ condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l <Job ID> Note If you run the condor_ce_history command on the CE that you are testing, omit the -name and -pool options. condor_ce_history takes the same arguments as condor_history and is documented in the HTCondor manual .","title":"Usage"},{"location":"troubleshooting/troubleshooting/#condor_ce_job_router_info","text":"","title":"condor_ce_job_router_info"},{"location":"troubleshooting/troubleshooting/#usage_6","text":"Use the condor_ce_job_router_info command to help troubleshoot your routes and how jobs will match to them. To see all of your routes (the output is long because it combines your routes with the JOB_ROUTER_DEFAULTS configuration variable): root@host # condor_ce_job_router_info -config To see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of condor_ce_q (replace the <JOB-ID> with the job ID that you are interested in): root@host # condor_ce_q -l <JOB-ID> | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - To inspect a job that has already left the queue, use condor_ce_history instead of condor_ce_q : root@host # condor_ce_history -l <JOB-ID> | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - Note If the proxy for the job has expired, the job will not match any routes. To work around this constraint: root@host # condor_ce_history -l <JOB-ID> | sed \"s/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date '+1 sec'`/\" | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - Alternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file: root@host # condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads <JOBAD-FILE>","title":"Usage"},{"location":"troubleshooting/troubleshooting/#troubleshooting_5","text":"If the job does not match any route: You can identify this case when you see 0 candidate jobs found in the condor_job_router_info output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to true . When troubleshooting, look at all of the expressions prior to the target.ProcId >= 0 expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again. If your job matches more than one route: the tool will tell you by showing all matching routes after the job ID: Checking Job src=162,0 against all routes Route Matches: Local_PBS Route Matches: Condor_Test To troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is highlighted below: Umbrella constraint: ((target.x509userproxysubject =!= UNDEFINED) && (target.x509UserProxyExpiration =!= UNDEFINED) && (time() < target.x509UserProxyExpiration) && (target.JobUniverse =?= 5 || target.JobUniverse =?= 1)) && ( (target.osgTestPBS is true) || (true) ) && (target.ProcId >= 0 && target.JobStatus == 1 && (target.StageInStart is undefined || target.StageInFinish isnt undefined) && target.Managed isnt \"ScheddDone\" && target.Managed isnt \"Extenal\" && target.Owner isnt Undefined && target.RoutedBy isnt \"htcondor-ce\") Both routes evaluate to true for the job\u2019s ClassAd because it contained osgTestPBS = true . Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the job route configuration page for more details. If it is unclear why jobs are matching a route: wrap the route's requirements expression in debug() and check the JobRouterLog for more information.","title":"Troubleshooting"},{"location":"troubleshooting/troubleshooting/#condor_ce_router_q","text":"","title":"condor_ce_router_q"},{"location":"troubleshooting/troubleshooting/#usage_7","text":"If you have multiple job routes and many jobs, condor_ce_router_q is a useful tool to see how jobs are being routed and their statuses: user@host $ condor_ce_router_q condor_ce_router_q takes the same options as condor_router_q and condor_q and is documented in the HTCondor manual","title":"Usage"},{"location":"troubleshooting/troubleshooting/#condor_ce_status","text":"","title":"condor_ce_status"},{"location":"troubleshooting/troubleshooting/#usage_8","text":"To see the daemons running on a CE, run the following command: user@host $ condor_ce_status -any condor_ce_status takes the same arguments as condor_status , which are documented in the HTCondor manual . \"Missing\" Worker Nodes An HTCondor-CE will not show any worker nodes (e.g. Machine entries in the condor_ce_status -any output) if it does not have any running GlideinWMS pilot jobs. This is expected since HTCondor-CE only forwards incoming pilot jobs to your batch system and does not match jobs to worker nodes.","title":"Usage"},{"location":"troubleshooting/troubleshooting/#troubleshooting_6","text":"If the output of condor_ce_status -any does not show at least the following daemons: Collector Scheduler DaemonMaster Job_Router Increase the debug level and consult the HTCondor-CE logs for errors.","title":"Troubleshooting"},{"location":"troubleshooting/troubleshooting/#condor_ce_config_val","text":"","title":"condor_ce_config_val"},{"location":"troubleshooting/troubleshooting/#usage_9","text":"To see the value of configuration variables and where they are set, use condor_ce_config_val . Primarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. To see the value of a single variable and where it is set: user@host $ condor_ce_config_val -v <CONFIGURATION-VARIABLE> To see a list of all configuration variables and their values: user@host $ condor_ce_config_val -dump To see a list of all the files that are used to create your configuration and the order that they are parsed, use the following command: user@host $ condor_ce_config_val -config condor_ce_config_val takes the same arguments as condor_config_val and is documented in the HTCondor manual .","title":"Usage"},{"location":"troubleshooting/troubleshooting/#condor_ce_reconfig","text":"","title":"condor_ce_reconfig"},{"location":"troubleshooting/troubleshooting/#usage_10","text":"To ensure that your configuration changes have taken effect, run condor_ce_reconfig . user@host $ condor_ce_reconfig","title":"Usage"},{"location":"troubleshooting/troubleshooting/#condor_ce_onoffrestart","text":"","title":"condor_ce_{on,off,restart}"},{"location":"troubleshooting/troubleshooting/#usage_11","text":"To turn on/off/restart HTCondor-CE daemons, use the following commands: root@host # condor_ce_on root@host # condor_ce_off root@host # condor_ce_restart The HTCondor-CE service uses the previous commands with default values. Using these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart: If you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command: root@host # condor_ce_restart -fast This will cause HTCondor-CE to restart and quickly reconnect to all running jobs. If you need to stop running new jobs, run the following: root@host # condor_ce_off -peaceful This will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down.","title":"Usage"},{"location":"troubleshooting/troubleshooting/#htcondor-ce-troubleshooting-data","text":"The following files are located on the CE host.","title":"HTCondor-CE Troubleshooting Data"},{"location":"troubleshooting/troubleshooting/#masterlog","text":"The HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if they fail to start. Location: /var/log/condor-ce/MasterLog Key contents: Start-up, shut-down, and communication with other HTCondor daemons Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: MASTER_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"MasterLog"},{"location":"troubleshooting/troubleshooting/#what-to-look-for","text":"Successful daemon start-up. The following line shows that the Collector daemon started successfully: 10/07/14 14:20:27 Started DaemonCore process \"/usr/sbin/condor_collector -f -port 9619\", pid and pgroup = 7318","title":"What to look for:"},{"location":"troubleshooting/troubleshooting/#schedlog","text":"The HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. It contains valuable information when trying to troubleshoot authentication issues. Location: /var/log/condor-ce/SchedLog Key contents: Every job submitted to the CE User authorization events Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: SCHEDD_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"SchedLog"},{"location":"troubleshooting/troubleshooting/#what-to-look-for_1","text":"Job is submitted to the CE queue: 10/07/14 16:52:17 Submitting new job 234.0 In this example, the ID of the submitted job is 234.0 . Job owner is authorized and mapped: 10/07/14 16:52:17 Command=QMGMT_WRITE_CMD, peer=<131.225.154.68:42262> 10/07/14 16:52:17 AuthMethod=GSI, AuthId=/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Lin 1047, /GLOW/Role=NULL/Capability=NULL, <CondorId=glow@users.opensciencegrid.org> In this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the glow user. User job submission fails due to improper authentication or authorization: 08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa) 08/30/16 16:53:12 PERMISSION DENIED to <gsi@unmapped> from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189 08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done Missing negotiator: 10/18/14 17:32:21 Can't find address for negotiator 10/18/14 17:32:21 Failed to send RESCHEDULE to unknown daemon: Since HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes: 06/23/15 11:15:03 Number of Active Workers 0 Corrupted job_queue.log : 02/07/17 10:55:49 WARNING: Encountered corrupt log record _654 (byte offset 5046225) 02/07/17 10:55:49 103 1354325.0 PeriodicRemove ( StageInFinish > 0 ) 105 02/07/17 10:55:49 Lines following corrupt log record _654 (up to 3): 02/07/17 10:55:49 103 1346101.0 RemoteWallClockTime 116668.000000 02/07/17 10:55:49 104 1346101.0 WallClockCheckpoint 02/07/17 10:55:49 104 1346101.0 ShadowBday 02/07/17 10:55:49 ERROR \"Error: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction, recovery failed\" at line 1080 in file /builddir/build/BUILD/condor-8.4.8/src/condor_utils/classad_log.cpp This means /var/lib/condor-ce/spool/job_queue.log has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the Lines following corrupt log record... line. The most common culprit of the corruption is that the disk containing the job_queue.log has filled up. To avoid this problem, you can change the location of job_queue.log by setting JOB_QUEUE_LOG in /etc/condor-ce/config.d/ to a path, preferably one on a large SSD.","title":"What to look for"},{"location":"troubleshooting/troubleshooting/#jobrouterlog","text":"The HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to troubleshoot issues with job routing. Location: /var/log/condor-ce/JobRouterLog Key contents: Every attempt to route a job Routing success messages Job attribute changes, based on chosen route Job submission errors to an HTCondor batch system Corresponding job IDs on an HTCondor batch system Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: JOB_ROUTER_DEBUG = D_ALWAYS:2 D_CAT Apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"JobRouterLog"},{"location":"troubleshooting/troubleshooting/#known-errors","text":"(HTCondor batch systems only) If you see the following error message: Can't find address of schedd This means that HTCondor-CE cannot communicate with your HTCondor batch system. Verify that the condor service is running on the HTCondor-CE host and is configured for your central manager. (HTCondor batch systems only) If you see the following error message: JobRouter failure (src=2810466.0,dest=47968.0,route=MWT2_UCORE): giving up, because submitted job is still not in job queue mirror (submitted 605 seconds ago). Perhaps it has been removed? Ensure that condor_config_val SPOOL and condor_ce_config_val JOB_ROUTER_SCHEDD2_SPOOL return the same value. If they don't, change the value of JOB_ROUTER_SCHEDD2_SPOOL in your HTCondor-CE configuration to match SPOOL from your HTCondor configuration. If you have D_ALWAYS:2 turned on for the job router, you will see errors like the following: 06/12/15 14:00:28 HOOK_UPDATE_JOB_INFO not configured. You can safely ignore these.","title":"Known Errors"},{"location":"troubleshooting/troubleshooting/#what-to-look-for_2","text":"Job is considered for routing: 09/17/14 15:00:56 JobRouter (src=86.0,route=Local_LSF): found candidate job In parentheses are the original HTCondor-CE job ID (e.g., 86.0 ) and the route (e.g., Local_LSF ). Job is successfully routed: 09/17/14 15:00:57 JobRouter (src=86.0,route=Local_LSF): claimed job Finding the corresponding job ID on your HTCondor batch system: 09/17/14 15:00:57 JobRouter (src=86.0,dest=205.0,route=Local_Condor): claimed job In parentheses are the original HTCondor-CE job ID (e.g., 86.0 ) and the resultant job ID on the HTCondor batch system (e.g., 205.0 ) If your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the condor_ce_job_router_info HTCondor batch systems only : The following error occurs when the job router daemon cannot submit the routed job: 10/19/14 13:09:15 Can't resolve collector condorce.example.com; skipping 10/19/14 13:09:15 ERROR (pool condorce.example.com) Can't find address of schedd 10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job","title":"What to look for"},{"location":"troubleshooting/troubleshooting/#gridmanagerlog","text":"The HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. It contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. Details on how to read the Gridmanager log can be found on the HTCondor Wiki . Location: /var/log/condor-ce/GridmanagerLog.<JOB-OWNER> Key contents: Every attempt to submit a job to a batch system or other grid resource Status updates of submitted jobs Corresponding job IDs on non-HTCondor batch systems Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: MAX_GRIDMANAGER_LOG = 6h MAX_NUM_GRIDMANAGER_LOG = 8 GRIDMANAGER_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"GridmanagerLog"},{"location":"troubleshooting/troubleshooting/#what-to-look-for_3","text":"Job is submitted to the batch system: 09/17/14 09:51:34 [12997] (85.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED Every state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)). Job status being updated: 09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE 09/17/14 15:07:24 [25543] GAHP[25563] <- 'BLAH_JOB_STATUS 3 lsf/20140917/482046' 09/17/14 15:07:24 [25543] GAHP[25563] -> 'S' 09/17/14 15:07:25 [25543] GAHP[25563] <- 'RESULTS' 09/17/14 15:07:25 [25543] GAHP[25563] -> 'R' 09/17/14 15:07:25 [25543] GAHP[25563] -> 'S' '1' 09/17/14 15:07:25 [25543] GAHP[25563] -> '3' '0' 'No Error' '4' '[ BatchjobId = \"482046\"; JobStatus = 4; ExitCode = 0; WorkerNode = \"atl-prod08\" ]' The first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here. Finding the corresponding job ID on your non-HTCondor batch system: 09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE 09/17/14 15:07:24 [25543] GAHP[25563] <- 'BLAH_JOB_STATUS 3 lsf/20140917/482046' On the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses, (87.0) . At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes, lsf/20140917/482046 . Job completion on the batch system: 09/17/14 15:07:25 [25543] (87.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE","title":"What to look for"},{"location":"troubleshooting/troubleshooting/#sharedportlog","text":"The HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the collector. This log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found here . Location: /var/log/condor-ce/SharedPortLog Key contents: Every attempt to connect to HTCondor-CE (except collector queries) Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: SHARED_PORT_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"SharedPortLog"},{"location":"troubleshooting/troubleshooting/#messages-log","text":"The messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. If there are issues with the authentication setup , the errors may appear here. Location: /var/log/messages Key contents: User authentication","title":"Messages log"},{"location":"troubleshooting/troubleshooting/#what-to-look-for_4","text":"A user is mapped: Oct 6 10:35:32 osgserv06 htondor-ce-llgt[12147]: Callout to \"LCMAPS\" returned local user (service condor): \"osgglow01\"","title":"What to look for"},{"location":"troubleshooting/troubleshooting/#blahp-configuration-file","text":"HTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client tools. Location: /etc/blah.config Key contents: Locations of the batch system's client binaries and logs Location to save files that are submitted to the local batch system You can also tell the BLAHP to save the files that are being submitted to the local batch system to <DIR-NAME> by adding the following line: blah_debug_save_submit_info=<DIR_NAME> The BLAHP will then create a directory with the format bl_* for each submission to the local jobmanager with the submit file and proxy used. Note Whitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within <DIR_NAME> .","title":"BLAHP Configuration File"},{"location":"troubleshooting/troubleshooting/#getting-help","text":"If you are still experiencing issues after using this document, please let us know! Gather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.) Gather system information: root@host # osg-system-profiler Start a support request using a web interface or by email to help@opensciencegrid.org Describe issue and expected or desired behavior Include basic HTCondor-CE and related information Attach the osg-system-profiler output","title":"Getting Help"},{"location":"troubleshooting/troubleshooting/#reference","text":"Here are some other HTCondor-CE documents that might be helpful: HTCondor-CE overview and architecture Installing HTCondor-CE Configuring HTCondor-CE job routes Submitting jobs to HTCondor-CE","title":"Reference"},{"location":"v3/","text":"HTCondor-CE 3 \u00b6 The HTCondor-CE software is a Compute Entrypoint (CE) based on HTCondor for sites that are part of a larger computing grid (e.g. European Grid Infrastructure , Open Science Grid ). As such, HTCondor-CE serves as a \"door\" for incoming resource allocation requests \u2014 it handles authorization and delegation of these requests to a grid site's local batch system. Supported batch systems include Grid Engine , HTCondor , LSF , PBS Pro / Torque , and Slurm . For an introduction to HTCondor-CE, watch our recorded webinar from the EGI Community Webinar Programme: Or visit the overview page for more details on the features and architecture of HTCondor-CE. Contact Us \u00b6 HTCondor-CE is developed and maintained by the Center for High Throughput Computing . If you have questions or issues regarding HTCondor-CE, please see the HTCondor support page for how to contact us.","title":"Home"},{"location":"v3/#htcondor-ce-3","text":"The HTCondor-CE software is a Compute Entrypoint (CE) based on HTCondor for sites that are part of a larger computing grid (e.g. European Grid Infrastructure , Open Science Grid ). As such, HTCondor-CE serves as a \"door\" for incoming resource allocation requests \u2014 it handles authorization and delegation of these requests to a grid site's local batch system. Supported batch systems include Grid Engine , HTCondor , LSF , PBS Pro / Torque , and Slurm . For an introduction to HTCondor-CE, watch our recorded webinar from the EGI Community Webinar Programme: Or visit the overview page for more details on the features and architecture of HTCondor-CE.","title":"HTCondor-CE 3"},{"location":"v3/#contact-us","text":"HTCondor-CE is developed and maintained by the Center for High Throughput Computing . If you have questions or issues regarding HTCondor-CE, please see the HTCondor support page for how to contact us.","title":"Contact Us"},{"location":"v3/batch-system-integration/","text":"Writing Routes For HTCondor-CE \u00b6 The JobRouter is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in /etc/condor-ce/config.d/02-ce-*.conf that provide enough basic functionality for a small site. If you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems. Definitions Incoming Job : A job which was submitted to the CE from an external source. Routed Job : A job that has been transformed by the JobRouter. Quirks and Pitfalls \u00b6 If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break. Make sure to run condor_ce_reconfig after changing your routes, otherwise they will not take effect. HTCondor batch system only: Local universe jobs are excluded from any routing. How Job Routes are Constructed \u00b6 Each job route\u2019s ClassAd is constructed by combining each entry from the JOB_ROUTER_ENTRIES with the JOB_ROUTER_DEFAULTS . Attributes that are set_* in JOB_ROUTER_ENTRIES will override those set_* in JOB_ROUTER_DEFAULTS JOB_ROUTER_ENTRIES \u00b6 JOB_ROUTER_ENTRIES is a configuration variable whose default is set in /etc/condor-ce/config.d/02-ce-*.conf but may be overriden by the administrator in /etc/condor-ce/config.d/99-local.conf . This document outlines the many changes you can make to JOB_ROUTER_ENTRIES to fit your site\u2019s needs. JOB_ROUTER_DEFAULTS \u00b6 JOB_ROUTER_DEFAULTS is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents in a readable format, run the following command: user@host $ condor_ce_config_val JOB_ROUTER_DEFAULTS | sed 's/;/;\\n/g' Warning If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break. Warning Do not set the JOB_ROUTER_DEFAULTS configuration variable yourself. This will cause the CE to stop functioning. How Jobs Match to Job Routes \u00b6 The job router considers jobs in the queue ( condor_ce_q ) that meet the following constraints: The job has not already been considered by the job router The job is associated with an unexpired x509 proxy The job's universe is standard or vanilla If the job meets the above constraints, then the job's ClassAd is compared against each route's requirements . If the job only meets one route's requirements, the job is matched to that route. If the job meets the requirements of multiple routes, the route that is chosen depends on your version of HTCondor ( condor_version ): If your version of HTCondor is... Then the route is chosen by... < 8.7.1 Round-robin between all matching routes. In this case, we recommend making each route's requirements mutually exclusive. >= 8.7.1, < 8.8.7 First matching route where routes are considered in hash-table order. In this case, we recommend making each route's requirements mutually exclusive. >= 8.8.7 First matching route where routes are considered in the order specified by JOB_ROUTER_ROUTE_NAMES Job Route Order For HTCondor versions < 8.8.7 (as well as versions >= 8.9.0 and < 8.9.5) the order of job routes does not match the order in which they are configured. As a result, we recommend updating to HTCondor 8.8.7 (or 8.9.5) and specifying the names of your routes in JOB_ROUTER_ROUTE_NAMES in the order that you'd like them considered. If you are using HTCondor >= 8.7.1 and would like to use round-robin matching, add the following text to a file in /etc/condor-ce/config.d/ : JOB_ROUTER_ROUND_ROBIN_SELECTION = True Generic Routes \u00b6 This section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in /etc/condor-ce/config.d/99-local.conf , not the original 02-ce-*.conf . Required fields \u00b6 The minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in /usr/share/condor-ce/config.d/02-ce-<batch system>-defaults.conf , provided by the htcondor-ce-<batch system> packages. Batch system \u00b6 Each route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the TargetUniverse attribute needs to be set to 5 or \"vanilla\" . For all other batch systems, the TargetUniverse attribute needs to be set to 9 or \"grid\" and the GridResource attribute needs to be set to \"batch <batch system>\" (where <batch system> can be one of pbs , slurm , lsf , or sge ). JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Route jobs to PBS\"; ] @jre Route name \u00b6 To identify routes, you will need to assign a name to the route with the name attribute: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; ] @jre The name of the route will be useful in debugging since it shows up in the output of condor_ce_job_router_info , the JobRouterLog , and in the ClassAd of the routed job, which can be viewed with condor_ce_q or condor_ce_history . Writing multiple routes \u00b6 Note Before writing multiple routes, consider the details of how jobs match to job routes If your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues), you will need to write multiple job routes where each route is enclosed by square brackets. The following routes takes incoming jobs that have a queue attribute set to \"analy\" and routes them to the site's HTCondor batch system. Any other jobs will be sent to that site's PBS batch system. JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; Requirements = (TARGET.queue =?= \"analy\"); ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Route jobs to PBS\"; Requirements = (TARGET.queue =!= \"analy\"); ] @jre Writing comments \u00b6 To write comments you can use # to comment a line: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"# comments\"; # This is a comment ] @jre Setting attributes for all routes \u00b6 To set an attribute that will be applied to all routes, you will need to ensure that MERGE_JOB_ROUTER_DEFAULT_ADS is set to True (check the value with condor_ce_config_val ) and use the set_ function in the JOB_ROUTER_DEFAULTS . The following configuration sets the Periodic_Hold attribute for all routes: # Use the defaults generated by the condor_ce_router_defaults script. To add # additional defaults, add additional lines of the form: # # JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;] # MERGE_JOB_ROUTER_DEFAULT_ADS=True JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1;] Filtering jobs based on\u2026 \u00b6 To filter jobs, use the Requirements attribute. Jobs will evaluate against the ClassAd expression set in the Requirements and if the expression evaluates to TRUE , the route will match. More information on the syntax of ClassAd's can be found in the HTCondor manual . For an example on how incoming jobs interact with filtering in job routes, consult this document . When setting requirements, you need to prefix job attributes that you are filtering with TARGET. so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a queue = \"analy\" attribute, then the following job route will not match: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by queue\"; queue = \"not-analy\"; Requirements = (queue =?= \"analy\"); ] @jre This is because when evaluating the route requirement, the job route will compare its own queue attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the HTCondor manual . Note If you have an HTCondor batch system, note the difference with set_requirements . Note Before writing multiple routes, consider the details of how jobs match to job routes . Glidein queue \u00b6 To filter jobs based on their glidein queue attribute, your routes will need a Requirements expression using the incoming job's queue attribute. The following entry routes jobs to HTCondor if the incoming job (specified by TARGET ) is an analy (Analysis) glidein: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by queue\"; Requirements = (TARGET.queue =?= \"analy\"); ] @jre Job submitter \u00b6 To filter jobs based on who submitted it, your routes will need a Requirements expression using the incoming job's Owner attribute. The following entry routes jobs to the HTCondor batch system if the submitter is usatlas2 : JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by job submitter\"; Requirements = (TARGET.Owner =?= \"usatlas2\"); ] @jre Alternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system if the submitter's name begins with usatlas : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Filtering by job submitter (regular expression)\"; Requirements = regexp(\"^usatlas\", TARGET.Owner); ] @jre VOMS attribute \u00b6 To filter jobs based on the subject of the job's proxy, your routes will need a Requirements expression using the incoming job's x509UserProxyFirstFQAN attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains /cms/Role=Pilot : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Filtering by VOMS attribute (regex)\"; Requirements = regexp(\"\\/cms\\/Role\\=pilot\", TARGET.x509UserProxyFirstFQAN); ] @jre Setting a default\u2026 \u00b6 This section outlines how to set default job limits, memory, cores, and maximum walltime. Maximum number of jobs \u00b6 To set a default limit to the maximum number of jobs per route, you can edit the configuration variable CONDORCE_MAX_JOBS in /etc/condor-ce/config.d/01-ce-router.conf : CONDORCE_MAX_JOBS = 10000 Note The above configuration is to be placed directly into the HTCondor-CE configuration, not into a job route. Maximum memory \u00b6 To set a default maximum memory (in MB) for routed jobs, set the attribute default_maxMemory : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Request memory\"; # Set the requested memory to 1 GB set_default_maxMemory = 1000; ] @jre Number of cores to request \u00b6 To set a default number of cores for routed jobs, set the attribute default_xcount : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Request CPU\"; # Set the requested cores to 8 set_default_xcount = 8; ] @jre Maximum walltime \u00b6 To set a default maximum walltime (in minutes) for routed jobs, set the attribute default_maxWallTime : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting WallTime\"; # Set the max walltime to 1 hr set_default_maxWallTime = 60; ] @jre Setting job environments \u00b6 HTCondor-CE offers two different methods for setting environment variables of routed jobs: CONDORCE_PILOT_JOB_ENV configuration, which should be used for setting environment variables for all routed jobs to static strings. set_default_pilot_job_env job route configuration, which should be used for setting environment variables: Per job route To values based on incoming job attributes Using ClassAd functions Both of these methods use the new HTCondor format of the environment command , which is described by environment variable/value pairs separated by whitespace and enclosed in double-quotes. For example, the following HTCondor-CE configuration would result in the following environment for all routed jobs: ``` tab=\"HTCondor-CE Configuration\" CONDORCE_PILOT_JOB_ENV = \"WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu\" ```bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu Contents of CONDORCE_PILOT_JOB_ENV can reference other HTCondor-CE configuration using HTCondor's configuration $() macro expansion . For example, the following HTCondor-CE configuration would result in the following environment for all routed jobs: ``` tab=\"HTCondor-CE Configuration\" LOCAL_PROXY = proxy.wisc.edu CONDORCE_PILOT_JOB_ENV = \"WN_SCRATCH_DIR=/nobackup/ http_proxy=$(LOCAL_PROXY)\" ```bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu To set environment variables per job route, based on incoming job attributes, or using ClassAd functions, add set_default_pilot_job_env to your job route configuration. For example, the following HTCondor-CE configuration would result in this environment for a job with these attributes: ``` tab=\"HTCondor-CE Configuration\" hl_lines=\"5 6 7\" JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Local_Condor\"; set_default_pilot_job_env = strcat(\"WN_SCRATCH_DIR=/nobackup\", \" PILOT_COLLECTOR=\", JOB_COLLECTOR, \" ACCOUNTING_GROUP=\", toLower(JOB_VO)); ] @jre ``` tab=\"Incoming Job Attributes\" JOB_COLLECTOR = \"collector.wisc.edu\" JOB_VO = \"GLOW\" ``` bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ PILOT_COLLECTOR=collector.wisc.edu ACCOUNTING_GROUP=glow !!!tip \"Debugging job route environment expressions\" While constructing `set_default_pilot_job_env` expressions, try wrapping your expression in [debug()](#debugging-routes) to help with any issues that may arise. Make sure to remove `debug()` after you're done! ### Editing attributes\u2026 The following functions are operations that affect job attributes and are evaluated in the following order: 1. `copy_*` 2. `delete_*` 3. `set_*` 4. `eval_set_*` After each job route\u2019s ClassAd is [constructed](#how-job-routes-are-constructed), the above operations are evaluated in order. For example, if the attribute `foo` is set using `eval_set_foo` in the `JOB_ROUTER_DEFAULTS`, you'll be unable to use `delete_foo` to remove it from your jobs since the attribute is set using `eval_set_foo` after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in `JOB_ROUTER_DEFAULTS` get overridden by the same operation in `JOB_ROUTER_ENTRIES`. So to 'delete' `foo`, we would add `eval_set_foo = \"\"` to the route in the `JOB_ROUTER_ENTRIES`, resulting in `foo` being absent from the routed job. More documentation can be found in the [HTCondor manual](https://htcondor.readthedocs.io/en/stable/grid-computing/job-router.html#routing-table-entry-classad-attributes). #### Copying attributes To copy the value of an attribute of the incoming job to an attribute of the routed job, use `copy_`. The following route copies the `environment` attribute of the incoming job and sets the attribute `Original_Environment` on the routed job to the same value: ```hl_lines=\"6\" JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Copying attributes\"; copy_environment = \"Original_Environment\"; ] @jre Removing attributes \u00b6 To remove an attribute of the incoming job from the routed job, use delete_ . The following route removes the environment attribute from the routed job: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Copying attributes\"; delete_environment = True; ] @jre Setting attributes \u00b6 To set an attribute on the routed job, use set_ . The following route sets the Job's Rank attribute to 5: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting an attribute\"; set_Rank = 5; ] @jre Setting attributes with ClassAd expressions \u00b6 To set an attribute to a ClassAd expression to be evaluated, use eval_set . The following route sets the Experiment attribute to atlas.osguser if the Owner of the incoming job is osguser : Note If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting an attribute with a !ClassAd expression\"; eval_set_Experiment = strcat(\"atlas.\", Owner); ] @jre Limiting the number of jobs \u00b6 This section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route). Note If you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via user priorities and group accounting . Total jobs \u00b6 To set a limit on the number of jobs for a specific route, set the MaxJobs attribute: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Limit the total number of jobs to 100\"; MaxJobs = 100; ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Limit the total number of jobs to 75\"; MaxJobs = 75; ] @jre Idle jobs \u00b6 To set a limit on the number of idle jobs for a specific route, set the MaxIdleJobs attribute: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Limit the total number of idle jobs to 100\"; MaxIdleJobs = 100; ] [ TargetUniverse = 5; name = \"Limit the total number of idle jobs to 75\"; MaxIdleJobs = 75; ] @jre Debugging routes \u00b6 To help debug expressions in your routes, you can use the debug() function. First, set the debug mode for the JobRouter by editing a file in /etc/condor-ce/config.d/ to read JOB_ROUTER_DEBUG = D_ALWAYS:2 D_CAT Then wrap the problematic attribute in debug() : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Debugging a difficult !ClassAd expression\"; eval_set_Experiment = debug(strcat(\"atlas\", Name)); ] @jre You will find the debugging output in /var/log/condor-ce/JobRouterLog . Routes for HTCondor Batch Systems \u00b6 This section contains information about job routes that can be used if you are running an HTCondor batch system at your site. Setting periodic hold, release or remove \u00b6 To release, remove or put a job on hold if it meets certain criteria, use the PERIODIC_* family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting PERIODIC_EXPR_INTERVAL in your CE's configuration. In this example, we set the routed job on hold if the job is idle and has been started at least once or if the job has tried to start more than once. This will catch jobs which are starting and stopping multiple times. JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Setting periodic statements\"; # Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; # Remove routed jobs if their walltime is longer than 3 days and 5 minutes set_Periodic_Remove = ( RemoteWallClockTime > (3*24*60*60 + 5*60) ); # Release routed jobs if the condor_starter couldn't start the executable and 'VMGAHP_ERR_INTERNAL' is in the HoldReason set_Periodic_Release = HoldReasonCode == 6 && regexp(\"VMGAHP_ERR_INTERNAL\", HoldReason); ] @jre Setting routed job requirements \u00b6 If you need to set requirements on your routed job, you will need to use set_Requirements instead of Requirements . The Requirements attribute filters jobs coming into your CE into different job routes whereas set_Requirements will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the HTCondor manual . To ensure that your job lands on a Linux machine in your pool: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; set_Requirements = OpSys == \"LINUX\"; ] @jre Preserving original job requirements \u00b6 To preserve and include the original job requirements, rather than just setting new requirements, you can use copy_Requirements to store the current value of Requirements to another variable, which we'll call original_requirements . To do this, replace the above set_Requirements line with: copy_Requirements = \"original_requirements\"; set_Requirements = original_requirements && ...; Routes for non-HTCondor Batch Systems \u00b6 This section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site. Setting a default batch queue \u00b6 To set a default queue for routed jobs, set the attribute default_queue : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting batch system queues\"; set_default_queue = \"osg_queue\"; ] @jre Setting batch system directives \u00b6 To write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in /etc/blahp/ (e.g., if your local batch system is PBS, edit /etc/blahp/pbs_local_submit_attributes.sh ). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via the default_remote_cerequirements attribute, which can take the following form: default_remote_cerequirements = \"foo == X && bar == \\\"Y\\\" && ...\" This sets foo to value X and bar to the string Y (escaped double-quotes are required for string values) in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the x509UserProxyFirstFQAN attribute of the job submitted to a PBS batch system JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting job submit variables\"; set_default_remote_cerequirements = strcat(\"Walltime == 3600 && AccountingGroup ==\"\", x509UserProxyFirstFQAN, \"\\\"\"); ] @jre With /etc/blahp/pbs_local_submit_attributes.sh containing. #!/bin/bash echo \"#PBS -l walltime=$Walltime\" echo \"#PBS -A $AccountingGroup\" This results in the following being appended to the script that gets submitted to your batch system: #PBS -l walltime=3600 #PBS -A <CE job's x509UserProxyFirstFQAN attribute> Getting Help \u00b6 If you have any questions or issues with configuring job routes, please contact us for assistance. Reference \u00b6 Here are some example HTCondor-CE job routes: AGLT2's job routes \u00b6 Atlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes. Setting various HTCondor-specific attributes like Rank , AccountingGroup , JobPrio and Periodic_Remove (see the HTCondor manual for more). Some of these are site-specific like LastandFrac , IdleMP8Pressure , localQue , IsAnalyJob and JobMemoryLimit . There is a difference between Requirements and set_requirements . The Requirements attribute matches jobs to specific routes while the set_requirements sets the Requirements attribute on the routed job, which confines which machines that the routed job can land on. Source: https://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content JOB_ROUTER_ENTRIES @=jre # Still to do on all routes, get job requirements and add them here # Route no 1 # Analysis queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue==\"analy\"; Name = \"Analysis Queue\"; TargetUniverse = 5; eval_set_IdleMP8Pressure = $(IdleMP8Pressure); eval_set_LastAndFrac = $(LastAndFrac); set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && (IfThenElse((Owner == \"atlasconnect\" || Owner == \"muoncal\"),IfThenElse(IdleMP8Pressure,(TARGET.PARTITIONED =!= TRUE),True),IfThenElse(LastAndFrac,(TARGET.PARTITIONED =!= TRUE),True))); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Analysis\"; set_IsAnalyJob = True; set_JobPrio = 5; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 2 # splitterNT queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"splitterNT\"; Name = \"Splitter ntuple queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = \"group_calibrate.muoncal\"; set_localQue = \"Splitter\"; set_IsAnalyJob = False; set_JobPrio = 10; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 3 # splitter queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"splitter\"; Name = \"Splitter queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = \"group_calibrate.muoncal\"; set_localQue = \"Splitter\"; set_IsAnalyJob = False; set_JobPrio = 15; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 4 # xrootd queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"xrootd\"; Name = \"Xrootd queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Analysis\"; set_IsAnalyJob = True; set_JobPrio = 35; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 5 # Tier3Test queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"Tier3Test\"; Name = \"Tier3 Test Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && ( IS_TIER3_TEST_QUEUE =?= True ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Tier3Test\"; set_IsTier3TestJob = True; set_IsAnalyJob = True; set_JobPrio = 20; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 6 # mp8 queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue==\"mp8\"; Name = \"MCORE Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && (( TARGET.Cpus == 8 && TARGET.CPU_TYPE =?= \"mp8\" ) || TARGET.PARTITIONED =?= True ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.mcore.\",Owner); set_localQue = \"MP8\"; set_IsAnalyJob = False; set_JobPrio = 25; set_Rank = 0.0; eval_set_RequestCpus = 8; set_JobMemoryLimit = 33552000; set_Slot_Type = \"mp8\"; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 7 # Installation queue, triggered by usatlas2 user [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && target.Owner == \"usatlas2\"; Name = \"Install Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && ( TARGET.IS_INSTALL_QUE =?= True ) && (TARGET.AGLT2_SITE == \"UM\" ); eval_set_AccountingGroup = strcat(\"group_gatekpr.other.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_IsInstallJob = True; set_JobPrio = 15; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 8 # Default queue for usatlas1 user [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && regexp(\"usatlas1\",target.Owner); Name = \"ATLAS Production Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.prod.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 9 # Default queue for any other usatlas account [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && (regexp(\"usatlas2\",target.Owner) || regexp(\"usatlas3\",target.Owner)); Name = \"Other ATLAS Production\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.other.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 10 # Anything else. Set queue as Default and assign to other VOs [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && ifThenElse(regexp(\"usatlas\",target.Owner),false,true); Name = \"Other Jobs\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_VOgener.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] @jre BNL's job routes \u00b6 ATLAS BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes: Setting various HTCondor-specific attributes like JobLeaseDuration , Requirements and Periodic_Hold (see the HTCondor manual ). Some of these are site-specific like RACF_Group , Experiment , Job_Type and VO . Jobs are split into different routes based on the GlideIn queue that they're in. There is a difference between Requirements and set_requirements . The Requirements attribute matches incoming jobs to specific routes while the set_requirements sets the Requirements attribute on the routed job, which confines which machines that the routed job can land on. JOB_ROUTER_ENTRIES @=jre [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_long\"; Requirements = target.queue==\"analysis.long\"; eval_set_RACF_Group = \"long\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_short\"; Requirements = target.queue==\"analysis.short\"; eval_set_RACF_Group = \"short\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_grid\"; Requirements = target.queue==\"grid\"; eval_set_RACF_Group = \"grid\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool\"; Requirements = target.queue is undefined; eval_set_RACF_Group = \"grid\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"rcf\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Experiment = \"atlas\"; set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] @jre","title":"Batch System Integration"},{"location":"v3/batch-system-integration/#writing-routes-for-htcondor-ce","text":"The JobRouter is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in /etc/condor-ce/config.d/02-ce-*.conf that provide enough basic functionality for a small site. If you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems. Definitions Incoming Job : A job which was submitted to the CE from an external source. Routed Job : A job that has been transformed by the JobRouter.","title":"Writing Routes For HTCondor-CE"},{"location":"v3/batch-system-integration/#quirks-and-pitfalls","text":"If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break. Make sure to run condor_ce_reconfig after changing your routes, otherwise they will not take effect. HTCondor batch system only: Local universe jobs are excluded from any routing.","title":"Quirks and Pitfalls"},{"location":"v3/batch-system-integration/#how-job-routes-are-constructed","text":"Each job route\u2019s ClassAd is constructed by combining each entry from the JOB_ROUTER_ENTRIES with the JOB_ROUTER_DEFAULTS . Attributes that are set_* in JOB_ROUTER_ENTRIES will override those set_* in JOB_ROUTER_DEFAULTS","title":"How Job Routes are Constructed"},{"location":"v3/batch-system-integration/#job_router_entries","text":"JOB_ROUTER_ENTRIES is a configuration variable whose default is set in /etc/condor-ce/config.d/02-ce-*.conf but may be overriden by the administrator in /etc/condor-ce/config.d/99-local.conf . This document outlines the many changes you can make to JOB_ROUTER_ENTRIES to fit your site\u2019s needs.","title":"JOB_ROUTER_ENTRIES"},{"location":"v3/batch-system-integration/#job_router_defaults","text":"JOB_ROUTER_DEFAULTS is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents in a readable format, run the following command: user@host $ condor_ce_config_val JOB_ROUTER_DEFAULTS | sed 's/;/;\\n/g' Warning If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break. Warning Do not set the JOB_ROUTER_DEFAULTS configuration variable yourself. This will cause the CE to stop functioning.","title":"JOB_ROUTER_DEFAULTS"},{"location":"v3/batch-system-integration/#how-jobs-match-to-job-routes","text":"The job router considers jobs in the queue ( condor_ce_q ) that meet the following constraints: The job has not already been considered by the job router The job is associated with an unexpired x509 proxy The job's universe is standard or vanilla If the job meets the above constraints, then the job's ClassAd is compared against each route's requirements . If the job only meets one route's requirements, the job is matched to that route. If the job meets the requirements of multiple routes, the route that is chosen depends on your version of HTCondor ( condor_version ): If your version of HTCondor is... Then the route is chosen by... < 8.7.1 Round-robin between all matching routes. In this case, we recommend making each route's requirements mutually exclusive. >= 8.7.1, < 8.8.7 First matching route where routes are considered in hash-table order. In this case, we recommend making each route's requirements mutually exclusive. >= 8.8.7 First matching route where routes are considered in the order specified by JOB_ROUTER_ROUTE_NAMES Job Route Order For HTCondor versions < 8.8.7 (as well as versions >= 8.9.0 and < 8.9.5) the order of job routes does not match the order in which they are configured. As a result, we recommend updating to HTCondor 8.8.7 (or 8.9.5) and specifying the names of your routes in JOB_ROUTER_ROUTE_NAMES in the order that you'd like them considered. If you are using HTCondor >= 8.7.1 and would like to use round-robin matching, add the following text to a file in /etc/condor-ce/config.d/ : JOB_ROUTER_ROUND_ROBIN_SELECTION = True","title":"How Jobs Match to Job Routes"},{"location":"v3/batch-system-integration/#generic-routes","text":"This section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in /etc/condor-ce/config.d/99-local.conf , not the original 02-ce-*.conf .","title":"Generic Routes"},{"location":"v3/batch-system-integration/#required-fields","text":"The minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in /usr/share/condor-ce/config.d/02-ce-<batch system>-defaults.conf , provided by the htcondor-ce-<batch system> packages.","title":"Required fields"},{"location":"v3/batch-system-integration/#batch-system","text":"Each route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the TargetUniverse attribute needs to be set to 5 or \"vanilla\" . For all other batch systems, the TargetUniverse attribute needs to be set to 9 or \"grid\" and the GridResource attribute needs to be set to \"batch <batch system>\" (where <batch system> can be one of pbs , slurm , lsf , or sge ). JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Route jobs to PBS\"; ] @jre","title":"Batch system"},{"location":"v3/batch-system-integration/#route-name","text":"To identify routes, you will need to assign a name to the route with the name attribute: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; ] @jre The name of the route will be useful in debugging since it shows up in the output of condor_ce_job_router_info , the JobRouterLog , and in the ClassAd of the routed job, which can be viewed with condor_ce_q or condor_ce_history .","title":"Route name"},{"location":"v3/batch-system-integration/#writing-multiple-routes","text":"Note Before writing multiple routes, consider the details of how jobs match to job routes If your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues), you will need to write multiple job routes where each route is enclosed by square brackets. The following routes takes incoming jobs that have a queue attribute set to \"analy\" and routes them to the site's HTCondor batch system. Any other jobs will be sent to that site's PBS batch system. JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Route jobs to HTCondor\"; Requirements = (TARGET.queue =?= \"analy\"); ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Route jobs to PBS\"; Requirements = (TARGET.queue =!= \"analy\"); ] @jre","title":"Writing multiple routes"},{"location":"v3/batch-system-integration/#writing-comments","text":"To write comments you can use # to comment a line: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"# comments\"; # This is a comment ] @jre","title":"Writing comments"},{"location":"v3/batch-system-integration/#setting-attributes-for-all-routes","text":"To set an attribute that will be applied to all routes, you will need to ensure that MERGE_JOB_ROUTER_DEFAULT_ADS is set to True (check the value with condor_ce_config_val ) and use the set_ function in the JOB_ROUTER_DEFAULTS . The following configuration sets the Periodic_Hold attribute for all routes: # Use the defaults generated by the condor_ce_router_defaults script. To add # additional defaults, add additional lines of the form: # # JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;] # MERGE_JOB_ROUTER_DEFAULT_ADS=True JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1;]","title":"Setting attributes for all routes"},{"location":"v3/batch-system-integration/#filtering-jobs-based-on","text":"To filter jobs, use the Requirements attribute. Jobs will evaluate against the ClassAd expression set in the Requirements and if the expression evaluates to TRUE , the route will match. More information on the syntax of ClassAd's can be found in the HTCondor manual . For an example on how incoming jobs interact with filtering in job routes, consult this document . When setting requirements, you need to prefix job attributes that you are filtering with TARGET. so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a queue = \"analy\" attribute, then the following job route will not match: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by queue\"; queue = \"not-analy\"; Requirements = (queue =?= \"analy\"); ] @jre This is because when evaluating the route requirement, the job route will compare its own queue attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the HTCondor manual . Note If you have an HTCondor batch system, note the difference with set_requirements . Note Before writing multiple routes, consider the details of how jobs match to job routes .","title":"Filtering jobs based on\u2026"},{"location":"v3/batch-system-integration/#glidein-queue","text":"To filter jobs based on their glidein queue attribute, your routes will need a Requirements expression using the incoming job's queue attribute. The following entry routes jobs to HTCondor if the incoming job (specified by TARGET ) is an analy (Analysis) glidein: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by queue\"; Requirements = (TARGET.queue =?= \"analy\"); ] @jre","title":"Glidein queue"},{"location":"v3/batch-system-integration/#job-submitter","text":"To filter jobs based on who submitted it, your routes will need a Requirements expression using the incoming job's Owner attribute. The following entry routes jobs to the HTCondor batch system if the submitter is usatlas2 : JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Filtering by job submitter\"; Requirements = (TARGET.Owner =?= \"usatlas2\"); ] @jre Alternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system if the submitter's name begins with usatlas : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Filtering by job submitter (regular expression)\"; Requirements = regexp(\"^usatlas\", TARGET.Owner); ] @jre","title":"Job submitter"},{"location":"v3/batch-system-integration/#voms-attribute","text":"To filter jobs based on the subject of the job's proxy, your routes will need a Requirements expression using the incoming job's x509UserProxyFirstFQAN attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains /cms/Role=Pilot : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Filtering by VOMS attribute (regex)\"; Requirements = regexp(\"\\/cms\\/Role\\=pilot\", TARGET.x509UserProxyFirstFQAN); ] @jre","title":"VOMS attribute"},{"location":"v3/batch-system-integration/#setting-a-default","text":"This section outlines how to set default job limits, memory, cores, and maximum walltime.","title":"Setting a default\u2026"},{"location":"v3/batch-system-integration/#maximum-number-of-jobs","text":"To set a default limit to the maximum number of jobs per route, you can edit the configuration variable CONDORCE_MAX_JOBS in /etc/condor-ce/config.d/01-ce-router.conf : CONDORCE_MAX_JOBS = 10000 Note The above configuration is to be placed directly into the HTCondor-CE configuration, not into a job route.","title":"Maximum number of jobs"},{"location":"v3/batch-system-integration/#maximum-memory","text":"To set a default maximum memory (in MB) for routed jobs, set the attribute default_maxMemory : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Request memory\"; # Set the requested memory to 1 GB set_default_maxMemory = 1000; ] @jre","title":"Maximum memory"},{"location":"v3/batch-system-integration/#number-of-cores-to-request","text":"To set a default number of cores for routed jobs, set the attribute default_xcount : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Request CPU\"; # Set the requested cores to 8 set_default_xcount = 8; ] @jre","title":"Number of cores to request"},{"location":"v3/batch-system-integration/#maximum-walltime","text":"To set a default maximum walltime (in minutes) for routed jobs, set the attribute default_maxWallTime : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting WallTime\"; # Set the max walltime to 1 hr set_default_maxWallTime = 60; ] @jre","title":"Maximum walltime"},{"location":"v3/batch-system-integration/#setting-job-environments","text":"HTCondor-CE offers two different methods for setting environment variables of routed jobs: CONDORCE_PILOT_JOB_ENV configuration, which should be used for setting environment variables for all routed jobs to static strings. set_default_pilot_job_env job route configuration, which should be used for setting environment variables: Per job route To values based on incoming job attributes Using ClassAd functions Both of these methods use the new HTCondor format of the environment command , which is described by environment variable/value pairs separated by whitespace and enclosed in double-quotes. For example, the following HTCondor-CE configuration would result in the following environment for all routed jobs: ``` tab=\"HTCondor-CE Configuration\" CONDORCE_PILOT_JOB_ENV = \"WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu\" ```bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu Contents of CONDORCE_PILOT_JOB_ENV can reference other HTCondor-CE configuration using HTCondor's configuration $() macro expansion . For example, the following HTCondor-CE configuration would result in the following environment for all routed jobs: ``` tab=\"HTCondor-CE Configuration\" LOCAL_PROXY = proxy.wisc.edu CONDORCE_PILOT_JOB_ENV = \"WN_SCRATCH_DIR=/nobackup/ http_proxy=$(LOCAL_PROXY)\" ```bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ http_proxy=proxy.wisc.edu To set environment variables per job route, based on incoming job attributes, or using ClassAd functions, add set_default_pilot_job_env to your job route configuration. For example, the following HTCondor-CE configuration would result in this environment for a job with these attributes: ``` tab=\"HTCondor-CE Configuration\" hl_lines=\"5 6 7\" JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Local_Condor\"; set_default_pilot_job_env = strcat(\"WN_SCRATCH_DIR=/nobackup\", \" PILOT_COLLECTOR=\", JOB_COLLECTOR, \" ACCOUNTING_GROUP=\", toLower(JOB_VO)); ] @jre ``` tab=\"Incoming Job Attributes\" JOB_COLLECTOR = \"collector.wisc.edu\" JOB_VO = \"GLOW\" ``` bash tab=\"Resulting Environment\" WN_SCRATCH_DIR=/nobackup/ PILOT_COLLECTOR=collector.wisc.edu ACCOUNTING_GROUP=glow !!!tip \"Debugging job route environment expressions\" While constructing `set_default_pilot_job_env` expressions, try wrapping your expression in [debug()](#debugging-routes) to help with any issues that may arise. Make sure to remove `debug()` after you're done! ### Editing attributes\u2026 The following functions are operations that affect job attributes and are evaluated in the following order: 1. `copy_*` 2. `delete_*` 3. `set_*` 4. `eval_set_*` After each job route\u2019s ClassAd is [constructed](#how-job-routes-are-constructed), the above operations are evaluated in order. For example, if the attribute `foo` is set using `eval_set_foo` in the `JOB_ROUTER_DEFAULTS`, you'll be unable to use `delete_foo` to remove it from your jobs since the attribute is set using `eval_set_foo` after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in `JOB_ROUTER_DEFAULTS` get overridden by the same operation in `JOB_ROUTER_ENTRIES`. So to 'delete' `foo`, we would add `eval_set_foo = \"\"` to the route in the `JOB_ROUTER_ENTRIES`, resulting in `foo` being absent from the routed job. More documentation can be found in the [HTCondor manual](https://htcondor.readthedocs.io/en/stable/grid-computing/job-router.html#routing-table-entry-classad-attributes). #### Copying attributes To copy the value of an attribute of the incoming job to an attribute of the routed job, use `copy_`. The following route copies the `environment` attribute of the incoming job and sets the attribute `Original_Environment` on the routed job to the same value: ```hl_lines=\"6\" JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Copying attributes\"; copy_environment = \"Original_Environment\"; ] @jre","title":"Setting job environments"},{"location":"v3/batch-system-integration/#removing-attributes","text":"To remove an attribute of the incoming job from the routed job, use delete_ . The following route removes the environment attribute from the routed job: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Copying attributes\"; delete_environment = True; ] @jre","title":"Removing attributes"},{"location":"v3/batch-system-integration/#setting-attributes","text":"To set an attribute on the routed job, use set_ . The following route sets the Job's Rank attribute to 5: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting an attribute\"; set_Rank = 5; ] @jre","title":"Setting attributes"},{"location":"v3/batch-system-integration/#setting-attributes-with-classad-expressions","text":"To set an attribute to a ClassAd expression to be evaluated, use eval_set . The following route sets the Experiment attribute to atlas.osguser if the Owner of the incoming job is osguser : Note If a value is set in JOB_ROUTER_DEFAULTS with eval_set_<variable> , override it by using eval_set_<variable> in the JOB_ROUTER_ENTRIES . JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting an attribute with a !ClassAd expression\"; eval_set_Experiment = strcat(\"atlas.\", Owner); ] @jre","title":"Setting attributes with ClassAd expressions"},{"location":"v3/batch-system-integration/#limiting-the-number-of-jobs","text":"This section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route). Note If you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via user priorities and group accounting .","title":"Limiting the number of jobs"},{"location":"v3/batch-system-integration/#total-jobs","text":"To set a limit on the number of jobs for a specific route, set the MaxJobs attribute: JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Limit the total number of jobs to 100\"; MaxJobs = 100; ] [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Limit the total number of jobs to 75\"; MaxJobs = 75; ] @jre","title":"Total jobs"},{"location":"v3/batch-system-integration/#idle-jobs","text":"To set a limit on the number of idle jobs for a specific route, set the MaxIdleJobs attribute: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Limit the total number of idle jobs to 100\"; MaxIdleJobs = 100; ] [ TargetUniverse = 5; name = \"Limit the total number of idle jobs to 75\"; MaxIdleJobs = 75; ] @jre","title":"Idle jobs"},{"location":"v3/batch-system-integration/#debugging-routes","text":"To help debug expressions in your routes, you can use the debug() function. First, set the debug mode for the JobRouter by editing a file in /etc/condor-ce/config.d/ to read JOB_ROUTER_DEBUG = D_ALWAYS:2 D_CAT Then wrap the problematic attribute in debug() : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Debugging a difficult !ClassAd expression\"; eval_set_Experiment = debug(strcat(\"atlas\", Name)); ] @jre You will find the debugging output in /var/log/condor-ce/JobRouterLog .","title":"Debugging routes"},{"location":"v3/batch-system-integration/#routes-for-htcondor-batch-systems","text":"This section contains information about job routes that can be used if you are running an HTCondor batch system at your site.","title":"Routes for HTCondor Batch Systems"},{"location":"v3/batch-system-integration/#setting-periodic-hold-release-or-remove","text":"To release, remove or put a job on hold if it meets certain criteria, use the PERIODIC_* family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting PERIODIC_EXPR_INTERVAL in your CE's configuration. In this example, we set the routed job on hold if the job is idle and has been started at least once or if the job has tried to start more than once. This will catch jobs which are starting and stopping multiple times. JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; name = \"Setting periodic statements\"; # Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; # Remove routed jobs if their walltime is longer than 3 days and 5 minutes set_Periodic_Remove = ( RemoteWallClockTime > (3*24*60*60 + 5*60) ); # Release routed jobs if the condor_starter couldn't start the executable and 'VMGAHP_ERR_INTERNAL' is in the HoldReason set_Periodic_Release = HoldReasonCode == 6 && regexp(\"VMGAHP_ERR_INTERNAL\", HoldReason); ] @jre","title":"Setting periodic hold, release or remove"},{"location":"v3/batch-system-integration/#setting-routed-job-requirements","text":"If you need to set requirements on your routed job, you will need to use set_Requirements instead of Requirements . The Requirements attribute filters jobs coming into your CE into different job routes whereas set_Requirements will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the HTCondor manual . To ensure that your job lands on a Linux machine in your pool: JOB_ROUTER_ENTRIES @=jre [ TargetUniverse = 5; set_Requirements = OpSys == \"LINUX\"; ] @jre","title":"Setting routed job requirements"},{"location":"v3/batch-system-integration/#preserving-original-job-requirements","text":"To preserve and include the original job requirements, rather than just setting new requirements, you can use copy_Requirements to store the current value of Requirements to another variable, which we'll call original_requirements . To do this, replace the above set_Requirements line with: copy_Requirements = \"original_requirements\"; set_Requirements = original_requirements && ...;","title":"Preserving original job requirements"},{"location":"v3/batch-system-integration/#routes-for-non-htcondor-batch-systems","text":"This section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site.","title":"Routes for non-HTCondor Batch Systems"},{"location":"v3/batch-system-integration/#setting-a-default-batch-queue","text":"To set a default queue for routed jobs, set the attribute default_queue : JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting batch system queues\"; set_default_queue = \"osg_queue\"; ] @jre","title":"Setting a default batch queue"},{"location":"v3/batch-system-integration/#setting-batch-system-directives","text":"To write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in /etc/blahp/ (e.g., if your local batch system is PBS, edit /etc/blahp/pbs_local_submit_attributes.sh ). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via the default_remote_cerequirements attribute, which can take the following form: default_remote_cerequirements = \"foo == X && bar == \\\"Y\\\" && ...\" This sets foo to value X and bar to the string Y (escaped double-quotes are required for string values) in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the x509UserProxyFirstFQAN attribute of the job submitted to a PBS batch system JOB_ROUTER_ENTRIES @=jre [ GridResource = \"batch pbs\"; TargetUniverse = 9; name = \"Setting job submit variables\"; set_default_remote_cerequirements = strcat(\"Walltime == 3600 && AccountingGroup ==\"\", x509UserProxyFirstFQAN, \"\\\"\"); ] @jre With /etc/blahp/pbs_local_submit_attributes.sh containing. #!/bin/bash echo \"#PBS -l walltime=$Walltime\" echo \"#PBS -A $AccountingGroup\" This results in the following being appended to the script that gets submitted to your batch system: #PBS -l walltime=3600 #PBS -A <CE job's x509UserProxyFirstFQAN attribute>","title":"Setting batch system directives"},{"location":"v3/batch-system-integration/#getting-help","text":"If you have any questions or issues with configuring job routes, please contact us for assistance.","title":"Getting Help"},{"location":"v3/batch-system-integration/#reference","text":"Here are some example HTCondor-CE job routes:","title":"Reference"},{"location":"v3/batch-system-integration/#aglt2s-job-routes","text":"Atlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes. Setting various HTCondor-specific attributes like Rank , AccountingGroup , JobPrio and Periodic_Remove (see the HTCondor manual for more). Some of these are site-specific like LastandFrac , IdleMP8Pressure , localQue , IsAnalyJob and JobMemoryLimit . There is a difference between Requirements and set_requirements . The Requirements attribute matches jobs to specific routes while the set_requirements sets the Requirements attribute on the routed job, which confines which machines that the routed job can land on. Source: https://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content JOB_ROUTER_ENTRIES @=jre # Still to do on all routes, get job requirements and add them here # Route no 1 # Analysis queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue==\"analy\"; Name = \"Analysis Queue\"; TargetUniverse = 5; eval_set_IdleMP8Pressure = $(IdleMP8Pressure); eval_set_LastAndFrac = $(LastAndFrac); set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && (IfThenElse((Owner == \"atlasconnect\" || Owner == \"muoncal\"),IfThenElse(IdleMP8Pressure,(TARGET.PARTITIONED =!= TRUE),True),IfThenElse(LastAndFrac,(TARGET.PARTITIONED =!= TRUE),True))); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Analysis\"; set_IsAnalyJob = True; set_JobPrio = 5; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 2 # splitterNT queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"splitterNT\"; Name = \"Splitter ntuple queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = \"group_calibrate.muoncal\"; set_localQue = \"Splitter\"; set_IsAnalyJob = False; set_JobPrio = 10; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 3 # splitter queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"splitter\"; Name = \"Splitter queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = \"group_calibrate.muoncal\"; set_localQue = \"Splitter\"; set_IsAnalyJob = False; set_JobPrio = 15; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 4 # xrootd queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"xrootd\"; Name = \"Xrootd queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Analysis\"; set_IsAnalyJob = True; set_JobPrio = 35; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 5 # Tier3Test queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue == \"Tier3Test\"; Name = \"Tier3 Test Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && ( IS_TIER3_TEST_QUEUE =?= True ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.analy.\",Owner); set_localQue = \"Tier3Test\"; set_IsTier3TestJob = True; set_IsAnalyJob = True; set_JobPrio = 20; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 6 # mp8 queue [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue==\"mp8\"; Name = \"MCORE Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && (( TARGET.Cpus == 8 && TARGET.CPU_TYPE =?= \"mp8\" ) || TARGET.PARTITIONED =?= True ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.mcore.\",Owner); set_localQue = \"MP8\"; set_IsAnalyJob = False; set_JobPrio = 25; set_Rank = 0.0; eval_set_RequestCpus = 8; set_JobMemoryLimit = 33552000; set_Slot_Type = \"mp8\"; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 7 # Installation queue, triggered by usatlas2 user [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && target.Owner == \"usatlas2\"; Name = \"Install Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) && ( TARGET.IS_INSTALL_QUE =?= True ) && (TARGET.AGLT2_SITE == \"UM\" ); eval_set_AccountingGroup = strcat(\"group_gatekpr.other.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_IsInstallJob = True; set_JobPrio = 15; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 8 # Default queue for usatlas1 user [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && regexp(\"usatlas1\",target.Owner); Name = \"ATLAS Production Queue\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.prod.prod.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 9 # Default queue for any other usatlas account [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && (regexp(\"usatlas2\",target.Owner) || regexp(\"usatlas3\",target.Owner)); Name = \"Other ATLAS Production\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_gatekpr.other.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] # Route no 10 # Anything else. Set queue as Default and assign to other VOs [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \" $(JOB_ROUTER_SCHEDD2_POOL)\"); Requirements = target.queue is undefined && ifThenElse(regexp(\"usatlas\",target.Owner),false,true); Name = \"Other Jobs\"; TargetUniverse = 5; set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk >= 21000000 ) ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); eval_set_AccountingGroup = strcat(\"group_VOgener.\",Owner); set_localQue = \"Default\"; set_IsAnalyJob = False; set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; set_JobMemoryLimit = 4194000; set_Periodic_Remove = ( ( RemoteWallClockTime > (3*24*60*60 + 5*60) ) || (ImageSize > JobMemoryLimit) ); ] @jre","title":"AGLT2's job routes"},{"location":"v3/batch-system-integration/#bnls-job-routes","text":"ATLAS BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes: Setting various HTCondor-specific attributes like JobLeaseDuration , Requirements and Periodic_Hold (see the HTCondor manual ). Some of these are site-specific like RACF_Group , Experiment , Job_Type and VO . Jobs are split into different routes based on the GlideIn queue that they're in. There is a difference between Requirements and set_requirements . The Requirements attribute matches incoming jobs to specific routes while the set_requirements sets the Requirements attribute on the routed job, which confines which machines that the routed job can land on. JOB_ROUTER_ENTRIES @=jre [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_long\"; Requirements = target.queue==\"analysis.long\"; eval_set_RACF_Group = \"long\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_short\"; Requirements = target.queue==\"analysis.short\"; eval_set_RACF_Group = \"short\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool_grid\"; Requirements = target.queue==\"grid\"; eval_set_RACF_Group = \"grid\"; set_Experiment = \"atlas\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"atlas\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] [ GridResource = \"condor localhost localhost\"; eval_set_GridResource = strcat(\"condor \", \"$(FULL_HOSTNAME)\", \"$(FULL_HOSTNAME)\"); TargetUniverse = 5; name = \"BNL_Condor_Pool\"; Requirements = target.queue is undefined; eval_set_RACF_Group = \"grid\"; set_requirements = ( ( Arch == \"INTEL\" || Arch == \"X86_64\" ) && ( CPU_Experiment == \"rcf\" ) ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ); set_Experiment = \"atlas\"; set_Job_Type = \"cas\"; set_JobLeaseDuration = 3600; set_Periodic_Hold = (NumJobStarts >= 1 && JobStatus == 1) || NumJobStarts > 1; eval_set_VO = x509UserProxyVOName; ] @jre","title":"BNL's job routes"},{"location":"v3/job-submission/","text":"Submitting Jobs to an HTCondor-CE \u00b6 This document outlines methods of manual submission to an HTCondor-CE. It is intended for site administrators wishing to verify the functionality of their HTCondor-CE installation and developers writing software to submit jobs to an HTCondor-CE (e.g., pilot jobs). Note Most incoming jobs are pilots from factories and that manual submission does not reflect the standard method that jobs are submitted to HTCondor-CEs. Submitting Jobs... \u00b6 There are two main methods for submitting files to an HTCondor-CE: using the tools bundled with the htcondor-ce-client package and using the condor_submit command with a submit file. Both methods will test end-to-end job submission but the former method is simpler while the latter will walk you through writing your own submit file. Before attempting to submit jobs, you will need to generate a proxy from a user certificate before running any jobs. To generate a proxy, run the following command on the host you plan on submitting from: user@host $ voms-proxy-init Using HTCondor-CE tools \u00b6 There are two HTCondor-CE tools that allow users to test the functionality of their HTCondor-CE: condor_ce_trace and condor_ce_run . The former is the preferred tool as it provides useful feedback if failure occurs while the latter is simply an automated submission tool. These commands may be run from any host that has htcondor-ce-client installed, which you may wish to do if you are testing availability of your CE from an external source. condor_ce_trace \u00b6 condor_ce_trace is a Python script that uses HTCondor's Python bindings to run diagnostics, including job submission, against your HTCondor-CE. To submit a job with condor_ce_trace , run the following command: user@host $ condor_ce_trace --debug condorce.example.com Replacing condorce.example.com with the hostname of the CE you wish to test. On success, you will see Job status: Completed and the environment of the job on the worker node it landed on. If you do not get the expected output, refer to the troubleshooting guide . Requesting resources \u00b6 condor_ce_trace doesn't make any specific resource requests so its jobs are only given the default resources by the CE. To request specific resources (or other job attributes), you can specify the --attribute option on the command line: user@host $ condor_ce_trace --debug --attribute = '+resource1=value1' ...--attribute = '+resourceN=valueN' condorce.example.com To submit a job that requests 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command: user@host $ condor_ce_trace --debug --attribute = '+xcount=4' --attribute = '+maxMemory=4000' --attribute = '+maxWallTime=120' --attribute = '+remote_queue=osg' condorce.example.com For a list of other attributes that can be set with the --attribute option, consult the job attributes section. Note Non-HTCondor batch systems may need additional configuration to support these job attributes. See the batch system integration for details on how to support them. condor_ce_run \u00b6 condor_ce_run is a Python script that calls condor_submit on a generated submit file and tracks its progress with condor_q . To submit a job with condor_ce_run , run the following command: user@host $ condor_ce_run -r condorce.example.com:9619 /bin/env Replacing condorce.example.com with the hostname of the CE you wish to test. The command will not return any output until it completes: When it does you will see the environment of the job on the worker noded it landed on. If you do not get the expected output, refer to the troubleshooting guide . Using a submit file... \u00b6 If you are familiar with HTCondor, submitting a job to an HTCondor-CE using a submit file follows the same procedure as submitting a job to an HTCondor batch system: Write a submit file and use condor_submit (or in one of our cases, condor_ce_submit ) to submit the job. This is by virtue of the fact that HTCondor-CE is just a special configuration of HTCondor. The major differences occur in the specific attributes for the submit files outlined below. From the CE host \u00b6 This method uses condor_ce_submit to submit directly to an HTCondor-CE. The only reason we use condor_ce_submit in this case is to take advantage of the already running daemons on the CE host. Write a submit file, ce_test.sub : # Required for local HTCondor-CE submission universe = vanilla use_x509userproxy = true +Owner = undefined # Files executable = ce_test.sh output = ce_test.out error = ce_test.err log = ce_test.log # File transfer behavior ShouldTransferFiles = YES WhenToTransferOutput = ON_EXIT # Optional resource requests #+xcount = 4 # Request 4 cores #+maxMemory = 4000 # Request 4GB of RAM #+maxWallTime = 120 # Request 2 hrs of wall clock time #+remote_queue = \"osg\" # Request the OSG queue # Run job once queue Replacing ce_test.sh with the path to the executable you wish to run. You can use any executable you choose for the executable field. If you don't have one in mind, you may use the following example test script: 1 2 3 4 5 #!/bin/bash date hostname env Mark the test script as executable: user@host $ chmod +x ce_test.sh Submit the job: user@host $ condor_ce_submit ce_test.sub From another host \u00b6 For this method, you will need a functional HTCondor submit node. If you do not have one readily available, you can install the condor package from the OSG repository to get a simple submit node: Install HTCondor: root@host # yum install condor Start the condor service: root@host $ service condor start Once the condor service is running, write a submit file and submit your job: Write a submit file, ce_test.sub : # Required for remote HTCondor-CE submission universe = grid use_x509userproxy = true grid_resource = condor condorce.example.com condorce.example.com:9619 # Files executable = ce_test.sh output = ce_test.out error = ce_test.err log = ce_test.log # File transfer behavior ShouldTransferFiles = YES WhenToTransferOutput = ON_EXIT # Optional resource requests #+xcount = 4 # Request 4 cores #+maxMemory = 4000 # Request 4GB of RAM #+maxWallTime = 120 # Request 2 hrs of wall clock time #+remote_queue = \"osg\" # Request the OSG queue # Run job once queue Replacing ce_test.sh with the path to the executable you wish to run and condorce.example.com with the hostname of the CE you wish to test. Note The grid_resource line should start with condor and is not related to which batch system you are using. You can use any executable you choose for the executable field. If you don't have one in mind, you may use the following example test script: 1 2 3 4 5 #!/bin/bash date hostname env Mark the test script as executable: user@host $ chmod +x ce_test.sh Submit the job: user@host $ condor_submit ce_test.sub Tracking job progress \u00b6 When the job completes, stdout will be placed into ce_test.out , stderr will be placed into ce_test.err , and HTCondor logging information will be placed in ce_test.log . You can track job progress by looking at the condor queue by running the following command on the CE host: user@host $ condor_ce_q Using the following table to determine job status: This value in the ST column... Means that the job is... I idle C complete X being removed H held < transferring input > transferring output How Job Routes Affect Your Job \u00b6 Upon successful submission of your job, the Job Router takes control of your job by matching it to routes and submitting a transformed job to your batch system. Matching \u00b6 See this section for details on how jobs are matched to job routes. Examples The following three routes only perform filtering and submission of routed jobs to an HTCondor batch system. The only differences are in the types of jobs that they match: Route 1: Matches jobs whose attribute foo is equal to bar . Route 2: Matches jobs whose attribute foo is equal to baz . Route 3: Matches jobs whose attribute foo is neither equal to bar nor baz . Note Setting a custom attribute for job submission requires the + prefix in your submit file but it is unnecessary in the job routes. JOB_ROUTER_ENTRIES = [ \\ TargetUniverse = 5; \\ name = \"Route 1\"; \\ Requirements = (TARGET.foo =?= \"bar\"); \\ ] \\ [ \\ TargetUniverse = 5; \\ name = \"Route 2\"; \\ Requirements = (TARGET.foo =?= \"baz\"); \\ ] \\ [ \\ TargetUniverse = 5; \\ name = \"Route 3\"; \\ Requirements = (TARGET.foo =!= \"bar\") && (TARGET.foo =!= \"baz\"); \\ ] If a user submitted their job with +foo = bar in their submit file, the job would match Route 1 . Route defaults \u00b6 Route defaults can be set for batch system queue, maximum memory, number of cores to request, and maximum walltime. The submitting user can override any of these by setting the corresponding attribute in their job. Examples The following route takes all incoming jobs and submits them to an HTCondor batch system requesting 1GB of memory. JOB_ROUTER_ENTRIES = [ \\ TargetUniverse = 5; \\ name = \"Route 1\"; \\ set_default_maxMemory = 1000; \\ ] A user could submit their job with the attribute +maxMemory=2000 and that job would be submitted requesting 2GB memory instead of the default of 1GB. Troubleshooting Your Jobs \u00b6 Troubleshooting \u00b6 All interactions between condor_submit and the CE will be recorded in the file specified by the log attribute in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion: 000 (786.000.000) 12/09 16:49:55 Job submitted from host: <131.225.154.68:53134> ... 027 (786.000.000) 12/09 16:50:09 Job submitted to grid resource GridResource: condor condorce.example.com condorce.example.com:9619 GridJobId: condor condorce.example.com condorce.example.com:9619 796.0 ... 005 (786.000.000) 12/09 16:52:19 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 0 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 0 - Total Bytes Received By Job If there are issues contacting the CE, you will see error messages about a 'Down Globus Resource': 020 (788.000.000) 12/09 16:56:17 Detected Down Globus Resource RM-Contact: fermicloud133.fnal.gov ... 026 (788.000.000) 12/09 16:56:17 Detected Down Grid Resource GridResource: condor condorce.example.com condorce.example.com:9619 This indicates a communication issue with your CE that can be diagnosed with condor_ce_ping . Reference \u00b6 Here are some other HTCondor-CE documents that might be helpful: HTCondor-CE overview and architecture Installing HTCondor-CE Configuring HTCondor-CE job routes The HTCondor-CE troubleshooting guide Job attributes \u00b6 The following table is a reference of job attributes that can be included in HTCondor submit files and their GlobusRSL equivalents. A more comprehensive list of submit file attributes specific to HTCondor-CE can be found in the HTCondor manual HTCondor Attribute Globus RSL Summary arguments arguments Arguments that will be provided to the executable for the job. error stderr Path to the file on the client machine that stores stderr from the job. executable executable Path to the file on the client machine that the job will execute. input stdin Path to the file on the client machine that stores input to be piped into the stdin of the job. +maxMemory maxMemory The amount of memory in MB that you wish to allocate to the job. +maxWallTime maxWallTime The maximum walltime (in minutes) the job is allowed to run before it is removed. output stdout Path to the file on the client machine that stores stdout from the job. +remote_queue queue Assign job to the target queue in the scheduler. Note that the queue name should be in quotes. transfer_input_files file_stage_in A comma-delimited list of all the files and directories to be transferred into the working directory for the job, before the job is started. transfer_output_files transfer_output_files A comma-delimited list of all the files and directories to be transferred back to the client, after the job completes. +xcount xcount The number of cores to allocate for the job. If you are setting an attribute to a string value, make sure to enclose the string in double-quotes ( \" ), otherwise HTCondor-CE will try to find an attribute by that name.","title":"Job submission"},{"location":"v3/job-submission/#submitting-jobs-to-an-htcondor-ce","text":"This document outlines methods of manual submission to an HTCondor-CE. It is intended for site administrators wishing to verify the functionality of their HTCondor-CE installation and developers writing software to submit jobs to an HTCondor-CE (e.g., pilot jobs). Note Most incoming jobs are pilots from factories and that manual submission does not reflect the standard method that jobs are submitted to HTCondor-CEs.","title":"Submitting Jobs to an HTCondor-CE"},{"location":"v3/job-submission/#submitting-jobs","text":"There are two main methods for submitting files to an HTCondor-CE: using the tools bundled with the htcondor-ce-client package and using the condor_submit command with a submit file. Both methods will test end-to-end job submission but the former method is simpler while the latter will walk you through writing your own submit file. Before attempting to submit jobs, you will need to generate a proxy from a user certificate before running any jobs. To generate a proxy, run the following command on the host you plan on submitting from: user@host $ voms-proxy-init","title":"Submitting Jobs..."},{"location":"v3/job-submission/#using-htcondor-ce-tools","text":"There are two HTCondor-CE tools that allow users to test the functionality of their HTCondor-CE: condor_ce_trace and condor_ce_run . The former is the preferred tool as it provides useful feedback if failure occurs while the latter is simply an automated submission tool. These commands may be run from any host that has htcondor-ce-client installed, which you may wish to do if you are testing availability of your CE from an external source.","title":"Using HTCondor-CE tools"},{"location":"v3/job-submission/#condor_ce_trace","text":"condor_ce_trace is a Python script that uses HTCondor's Python bindings to run diagnostics, including job submission, against your HTCondor-CE. To submit a job with condor_ce_trace , run the following command: user@host $ condor_ce_trace --debug condorce.example.com Replacing condorce.example.com with the hostname of the CE you wish to test. On success, you will see Job status: Completed and the environment of the job on the worker node it landed on. If you do not get the expected output, refer to the troubleshooting guide .","title":"condor_ce_trace"},{"location":"v3/job-submission/#requesting-resources","text":"condor_ce_trace doesn't make any specific resource requests so its jobs are only given the default resources by the CE. To request specific resources (or other job attributes), you can specify the --attribute option on the command line: user@host $ condor_ce_trace --debug --attribute = '+resource1=value1' ...--attribute = '+resourceN=valueN' condorce.example.com To submit a job that requests 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command: user@host $ condor_ce_trace --debug --attribute = '+xcount=4' --attribute = '+maxMemory=4000' --attribute = '+maxWallTime=120' --attribute = '+remote_queue=osg' condorce.example.com For a list of other attributes that can be set with the --attribute option, consult the job attributes section. Note Non-HTCondor batch systems may need additional configuration to support these job attributes. See the batch system integration for details on how to support them.","title":"Requesting resources"},{"location":"v3/job-submission/#condor_ce_run","text":"condor_ce_run is a Python script that calls condor_submit on a generated submit file and tracks its progress with condor_q . To submit a job with condor_ce_run , run the following command: user@host $ condor_ce_run -r condorce.example.com:9619 /bin/env Replacing condorce.example.com with the hostname of the CE you wish to test. The command will not return any output until it completes: When it does you will see the environment of the job on the worker noded it landed on. If you do not get the expected output, refer to the troubleshooting guide .","title":"condor_ce_run"},{"location":"v3/job-submission/#using-a-submit-file","text":"If you are familiar with HTCondor, submitting a job to an HTCondor-CE using a submit file follows the same procedure as submitting a job to an HTCondor batch system: Write a submit file and use condor_submit (or in one of our cases, condor_ce_submit ) to submit the job. This is by virtue of the fact that HTCondor-CE is just a special configuration of HTCondor. The major differences occur in the specific attributes for the submit files outlined below.","title":"Using a submit file..."},{"location":"v3/job-submission/#from-the-ce-host","text":"This method uses condor_ce_submit to submit directly to an HTCondor-CE. The only reason we use condor_ce_submit in this case is to take advantage of the already running daemons on the CE host. Write a submit file, ce_test.sub : # Required for local HTCondor-CE submission universe = vanilla use_x509userproxy = true +Owner = undefined # Files executable = ce_test.sh output = ce_test.out error = ce_test.err log = ce_test.log # File transfer behavior ShouldTransferFiles = YES WhenToTransferOutput = ON_EXIT # Optional resource requests #+xcount = 4 # Request 4 cores #+maxMemory = 4000 # Request 4GB of RAM #+maxWallTime = 120 # Request 2 hrs of wall clock time #+remote_queue = \"osg\" # Request the OSG queue # Run job once queue Replacing ce_test.sh with the path to the executable you wish to run. You can use any executable you choose for the executable field. If you don't have one in mind, you may use the following example test script: 1 2 3 4 5 #!/bin/bash date hostname env Mark the test script as executable: user@host $ chmod +x ce_test.sh Submit the job: user@host $ condor_ce_submit ce_test.sub","title":"From the CE host"},{"location":"v3/job-submission/#from-another-host","text":"For this method, you will need a functional HTCondor submit node. If you do not have one readily available, you can install the condor package from the OSG repository to get a simple submit node: Install HTCondor: root@host # yum install condor Start the condor service: root@host $ service condor start Once the condor service is running, write a submit file and submit your job: Write a submit file, ce_test.sub : # Required for remote HTCondor-CE submission universe = grid use_x509userproxy = true grid_resource = condor condorce.example.com condorce.example.com:9619 # Files executable = ce_test.sh output = ce_test.out error = ce_test.err log = ce_test.log # File transfer behavior ShouldTransferFiles = YES WhenToTransferOutput = ON_EXIT # Optional resource requests #+xcount = 4 # Request 4 cores #+maxMemory = 4000 # Request 4GB of RAM #+maxWallTime = 120 # Request 2 hrs of wall clock time #+remote_queue = \"osg\" # Request the OSG queue # Run job once queue Replacing ce_test.sh with the path to the executable you wish to run and condorce.example.com with the hostname of the CE you wish to test. Note The grid_resource line should start with condor and is not related to which batch system you are using. You can use any executable you choose for the executable field. If you don't have one in mind, you may use the following example test script: 1 2 3 4 5 #!/bin/bash date hostname env Mark the test script as executable: user@host $ chmod +x ce_test.sh Submit the job: user@host $ condor_submit ce_test.sub","title":"From another host"},{"location":"v3/job-submission/#tracking-job-progress","text":"When the job completes, stdout will be placed into ce_test.out , stderr will be placed into ce_test.err , and HTCondor logging information will be placed in ce_test.log . You can track job progress by looking at the condor queue by running the following command on the CE host: user@host $ condor_ce_q Using the following table to determine job status: This value in the ST column... Means that the job is... I idle C complete X being removed H held < transferring input > transferring output","title":"Tracking job progress"},{"location":"v3/job-submission/#how-job-routes-affect-your-job","text":"Upon successful submission of your job, the Job Router takes control of your job by matching it to routes and submitting a transformed job to your batch system.","title":"How Job Routes Affect Your Job"},{"location":"v3/job-submission/#matching","text":"See this section for details on how jobs are matched to job routes. Examples The following three routes only perform filtering and submission of routed jobs to an HTCondor batch system. The only differences are in the types of jobs that they match: Route 1: Matches jobs whose attribute foo is equal to bar . Route 2: Matches jobs whose attribute foo is equal to baz . Route 3: Matches jobs whose attribute foo is neither equal to bar nor baz . Note Setting a custom attribute for job submission requires the + prefix in your submit file but it is unnecessary in the job routes. JOB_ROUTER_ENTRIES = [ \\ TargetUniverse = 5; \\ name = \"Route 1\"; \\ Requirements = (TARGET.foo =?= \"bar\"); \\ ] \\ [ \\ TargetUniverse = 5; \\ name = \"Route 2\"; \\ Requirements = (TARGET.foo =?= \"baz\"); \\ ] \\ [ \\ TargetUniverse = 5; \\ name = \"Route 3\"; \\ Requirements = (TARGET.foo =!= \"bar\") && (TARGET.foo =!= \"baz\"); \\ ] If a user submitted their job with +foo = bar in their submit file, the job would match Route 1 .","title":"Matching"},{"location":"v3/job-submission/#route-defaults","text":"Route defaults can be set for batch system queue, maximum memory, number of cores to request, and maximum walltime. The submitting user can override any of these by setting the corresponding attribute in their job. Examples The following route takes all incoming jobs and submits them to an HTCondor batch system requesting 1GB of memory. JOB_ROUTER_ENTRIES = [ \\ TargetUniverse = 5; \\ name = \"Route 1\"; \\ set_default_maxMemory = 1000; \\ ] A user could submit their job with the attribute +maxMemory=2000 and that job would be submitted requesting 2GB memory instead of the default of 1GB.","title":"Route defaults"},{"location":"v3/job-submission/#troubleshooting-your-jobs","text":"","title":"Troubleshooting Your Jobs"},{"location":"v3/job-submission/#troubleshooting","text":"All interactions between condor_submit and the CE will be recorded in the file specified by the log attribute in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion: 000 (786.000.000) 12/09 16:49:55 Job submitted from host: <131.225.154.68:53134> ... 027 (786.000.000) 12/09 16:50:09 Job submitted to grid resource GridResource: condor condorce.example.com condorce.example.com:9619 GridJobId: condor condorce.example.com condorce.example.com:9619 796.0 ... 005 (786.000.000) 12/09 16:52:19 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 0 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 0 - Total Bytes Received By Job If there are issues contacting the CE, you will see error messages about a 'Down Globus Resource': 020 (788.000.000) 12/09 16:56:17 Detected Down Globus Resource RM-Contact: fermicloud133.fnal.gov ... 026 (788.000.000) 12/09 16:56:17 Detected Down Grid Resource GridResource: condor condorce.example.com condorce.example.com:9619 This indicates a communication issue with your CE that can be diagnosed with condor_ce_ping .","title":"Troubleshooting"},{"location":"v3/job-submission/#reference","text":"Here are some other HTCondor-CE documents that might be helpful: HTCondor-CE overview and architecture Installing HTCondor-CE Configuring HTCondor-CE job routes The HTCondor-CE troubleshooting guide","title":"Reference"},{"location":"v3/job-submission/#job-attributes","text":"The following table is a reference of job attributes that can be included in HTCondor submit files and their GlobusRSL equivalents. A more comprehensive list of submit file attributes specific to HTCondor-CE can be found in the HTCondor manual HTCondor Attribute Globus RSL Summary arguments arguments Arguments that will be provided to the executable for the job. error stderr Path to the file on the client machine that stores stderr from the job. executable executable Path to the file on the client machine that the job will execute. input stdin Path to the file on the client machine that stores input to be piped into the stdin of the job. +maxMemory maxMemory The amount of memory in MB that you wish to allocate to the job. +maxWallTime maxWallTime The maximum walltime (in minutes) the job is allowed to run before it is removed. output stdout Path to the file on the client machine that stores stdout from the job. +remote_queue queue Assign job to the target queue in the scheduler. Note that the queue name should be in quotes. transfer_input_files file_stage_in A comma-delimited list of all the files and directories to be transferred into the working directory for the job, before the job is started. transfer_output_files transfer_output_files A comma-delimited list of all the files and directories to be transferred back to the client, after the job completes. +xcount xcount The number of cores to allocate for the job. If you are setting an attribute to a string value, make sure to enclose the string in double-quotes ( \" ), otherwise HTCondor-CE will try to find an attribute by that name.","title":"Job attributes"},{"location":"v3/overview/","text":"Overview \u00b6 This document serves as an introduction to HTCondor-CE and how it works. Before continuing with the overview, make sure that you are familiar with the following concepts: What is a batch system and which one will you use ( HTCondor , Grid Engine , LSF , PBS Pro / Torque , and Slurm )? Pilot jobs and factories (e.g., GlideinWMS , PanDA , DIRAC ) What is a Compute Entrypoint? \u00b6 A Compute Entrypoint (CE) is the entry point for grid jobs into your local resources. CEs are made up of a layer of software that you install on a machine that can submit jobs into your local batch system. At the heart of the CE is the job gateway software, which is responsible for handling incoming jobs, authenticating and authorizing them, and delegating them to your batch system for execution. In many grids today, jobs that arrive at a CE (called grid jobs ) are not end-user jobs, but rather pilot jobs submitted from job factories. Successful pilot jobs create and make available an environment for end-user jobs to match and ultimately run within the pilot job. Note The Compute Entrypoint was previously known as the \"Compute Element\". What is HTCondor-CE? \u00b6 HTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for computing grids (e.g. European Grid Infrastructure , Open Science Grid ). It is configured to use the Job Router daemon to delegate jobs by transforming and submitting them to the site\u2019s batch system. Benefits of running the HTCondor-CE: Scalability: HTCondor-CE is capable of supporting job workloads of large sites Debugging tools: HTCondor-CE offers many tools to help troubleshoot issues with jobs Routing as configuration: HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs How Jobs Run \u00b6 Once an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the Job Router creates a transformed copy (called the routed job ) and submits the copy to the batch system (called the batch system job ). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter. On HTCondor batch systems \u00b6 For a site with an HTCondor batch system , the Job Router uses HTCondor protocols to place a transformed copy of the grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the same. Thus, there are three representations of your job, each with its own ID (see diagram below): Submitter: the HTCondor job ID in the original queue HTCondor-CE: the incoming grid job\u2019s ID HTCondor batch system: the routed job\u2019s ID In an HTCondor-CE/HTCondor setup, file transfer is handled natively between the two sets of daemons by the underlying HTCondor software. If you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in /etc/condor/ and the other in /etc/condor-ce ) and will need to make sure to differentiate the two when modifying any configuration. On other batch systems \u00b6 For non-HTCondor batch systems, the Job Router transforms the grid job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below): Submitter: the HTCondor job ID in the original queue HTCondor-CE: the incoming grid job\u2019s ID and the routed job\u2019s ID HTCondor batch system: the batch system\u2019s job ID Although the following figure specifies the PBS case, it applies to all non-HTCondor batch systems: With non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its \"spool\" directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes. Hosted CE over SSH \u00b6 The Hosted CE is designed to be an HTCondor-CE as a Service offered by central grid operations. Hosted CEs submit jobs to remote clusters over SSH, providing a simple starting point for opportunistic resource owners that want to start contributing to a computing grid with minimal effort. If your site intends to run over 10,000 concurrent grid jobs, you will need to host your own HTCondor-CE because the Hosted CE has not yet been optimized for such loads. How the CE is Customized \u00b6 Aside from the basic configuration required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all): Deciding which Virtual Organizations (VOs) are allowed to run at your site: HTCondor-CE leverages HTCondor's built-in ability to authenticate incoming jobs based on their GSI credentials, including VOMS attributes. Additionally, HTCondor may be configured to callout to external authentication services like Argus or LCMAPS. How to filter and transform the grid jobs to be run on your batch system: Filtering and transforming grid jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the batch system integration page. How Security Works \u00b6 In the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certificates. When these clients and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust. In its default configuration, HTCondor-CE uses GSI-based authentication and authorization to verify the certificate chain, which will work with technologies such as LCMAPS or Argus. Additionally, it can be reconfigured to provide alternate authentication mechanisms such as token, Kerberos, SSL, shared secret, or even IP-based authentication. More information about authorization methods can be found here . Getting Help \u00b6 If you have any questions about the architecture of HTCondor-CE, please contact us for assistance.","title":"Overview"},{"location":"v3/overview/#overview","text":"This document serves as an introduction to HTCondor-CE and how it works. Before continuing with the overview, make sure that you are familiar with the following concepts: What is a batch system and which one will you use ( HTCondor , Grid Engine , LSF , PBS Pro / Torque , and Slurm )? Pilot jobs and factories (e.g., GlideinWMS , PanDA , DIRAC )","title":"Overview"},{"location":"v3/overview/#what-is-a-compute-entrypoint","text":"A Compute Entrypoint (CE) is the entry point for grid jobs into your local resources. CEs are made up of a layer of software that you install on a machine that can submit jobs into your local batch system. At the heart of the CE is the job gateway software, which is responsible for handling incoming jobs, authenticating and authorizing them, and delegating them to your batch system for execution. In many grids today, jobs that arrive at a CE (called grid jobs ) are not end-user jobs, but rather pilot jobs submitted from job factories. Successful pilot jobs create and make available an environment for end-user jobs to match and ultimately run within the pilot job. Note The Compute Entrypoint was previously known as the \"Compute Element\".","title":"What is a Compute Entrypoint?"},{"location":"v3/overview/#what-is-htcondor-ce","text":"HTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for computing grids (e.g. European Grid Infrastructure , Open Science Grid ). It is configured to use the Job Router daemon to delegate jobs by transforming and submitting them to the site\u2019s batch system. Benefits of running the HTCondor-CE: Scalability: HTCondor-CE is capable of supporting job workloads of large sites Debugging tools: HTCondor-CE offers many tools to help troubleshoot issues with jobs Routing as configuration: HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs","title":"What is HTCondor-CE?"},{"location":"v3/overview/#how-jobs-run","text":"Once an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the Job Router creates a transformed copy (called the routed job ) and submits the copy to the batch system (called the batch system job ). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter.","title":"How Jobs Run"},{"location":"v3/overview/#on-htcondor-batch-systems","text":"For a site with an HTCondor batch system , the Job Router uses HTCondor protocols to place a transformed copy of the grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the same. Thus, there are three representations of your job, each with its own ID (see diagram below): Submitter: the HTCondor job ID in the original queue HTCondor-CE: the incoming grid job\u2019s ID HTCondor batch system: the routed job\u2019s ID In an HTCondor-CE/HTCondor setup, file transfer is handled natively between the two sets of daemons by the underlying HTCondor software. If you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in /etc/condor/ and the other in /etc/condor-ce ) and will need to make sure to differentiate the two when modifying any configuration.","title":"On HTCondor batch systems"},{"location":"v3/overview/#on-other-batch-systems","text":"For non-HTCondor batch systems, the Job Router transforms the grid job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below): Submitter: the HTCondor job ID in the original queue HTCondor-CE: the incoming grid job\u2019s ID and the routed job\u2019s ID HTCondor batch system: the batch system\u2019s job ID Although the following figure specifies the PBS case, it applies to all non-HTCondor batch systems: With non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its \"spool\" directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes.","title":"On other batch systems"},{"location":"v3/overview/#hosted-ce-over-ssh","text":"The Hosted CE is designed to be an HTCondor-CE as a Service offered by central grid operations. Hosted CEs submit jobs to remote clusters over SSH, providing a simple starting point for opportunistic resource owners that want to start contributing to a computing grid with minimal effort. If your site intends to run over 10,000 concurrent grid jobs, you will need to host your own HTCondor-CE because the Hosted CE has not yet been optimized for such loads.","title":"Hosted CE over SSH"},{"location":"v3/overview/#how-the-ce-is-customized","text":"Aside from the basic configuration required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all): Deciding which Virtual Organizations (VOs) are allowed to run at your site: HTCondor-CE leverages HTCondor's built-in ability to authenticate incoming jobs based on their GSI credentials, including VOMS attributes. Additionally, HTCondor may be configured to callout to external authentication services like Argus or LCMAPS. How to filter and transform the grid jobs to be run on your batch system: Filtering and transforming grid jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the batch system integration page.","title":"How the CE is Customized"},{"location":"v3/overview/#how-security-works","text":"In the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certificates. When these clients and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust. In its default configuration, HTCondor-CE uses GSI-based authentication and authorization to verify the certificate chain, which will work with technologies such as LCMAPS or Argus. Additionally, it can be reconfigured to provide alternate authentication mechanisms such as token, Kerberos, SSL, shared secret, or even IP-based authentication. More information about authorization methods can be found here .","title":"How Security Works"},{"location":"v3/overview/#getting-help","text":"If you have any questions about the architecture of HTCondor-CE, please contact us for assistance.","title":"Getting Help"},{"location":"v3/reference/","text":"Reference \u00b6 Configuration \u00b6 The following directories contain the configuration for HTCondor-CE. The directories are parsed in the order presented and thus configuration within the final directory will override configuration specified in the previous directories. Location Comment /usr/share/condor-ce/config.d/ Configuration defaults (overwritten on package updates) /etc/condor-ce/config.d/ Files in this directory are parsed in alphanumeric order (i.e., 99-local.conf will override values in 01-ce-auth.conf ) For a detailed order of the way configuration files are parsed, run the following command: user@host $ condor_ce_config_val -config Users \u00b6 The following users are needed by HTCondor-CE at all sites: User Comment condor The HTCondor-CE will be run as root, but perform most of its operations as the condor user. Certificates \u00b6 File User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem Host key root /etc/grid-security/hostkey.pem Networking \u00b6 Service Name Protocol Port Number Inbound Outbound Comment Htcondor-CE tcp 9619 X HTCondor-CE shared port Allow inbound and outbound network connection to all internal site servers, such as the batch system head-node. Only ephemeral outgoing ports are necessary.","title":"Reference"},{"location":"v3/reference/#reference","text":"","title":"Reference"},{"location":"v3/reference/#configuration","text":"The following directories contain the configuration for HTCondor-CE. The directories are parsed in the order presented and thus configuration within the final directory will override configuration specified in the previous directories. Location Comment /usr/share/condor-ce/config.d/ Configuration defaults (overwritten on package updates) /etc/condor-ce/config.d/ Files in this directory are parsed in alphanumeric order (i.e., 99-local.conf will override values in 01-ce-auth.conf ) For a detailed order of the way configuration files are parsed, run the following command: user@host $ condor_ce_config_val -config","title":"Configuration"},{"location":"v3/reference/#users","text":"The following users are needed by HTCondor-CE at all sites: User Comment condor The HTCondor-CE will be run as root, but perform most of its operations as the condor user.","title":"Users"},{"location":"v3/reference/#certificates","text":"File User that owns certificate Path to certificate Host certificate root /etc/grid-security/hostcert.pem Host key root /etc/grid-security/hostkey.pem","title":"Certificates"},{"location":"v3/reference/#networking","text":"Service Name Protocol Port Number Inbound Outbound Comment Htcondor-CE tcp 9619 X HTCondor-CE shared port Allow inbound and outbound network connection to all internal site servers, such as the batch system head-node. Only ephemeral outgoing ports are necessary.","title":"Networking"},{"location":"v3/releases/","text":"Releases \u00b6 HTCondor-CE 3 is distributed via RPM and is available from the following Yum repositories: HTCondor stable Open Science Grid HTCondor-CE 3 Version History \u00b6 This section contains release notes for each version of HTCondor-CE 3. HTCondor-CE version history can be found on GitHub . 3.4.3 \u00b6 This release includes the following bugfixes: Fix a stacktrace with the BDII provider when HTCONDORCE_SPEC isn't defined in the local HTCondor configuration Fixed a race condition that could result in removed jobs being put on hold 3.4.2 \u00b6 This release includes the following new features: Identify broken job routes upon startup Add benchmarking parameters to the BDII provider via HTCONDORCE_SPEC in the configuration. See /etc/condor-ce/config.d/99-ce-bdii.conf for examples. This release also includes the following bug-fixes: Replace APEL uploader SchedD cron with init and systemd services Fix reference to old BDII configuration values 3.4.1 \u00b6 This release includes improvements to the GLUE2 validation for the BDII provider. 3.4.0 \u00b6 This release includes the following new features: Added the ability to configure the environment of routed jobs: Administrators may now add or override environment variables for resultant batch system jobs. Simplified APEL configuration : HTCondor-CE provides appropriate default configuration for its APEL scripts so administrators only need to configure their HTCondor worker nodes as well as the APEL parser, client, and SSM. Details can be found in the documentation . This release also includes the following bug-fixes: Refined the APEL record filter to ignore jobs that have not yet started Improved BDII provider error handling 3.3.0 \u00b6 Add APEL support for HTCondor-CE and HTCondor backends Store malformed ads reporting to htcondor-ce-collector 3.2.2 \u00b6 Make blahp requirement binary package specific ( SOFTWARE-3623 ) Added condor_ce_store_cred for PASSWORD authentication Use new multi-line syntax configuration syntax ( SOFTWARE-3637 ) Update MyOSG URL and queries 3.2.1 \u00b6 Explicitly set ALLOW_READ to support HTCondor 8.9 ( SOFTWARE-3538 ) Add timeouts to the BDII provider 3.2.0 \u00b6 Map certs with VOMS attr before local daemons ( SOFTWARE-3489 ) Send CEView keepalives as the condor user ( SOFTWARE-3486 ) 3.1.4 \u00b6 Fix condor_ce_trace failures caused by transferring /usr/bin/env ( SOFTWARE-3387 ) Fix regex for RDIG certs (SOFTWARE-3399) Don't require authz check for condor_ce_q ( SOFTWARE-3414 ) 3.1.3 \u00b6 Fix condor_ce_info_status using the wrong port for the central collector ( SOFTWARE-3381 ) 3.1.2-3 \u00b6 Ensure that all BDII files exist for the condor repository 3.1.2-2 \u00b6 This version enables building of the BDII sub-package for the condor repository 3.1.2 \u00b6 Require voms-clients-cpp explicitly ( SOFTWARE-3201 ) Add daemon mapping for Let's Encrypt certificates ( SOFTWARE-3236 ) 3.1.1 \u00b6 Allow InCommon host certs Drop vestigal HTCondor-related configuration Add documentation for mapping multiple VOMS attributes 3.1.0 \u00b6 Removed OSG-specific code and configuration from builds intended for the HTCondor repo Updated the CERN BDII provider Removed packaging necessary for EL5 builds 3.0.4 \u00b6 Handle missing MyType attribute in condor 8.7.5 3.0.3 \u00b6 Fix condor_ce_ping with IPv6 addresses ( SOFTWARE-3030 ) Fix for CEView being killed after 24h ( SOFTWARE-2820 ) Import the web_utils library for condor_ce_metric 3.0.2 \u00b6 Fix traceback if JOB_ROUTER_ENTRIES not present ( SOFTWARE-2814 ) Improve POSIX compatability 3.0.1 \u00b6 Fix bug that resulted in losing track of some payload jobs in the Collector audit plugin 3.0.0 \u00b6 HTCondor-CE 3.0.0 adds user job auditing using HTCondor's collector plugin feature, which requires HTCondor 8.6.5. Add the audit_payloads function. This logs the starting and stopping of all payloads that were started from pilot systems based on condor. Do not hold jobs with expired proxy ( SOFTWARE-2803 ) Only warn about configuration if osg-configure is present ( SOFTWARE-2805 ) CEView VO tab throws 500 error on inital installation ( SOFTWARE-2826 ) Getting Help \u00b6 If you have any questions about the release process or run into issues with an upgrade, please contact us for assistance.","title":"Releases"},{"location":"v3/releases/#releases","text":"HTCondor-CE 3 is distributed via RPM and is available from the following Yum repositories: HTCondor stable Open Science Grid","title":"Releases"},{"location":"v3/releases/#htcondor-ce-3-version-history","text":"This section contains release notes for each version of HTCondor-CE 3. HTCondor-CE version history can be found on GitHub .","title":"HTCondor-CE 3 Version History"},{"location":"v3/releases/#343","text":"This release includes the following bugfixes: Fix a stacktrace with the BDII provider when HTCONDORCE_SPEC isn't defined in the local HTCondor configuration Fixed a race condition that could result in removed jobs being put on hold","title":"3.4.3"},{"location":"v3/releases/#342","text":"This release includes the following new features: Identify broken job routes upon startup Add benchmarking parameters to the BDII provider via HTCONDORCE_SPEC in the configuration. See /etc/condor-ce/config.d/99-ce-bdii.conf for examples. This release also includes the following bug-fixes: Replace APEL uploader SchedD cron with init and systemd services Fix reference to old BDII configuration values","title":"3.4.2"},{"location":"v3/releases/#341","text":"This release includes improvements to the GLUE2 validation for the BDII provider.","title":"3.4.1"},{"location":"v3/releases/#340","text":"This release includes the following new features: Added the ability to configure the environment of routed jobs: Administrators may now add or override environment variables for resultant batch system jobs. Simplified APEL configuration : HTCondor-CE provides appropriate default configuration for its APEL scripts so administrators only need to configure their HTCondor worker nodes as well as the APEL parser, client, and SSM. Details can be found in the documentation . This release also includes the following bug-fixes: Refined the APEL record filter to ignore jobs that have not yet started Improved BDII provider error handling","title":"3.4.0"},{"location":"v3/releases/#330","text":"Add APEL support for HTCondor-CE and HTCondor backends Store malformed ads reporting to htcondor-ce-collector","title":"3.3.0"},{"location":"v3/releases/#322","text":"Make blahp requirement binary package specific ( SOFTWARE-3623 ) Added condor_ce_store_cred for PASSWORD authentication Use new multi-line syntax configuration syntax ( SOFTWARE-3637 ) Update MyOSG URL and queries","title":"3.2.2"},{"location":"v3/releases/#321","text":"Explicitly set ALLOW_READ to support HTCondor 8.9 ( SOFTWARE-3538 ) Add timeouts to the BDII provider","title":"3.2.1"},{"location":"v3/releases/#320","text":"Map certs with VOMS attr before local daemons ( SOFTWARE-3489 ) Send CEView keepalives as the condor user ( SOFTWARE-3486 )","title":"3.2.0"},{"location":"v3/releases/#314","text":"Fix condor_ce_trace failures caused by transferring /usr/bin/env ( SOFTWARE-3387 ) Fix regex for RDIG certs (SOFTWARE-3399) Don't require authz check for condor_ce_q ( SOFTWARE-3414 )","title":"3.1.4"},{"location":"v3/releases/#313","text":"Fix condor_ce_info_status using the wrong port for the central collector ( SOFTWARE-3381 )","title":"3.1.3"},{"location":"v3/releases/#312-3","text":"Ensure that all BDII files exist for the condor repository","title":"3.1.2-3"},{"location":"v3/releases/#312-2","text":"This version enables building of the BDII sub-package for the condor repository","title":"3.1.2-2"},{"location":"v3/releases/#312","text":"Require voms-clients-cpp explicitly ( SOFTWARE-3201 ) Add daemon mapping for Let's Encrypt certificates ( SOFTWARE-3236 )","title":"3.1.2"},{"location":"v3/releases/#311","text":"Allow InCommon host certs Drop vestigal HTCondor-related configuration Add documentation for mapping multiple VOMS attributes","title":"3.1.1"},{"location":"v3/releases/#310","text":"Removed OSG-specific code and configuration from builds intended for the HTCondor repo Updated the CERN BDII provider Removed packaging necessary for EL5 builds","title":"3.1.0"},{"location":"v3/releases/#304","text":"Handle missing MyType attribute in condor 8.7.5","title":"3.0.4"},{"location":"v3/releases/#303","text":"Fix condor_ce_ping with IPv6 addresses ( SOFTWARE-3030 ) Fix for CEView being killed after 24h ( SOFTWARE-2820 ) Import the web_utils library for condor_ce_metric","title":"3.0.3"},{"location":"v3/releases/#302","text":"Fix traceback if JOB_ROUTER_ENTRIES not present ( SOFTWARE-2814 ) Improve POSIX compatability","title":"3.0.2"},{"location":"v3/releases/#301","text":"Fix bug that resulted in losing track of some payload jobs in the Collector audit plugin","title":"3.0.1"},{"location":"v3/releases/#300","text":"HTCondor-CE 3.0.0 adds user job auditing using HTCondor's collector plugin feature, which requires HTCondor 8.6.5. Add the audit_payloads function. This logs the starting and stopping of all payloads that were started from pilot systems based on condor. Do not hold jobs with expired proxy ( SOFTWARE-2803 ) Only warn about configuration if osg-configure is present ( SOFTWARE-2805 ) CEView VO tab throws 500 error on inital installation ( SOFTWARE-2826 )","title":"3.0.0"},{"location":"v3/releases/#getting-help","text":"If you have any questions about the release process or run into issues with an upgrade, please contact us for assistance.","title":"Getting Help"},{"location":"v3/verification/","text":"Verifying an HTCondor-CE \u00b6 To verify that you have a working installation of HTCondor-CE, ensure that all the relevant services are started and enabled then perform the validation steps below. Managing HTCondor-CE services \u00b6 In addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Fetch CRL fetch-crl-boot and fetch-crl-cron Your batch system condor or pbs_server or \u2026 HTCondor-CE condor-ce (Optional) APEL uploader condor-ce-apel (Optional, EL7-only) APEL uploader condor-ce-apel.timer Start and enable the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... On EL6, run the command... On EL7, run the command... Start a service service <SERVICE-NAME> start systemctl start <SERVICE-NAME> Stop a service service <SERVICE-NAME> stop systemctl stop <SERVICE-NAME> Enable a service to start on boot chkconfig <SERVICE-NAME> on systemctl enable <SERVICE-NAME> Disable a service from starting on boot chkconfig <SERVICE-NAME> off systemctl disable <SERVICE-NAME> Validating HTCondor-CE \u00b6 To validate an HTCondor-CE, perform the following steps: Verify that local job submissions complete successfully from the CE host. For example, if you have a Slurm cluster, run sbatch from the CE and verify that it runs and completes with scontrol and sacct . Verify that all the necessary daemons are running with condor_ce_status -any . Verify the CE's network configuration using condor_ce_host_network_check . Verify that jobs can complete successfully using condor_ce_trace . Getting Help \u00b6 If any of the above validation steps fail, consult the troubleshooting guide . If that still doesn't resolve your issue, please contact us for assistance.","title":"Verification"},{"location":"v3/verification/#verifying-an-htcondor-ce","text":"To verify that you have a working installation of HTCondor-CE, ensure that all the relevant services are started and enabled then perform the validation steps below.","title":"Verifying an HTCondor-CE"},{"location":"v3/verification/#managing-htcondor-ce-services","text":"In addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation. The specific services are: Software Service name Fetch CRL fetch-crl-boot and fetch-crl-cron Your batch system condor or pbs_server or \u2026 HTCondor-CE condor-ce (Optional) APEL uploader condor-ce-apel (Optional, EL7-only) APEL uploader condor-ce-apel.timer Start and enable the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as root ): To... On EL6, run the command... On EL7, run the command... Start a service service <SERVICE-NAME> start systemctl start <SERVICE-NAME> Stop a service service <SERVICE-NAME> stop systemctl stop <SERVICE-NAME> Enable a service to start on boot chkconfig <SERVICE-NAME> on systemctl enable <SERVICE-NAME> Disable a service from starting on boot chkconfig <SERVICE-NAME> off systemctl disable <SERVICE-NAME>","title":"Managing HTCondor-CE services"},{"location":"v3/verification/#validating-htcondor-ce","text":"To validate an HTCondor-CE, perform the following steps: Verify that local job submissions complete successfully from the CE host. For example, if you have a Slurm cluster, run sbatch from the CE and verify that it runs and completes with scontrol and sacct . Verify that all the necessary daemons are running with condor_ce_status -any . Verify the CE's network configuration using condor_ce_host_network_check . Verify that jobs can complete successfully using condor_ce_trace .","title":"Validating HTCondor-CE"},{"location":"v3/verification/#getting-help","text":"If any of the above validation steps fail, consult the troubleshooting guide . If that still doesn't resolve your issue, please contact us for assistance.","title":"Getting Help"},{"location":"v3/installation/central-collector/","text":"Installing an HTCondor-CE Central Collector \u00b6","title":"Central collector"},{"location":"v3/installation/central-collector/#installing-an-htcondor-ce-central-collector","text":"","title":"Installing an HTCondor-CE Central Collector"},{"location":"v3/installation/hosted-ce/","text":"Installing a Hosted CE \u00b6","title":"Hosted ce"},{"location":"v3/installation/hosted-ce/#installing-a-hosted-ce","text":"","title":"Installing a Hosted CE"},{"location":"v3/installation/htcondor-ce/","text":"Installing an HTCondor-CE \u00b6 Note If you are installing an HTCondor-CE for the Open Science Grid (OSG), consult the OSG-specific documentation . The HTCondor-CE software is a job gateway based on HTCondor for Compute Entrypoints (CE) belonging to a computing grid (e.g. European Grid Infrastructure , Open Science Grid ). As such, HTCondor-CE serves as an entry point for incoming grid jobs \u2014 it handles authorization and delegation of jobs to a grid site's local batch system. See the overview page for more details on the features and architecture of HTCondor-CE. Use this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the HTCondor Yum repositories . Before Starting \u00b6 Before starting the installation process, consider the following points (consulting the reference page as necessary): User IDs: If they do not exist already, the installation will create the condor Linux user (UID 4716) SSL certificate: The HTCondor-CE service uses a host certificate at /etc/grid-security/hostcert.pem and an accompanying key at /etc/grid-security/hostkey.pem DNS entries: Forward and reverse DNS must resolve for the HTCondor-CE host Network ports: The pilot factories must be able to contact your HTCondor-CE service on port 9619 (TCP) Submit host: HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster running supported batch system software (Grid Engine, HTCondor, LSF, PBS/Torque, Slurm) File Systems : Non-HTCondor batch systems require a shared file system between the HTCondor-CE host and the batch system worker nodes. There are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system (Red Hat Enterprise Linux variant 7) Obtain root access to the host Prepare the EPEL and HTCondor Yum repositories Install CA certificates and VO data into /etc/grid-security/certificates and /etc/grid-security/vomsdir , respectively Installing HTCondor-CE \u00b6 Important HTCondor-CE must be installed on a host that is configured to submit jobs to your batch system. The details of this setup is site-specific by nature and therefore beyond the scope of this document. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the fetch-crl package, available from the EPEL repositories. root@host # yum install fetch-crl Select the appropriate convenience RPM: If your batch system is... Then use the following package... Grid Engine htcondor-ce-sge HTCondor htcondor-ce-condor LSF htcondor-ce-lsf PBS/Torque htcondor-ce-pbs SLURM htcondor-ce-slurm Install the CE software: root@host # yum install <PACKAGE> Where <PACKAGE> is the package you selected in the above step. Configuring HTCondor-CE \u00b6 There are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on optional configurations . Configuring authentication \u00b6 To authenticate job submission from external users and VOs, HTCondor-CE can be configured to use a built-in mapfile or to make Globus callouts to an external service like Argus or LCMAPS. THe former option is simpler but the latter option may be preferred if your grid supports it or your site already runs such a service. Built-in mapfile \u00b6 The built-in mapfile is a unified HTCondor mapfile located at /etc/condor-ce/condor_mapfile . This file is parsed in line-by-line order and HTCondor-CE will use the first line that matches. Therefore, mappings should be added to the top of the file. Warning condor_mapfile.rpmnew files may be generated upon HTCondor-CE version updates and they should be merged into condor_mapfile . To configure authorization for users submitting jobs with an X.509 proxy certificate to your HTCondor-CE, add lines of the following format: GSI \"^<DISTINGUISHED NAME>$\" <USERNAME>@users.htcondor.org Replacing <DISTINGUISHED NAME > (escaping any '/' with '\\/') and <USERNAME > with the distinguished name of the incoming certificate and the unix account under which the job should run, respectively. VOMS attributes of incoming X.509 proxy certificates can also be used for mapping: GSI \"<DISTINGUISHED NAME>,<VOMS FQAN 1>,<VOMS FQAN 2>,...,<VOMSFQAN N>\" <USERNAME>@users.htcondor.org Replacing <DISTINGUISHED NAME > (escaping any '/' with '\\/'), <VOMSFQAN > fields, and <USERNAME > with the distinguished name of the incoming certificate, the VOMS roles and groups, and the unix account under which the job should run, respectively. Additionally, you can use regular expressions for mapping certificate and VOMS attribute credentials. For example, to map any certificate from the GLOW VO with the htpc role to the glow user, add the following line: GSI \".*,\\/GLOW\\/Role=htpc.*\" glow@users.htcondor.org Warning You should only add mappings to the mapfile. Do not remove any of the default mappings: GSI \"(/CN=[-.A-Za-z0-9/= ]+)\" \\1@unmapped.htcondor.org CLAIMTOBE .* anonymous@claimtobe FS (.*) \\1 Globus Callout \u00b6 To use a Globus callout to a service like LCMAPS or Argus, you will need to have the relevant library installed as well as the following HTCondor-CE configuration: Add the following line to the top of /etc/condor-ce/condor_mapfile : GSI (.*) GSS_ASSIST_GRIDMAP Create /etc/grid-security/gsi-authz.conf with the following content: For LCMAPS: globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout For Argus: globus_mapping /usr/lib64/libgsi_pep_callout.so argus_pep_callout Configuring the batch system \u00b6 Before HTCondor-CE can submit jobs to your local batch system, it has to be configured to do so. The configuration will differ depending on if your local batch system is HTCondor or one of the other supported batch systems. Choose the section corresponding to your batch system below. HTCondor batch systems \u00b6 To configure HTCondor-CE for an HTCondor batch system, set JOB_ROUTER_SCHEDD2_POOL to your site's central manager host and port: JOB_ROUTER_SCHEDD2_POOL = cm.chtc.wisc.edu:9618 Additionally, set JOB_ROUTER_SCHEDD2_SPOOL to the location of the local batch SPOOL directory on the CE host if it is different than the default location ( /var/lib/condor/spool ). Non-HTCondor batch systems \u00b6 Configuring the BLAHP \u00b6 HTCondor-CE uses the Batch Language ASCII Helper Protocol (BLAHP) to submit and track jobs to non-HTCondor batch systems. To work with the HTCondor-CE, modify /usr/libexec/condor/glite/etc/batch_gahp.config using the following steps: Disable BLAHP handling of certificate proxies: blah_disable_wn_proxy_renewal=yes blah_delegate_renewed_proxies=no blah_disable_limited_proxy=yes (Optional) If your batch system tools are installed in a non-standard location (i.e., outside of /usr/bin/ ), set the corresponding *_binpath variable to the directory containing your batch system tools: If your batch system is... Then change the following configuration variable... LSF lsf_binpath PBS/Torque pbs_binpath SGE sge_binpath Slurm slurm_binpath For example, if your Slurm binaries (e.g. sbatch ) exist in /opt/slurm/bin , you would set the following: slurm_binpath=/opt/slurm/bin/ Sharing the SPOOL directory \u00b6 Non-HTCondor batch systems require a shared file system configuration to support file transfer from the HTCondor-CE to your site's worker nodes. The current recommendation is to run a dedicated NFS server on the CE host . In this setup, HTCondor-CE writes to the local spool directory, the NFS server shares the directory, and each worker node mounts the directory in the same location as on the CE. For example, if your spool directory is /var/lib/condor-ce (the default), you must mount the shared directory to /var/lib/condor-ce on the worker nodes. Note If you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE daemons can write to the spool directory. You can control the value of the spool directory by setting SPOOL in /etc/condor-ce/config.d/99-local.conf (create this file if it doesn't exist). For example, the following sets the SPOOL directory to /home/condor : SPOOL = /home/condor Note The shared spool directory must be readable and writeable by the condor user for HTCondor-CE to function correctly. Optional Configuration \u00b6 The following configuration steps are optional and will not be required for all sites. If you do not need any of the following special configurations, skip to the section on next steps . Configuring for multiple network interfaces \u00b6 If you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname and interface to use when communicating with each other. Set NETWORK_HOSTNAME and NETWORK_INTERFACE to the hostname and IP address of your public interface, respectively, in /etc/condor-ce/config.d/99-local.conf directory with the line: NETWORK_HOSTNAME = condorce.example.com NETWORK_INTERFACE = 127.0.0.1 Replacing condorce.example.com text with your public interface\u2019s hostname and 127.0.0.1 with your public interface\u2019s IP address. Limiting or disabling locally running jobs on the CE \u00b6 If you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and scheduler universes. Local and scheduler universes allow jobs to be run on the CE itself, mainly for remote troubleshooting. Pilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another worker node. The two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be configuring them in unison. To change the default limit on the number of locally run jobs (the current default is 20), add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning < <JOB-LIMIT> START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Where <JOB-LIMIT> is the maximum number of jobs allowed to run locally To only allow a specific user to start locally run jobs, add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = target.Owner =?= \"<USERNAME>\" START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Change <USERNAME> for the username allowed to run jobs locally To disable locally run jobs, add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = False START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Enabling the monitoring web interface \u00b6 The HTCondor-CE View is an optional web interface to the status of your CE. To run the HTCondor-CE View, install the appropriate package and set the relevant configuration. Begin by installing the htcondor-ce-view package: root@host # yum install htcondor-ce-view Next, uncomment the DAEMON_LIST configuration located at /etc/condor-ce/config.d/05-ce-view.conf : DAEMON_LIST = $(DAEMON_LIST), CEVIEW, GANGLIAD, SCHEDD Restart the condor-ce service Verify the service by entering your CE's hostname into your web browser The website is served on port 80 by default. To change this default, edit the value of HTCONDORCE_VIEW_PORT in /etc/condor-ce/config.d/05-ce-view.conf . Uploading accounting records to APEL \u00b6 For sites outside of the OSG that need to upload the APEL accounting records, HTCondor-CE supports uploading batch and blah APEL records for HTCondor batch systems: Install the HTCondor-CE APEL package on your CE host: root@host # yum install htcondor-ce-apel On each worker node, set the appropriate scaling factor in the HTCondor configuration (i.e. /etc/condor/config.d/ ) and advertise it in the startd ad: ApelScaling = <SCALING FACTOR> # For example, 1.062 STARTD_ATTRS = $(STARTD_ATTRS) ApelScaling Configure the APEL parser, client, and SSM Records are written to APEL_OUTPUT_DIR in the HTCondor-CE configuration (default: /var/lib/condor-ce/apel/ ) Batch and blah record filenames are prefixed batch- and blah- , respectively Start and enable the condor-ce-apel services appropriate for your operating system. Enabling BDII integration \u00b6 HTCondor-CE supports reporting BDII information for all HTCondor-CE endpoints and batch information for an HTCondor batch system. To make this information available, perform the following instructions on your site BDII host. Install the HTCondor-CE BDII package: root@host # yum install htcondor-ce-bdii Configure HTCondor ( /etc/condor/config.d/ ) on your site BDII host to point to your central manager: CONDOR_HOST = <CENTRAL MANAGER> Replacing <CENTRAL MANAGER> with the hostname of your HTCondor central manager Configure BDII static information by modifying /etc/condor/config.d/99-ce-bdii.conf Additionally, install the HTCondor-CE BDII package on each of your HTCondor-CE hosts: root@host # yum install htcondor-ce-bdii Next Steps \u00b6 At this point, you should have an installation of HTCondor-CE that will forward grid jobs into your site's batch system unchanged. If you need to transform incoming grid jobs (e.g. by setting a partition, queue, or accounting group), configure the HTCondor-CE Job Router . Otherwise, continue to the this document to start the relevant services and verify your installation. Getting Help \u00b6 If you have any questions or issues with the installation process, please contact us for assistance,","title":"Installation"},{"location":"v3/installation/htcondor-ce/#installing-an-htcondor-ce","text":"Note If you are installing an HTCondor-CE for the Open Science Grid (OSG), consult the OSG-specific documentation . The HTCondor-CE software is a job gateway based on HTCondor for Compute Entrypoints (CE) belonging to a computing grid (e.g. European Grid Infrastructure , Open Science Grid ). As such, HTCondor-CE serves as an entry point for incoming grid jobs \u2014 it handles authorization and delegation of jobs to a grid site's local batch system. See the overview page for more details on the features and architecture of HTCondor-CE. Use this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the HTCondor Yum repositories .","title":"Installing an HTCondor-CE"},{"location":"v3/installation/htcondor-ce/#before-starting","text":"Before starting the installation process, consider the following points (consulting the reference page as necessary): User IDs: If they do not exist already, the installation will create the condor Linux user (UID 4716) SSL certificate: The HTCondor-CE service uses a host certificate at /etc/grid-security/hostcert.pem and an accompanying key at /etc/grid-security/hostkey.pem DNS entries: Forward and reverse DNS must resolve for the HTCondor-CE host Network ports: The pilot factories must be able to contact your HTCondor-CE service on port 9619 (TCP) Submit host: HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster running supported batch system software (Grid Engine, HTCondor, LSF, PBS/Torque, Slurm) File Systems : Non-HTCondor batch systems require a shared file system between the HTCondor-CE host and the batch system worker nodes. There are some one-time (per host) steps to prepare in advance: Ensure the host has a supported operating system (Red Hat Enterprise Linux variant 7) Obtain root access to the host Prepare the EPEL and HTCondor Yum repositories Install CA certificates and VO data into /etc/grid-security/certificates and /etc/grid-security/vomsdir , respectively","title":"Before Starting"},{"location":"v3/installation/htcondor-ce/#installing-htcondor-ce","text":"Important HTCondor-CE must be installed on a host that is configured to submit jobs to your batch system. The details of this setup is site-specific by nature and therefore beyond the scope of this document. Clean yum cache: root@host # yum clean all --enablerepo = * Update software: root@host # yum update This command will update all packages Install the fetch-crl package, available from the EPEL repositories. root@host # yum install fetch-crl Select the appropriate convenience RPM: If your batch system is... Then use the following package... Grid Engine htcondor-ce-sge HTCondor htcondor-ce-condor LSF htcondor-ce-lsf PBS/Torque htcondor-ce-pbs SLURM htcondor-ce-slurm Install the CE software: root@host # yum install <PACKAGE> Where <PACKAGE> is the package you selected in the above step.","title":"Installing HTCondor-CE"},{"location":"v3/installation/htcondor-ce/#configuring-htcondor-ce","text":"There are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on optional configurations .","title":"Configuring HTCondor-CE"},{"location":"v3/installation/htcondor-ce/#configuring-authentication","text":"To authenticate job submission from external users and VOs, HTCondor-CE can be configured to use a built-in mapfile or to make Globus callouts to an external service like Argus or LCMAPS. THe former option is simpler but the latter option may be preferred if your grid supports it or your site already runs such a service.","title":"Configuring authentication"},{"location":"v3/installation/htcondor-ce/#built-in-mapfile","text":"The built-in mapfile is a unified HTCondor mapfile located at /etc/condor-ce/condor_mapfile . This file is parsed in line-by-line order and HTCondor-CE will use the first line that matches. Therefore, mappings should be added to the top of the file. Warning condor_mapfile.rpmnew files may be generated upon HTCondor-CE version updates and they should be merged into condor_mapfile . To configure authorization for users submitting jobs with an X.509 proxy certificate to your HTCondor-CE, add lines of the following format: GSI \"^<DISTINGUISHED NAME>$\" <USERNAME>@users.htcondor.org Replacing <DISTINGUISHED NAME > (escaping any '/' with '\\/') and <USERNAME > with the distinguished name of the incoming certificate and the unix account under which the job should run, respectively. VOMS attributes of incoming X.509 proxy certificates can also be used for mapping: GSI \"<DISTINGUISHED NAME>,<VOMS FQAN 1>,<VOMS FQAN 2>,...,<VOMSFQAN N>\" <USERNAME>@users.htcondor.org Replacing <DISTINGUISHED NAME > (escaping any '/' with '\\/'), <VOMSFQAN > fields, and <USERNAME > with the distinguished name of the incoming certificate, the VOMS roles and groups, and the unix account under which the job should run, respectively. Additionally, you can use regular expressions for mapping certificate and VOMS attribute credentials. For example, to map any certificate from the GLOW VO with the htpc role to the glow user, add the following line: GSI \".*,\\/GLOW\\/Role=htpc.*\" glow@users.htcondor.org Warning You should only add mappings to the mapfile. Do not remove any of the default mappings: GSI \"(/CN=[-.A-Za-z0-9/= ]+)\" \\1@unmapped.htcondor.org CLAIMTOBE .* anonymous@claimtobe FS (.*) \\1","title":"Built-in mapfile"},{"location":"v3/installation/htcondor-ce/#globus-callout","text":"To use a Globus callout to a service like LCMAPS or Argus, you will need to have the relevant library installed as well as the following HTCondor-CE configuration: Add the following line to the top of /etc/condor-ce/condor_mapfile : GSI (.*) GSS_ASSIST_GRIDMAP Create /etc/grid-security/gsi-authz.conf with the following content: For LCMAPS: globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout For Argus: globus_mapping /usr/lib64/libgsi_pep_callout.so argus_pep_callout","title":"Globus Callout"},{"location":"v3/installation/htcondor-ce/#configuring-the-batch-system","text":"Before HTCondor-CE can submit jobs to your local batch system, it has to be configured to do so. The configuration will differ depending on if your local batch system is HTCondor or one of the other supported batch systems. Choose the section corresponding to your batch system below.","title":"Configuring the batch system"},{"location":"v3/installation/htcondor-ce/#htcondor-batch-systems","text":"To configure HTCondor-CE for an HTCondor batch system, set JOB_ROUTER_SCHEDD2_POOL to your site's central manager host and port: JOB_ROUTER_SCHEDD2_POOL = cm.chtc.wisc.edu:9618 Additionally, set JOB_ROUTER_SCHEDD2_SPOOL to the location of the local batch SPOOL directory on the CE host if it is different than the default location ( /var/lib/condor/spool ).","title":"HTCondor batch systems"},{"location":"v3/installation/htcondor-ce/#non-htcondor-batch-systems","text":"","title":"Non-HTCondor batch systems"},{"location":"v3/installation/htcondor-ce/#configuring-the-blahp","text":"HTCondor-CE uses the Batch Language ASCII Helper Protocol (BLAHP) to submit and track jobs to non-HTCondor batch systems. To work with the HTCondor-CE, modify /usr/libexec/condor/glite/etc/batch_gahp.config using the following steps: Disable BLAHP handling of certificate proxies: blah_disable_wn_proxy_renewal=yes blah_delegate_renewed_proxies=no blah_disable_limited_proxy=yes (Optional) If your batch system tools are installed in a non-standard location (i.e., outside of /usr/bin/ ), set the corresponding *_binpath variable to the directory containing your batch system tools: If your batch system is... Then change the following configuration variable... LSF lsf_binpath PBS/Torque pbs_binpath SGE sge_binpath Slurm slurm_binpath For example, if your Slurm binaries (e.g. sbatch ) exist in /opt/slurm/bin , you would set the following: slurm_binpath=/opt/slurm/bin/","title":"Configuring the BLAHP"},{"location":"v3/installation/htcondor-ce/#sharing-the-spool-directory","text":"Non-HTCondor batch systems require a shared file system configuration to support file transfer from the HTCondor-CE to your site's worker nodes. The current recommendation is to run a dedicated NFS server on the CE host . In this setup, HTCondor-CE writes to the local spool directory, the NFS server shares the directory, and each worker node mounts the directory in the same location as on the CE. For example, if your spool directory is /var/lib/condor-ce (the default), you must mount the shared directory to /var/lib/condor-ce on the worker nodes. Note If you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE daemons can write to the spool directory. You can control the value of the spool directory by setting SPOOL in /etc/condor-ce/config.d/99-local.conf (create this file if it doesn't exist). For example, the following sets the SPOOL directory to /home/condor : SPOOL = /home/condor Note The shared spool directory must be readable and writeable by the condor user for HTCondor-CE to function correctly.","title":"Sharing the SPOOL directory"},{"location":"v3/installation/htcondor-ce/#optional-configuration","text":"The following configuration steps are optional and will not be required for all sites. If you do not need any of the following special configurations, skip to the section on next steps .","title":"Optional Configuration"},{"location":"v3/installation/htcondor-ce/#configuring-for-multiple-network-interfaces","text":"If you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname and interface to use when communicating with each other. Set NETWORK_HOSTNAME and NETWORK_INTERFACE to the hostname and IP address of your public interface, respectively, in /etc/condor-ce/config.d/99-local.conf directory with the line: NETWORK_HOSTNAME = condorce.example.com NETWORK_INTERFACE = 127.0.0.1 Replacing condorce.example.com text with your public interface\u2019s hostname and 127.0.0.1 with your public interface\u2019s IP address.","title":"Configuring for multiple network interfaces"},{"location":"v3/installation/htcondor-ce/#limiting-or-disabling-locally-running-jobs-on-the-ce","text":"If you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and scheduler universes. Local and scheduler universes allow jobs to be run on the CE itself, mainly for remote troubleshooting. Pilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another worker node. The two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be configuring them in unison. To change the default limit on the number of locally run jobs (the current default is 20), add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning < <JOB-LIMIT> START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Where <JOB-LIMIT> is the maximum number of jobs allowed to run locally To only allow a specific user to start locally run jobs, add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = target.Owner =?= \"<USERNAME>\" START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) Change <USERNAME> for the username allowed to run jobs locally To disable locally run jobs, add the following to /etc/condor-ce/config.d/99-local.conf : START_LOCAL_UNIVERSE = False START_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)","title":"Limiting or disabling locally running jobs on the CE"},{"location":"v3/installation/htcondor-ce/#enabling-the-monitoring-web-interface","text":"The HTCondor-CE View is an optional web interface to the status of your CE. To run the HTCondor-CE View, install the appropriate package and set the relevant configuration. Begin by installing the htcondor-ce-view package: root@host # yum install htcondor-ce-view Next, uncomment the DAEMON_LIST configuration located at /etc/condor-ce/config.d/05-ce-view.conf : DAEMON_LIST = $(DAEMON_LIST), CEVIEW, GANGLIAD, SCHEDD Restart the condor-ce service Verify the service by entering your CE's hostname into your web browser The website is served on port 80 by default. To change this default, edit the value of HTCONDORCE_VIEW_PORT in /etc/condor-ce/config.d/05-ce-view.conf .","title":"Enabling the monitoring web interface"},{"location":"v3/installation/htcondor-ce/#uploading-accounting-records-to-apel","text":"For sites outside of the OSG that need to upload the APEL accounting records, HTCondor-CE supports uploading batch and blah APEL records for HTCondor batch systems: Install the HTCondor-CE APEL package on your CE host: root@host # yum install htcondor-ce-apel On each worker node, set the appropriate scaling factor in the HTCondor configuration (i.e. /etc/condor/config.d/ ) and advertise it in the startd ad: ApelScaling = <SCALING FACTOR> # For example, 1.062 STARTD_ATTRS = $(STARTD_ATTRS) ApelScaling Configure the APEL parser, client, and SSM Records are written to APEL_OUTPUT_DIR in the HTCondor-CE configuration (default: /var/lib/condor-ce/apel/ ) Batch and blah record filenames are prefixed batch- and blah- , respectively Start and enable the condor-ce-apel services appropriate for your operating system.","title":"Uploading accounting records to APEL"},{"location":"v3/installation/htcondor-ce/#enabling-bdii-integration","text":"HTCondor-CE supports reporting BDII information for all HTCondor-CE endpoints and batch information for an HTCondor batch system. To make this information available, perform the following instructions on your site BDII host. Install the HTCondor-CE BDII package: root@host # yum install htcondor-ce-bdii Configure HTCondor ( /etc/condor/config.d/ ) on your site BDII host to point to your central manager: CONDOR_HOST = <CENTRAL MANAGER> Replacing <CENTRAL MANAGER> with the hostname of your HTCondor central manager Configure BDII static information by modifying /etc/condor/config.d/99-ce-bdii.conf Additionally, install the HTCondor-CE BDII package on each of your HTCondor-CE hosts: root@host # yum install htcondor-ce-bdii","title":"Enabling BDII integration"},{"location":"v3/installation/htcondor-ce/#next-steps","text":"At this point, you should have an installation of HTCondor-CE that will forward grid jobs into your site's batch system unchanged. If you need to transform incoming grid jobs (e.g. by setting a partition, queue, or accounting group), configure the HTCondor-CE Job Router . Otherwise, continue to the this document to start the relevant services and verify your installation.","title":"Next Steps"},{"location":"v3/installation/htcondor-ce/#getting-help","text":"If you have any questions or issues with the installation process, please contact us for assistance,","title":"Getting Help"},{"location":"v3/troubleshooting/common-issues/","text":"Common Issues \u00b6","title":"Common issues"},{"location":"v3/troubleshooting/common-issues/#common-issues","text":"","title":"Common Issues"},{"location":"v3/troubleshooting/debugging-tools/","text":"Debugging Tools \u00b6","title":"Debugging tools"},{"location":"v3/troubleshooting/debugging-tools/#debugging-tools","text":"","title":"Debugging Tools"},{"location":"v3/troubleshooting/logs/","text":"Helpful Logs \u00b6","title":"Logs"},{"location":"v3/troubleshooting/logs/#helpful-logs","text":"","title":"Helpful Logs"},{"location":"v3/troubleshooting/troubleshooting/","text":"HTCondor-CE Troubleshooting Guide \u00b6 In this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of common issues with suggested troubleshooting steps. Known Issues \u00b6 SUBMIT_ATTRS are not applied to jobs on the local HTCondor \u00b6 If you are adding attributes to jobs submitted to your HTCondor pool with SUBMIT_ATTRS , these will not be applied to jobs that are entering your pool from the HTCondor-CE. To get around this, you will want to add the attributes to your job routes . If the CE is the only entry point for jobs into your pool, you can get rid of SUBMIT_ATTRS on your backend. Otherwise, you will have to maintain your list of attributes both in your list of routes and in your SUBMIT_ATTRS . General Troubleshooting Items \u00b6 Making sure packages are up-to-date \u00b6 It is important to make sure that the HTCondor-CE and related RPMs are up-to-date. root@host # yum update \"htcondor-ce*\" blahp condor If you just want to see the packages to update, but do not want to perform the update now, answer N at the prompt. Verify package contents \u00b6 If the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the contents of your packages (ignoring changes to configuration files): user@host $ rpm -q --verify htcondor-ce htcondor-ce-client blahp | grep -v '/var/' | awk '$2 != \"c\" {print $0}' If the verification command returns output, this means that your packages have been changed. To fix this, you can reinstall the packages: user@host $ yum reinstall htcondor-ce htcondor-ce-client blahp Note The reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an .rpmnew suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration. Verify clocks are synchronized \u00b6 Like all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is synchronized using a utility such as ntpd . Additionally, HTCondor itself is sensitive to time skews on the NFS server. If you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew. Verify host cerificates and CRLs are valid \u00b6 An expired host certificate or CRLs will cause various issues with GSI authentication. Verify that your host certificate is valid by running: root@host # openssl x509 -in /etc/grid-security/hostcert.pem -noout -dates Likewise, run the fetch-crl script to update your CRLs: root@host # fetch-crl If updating CRLs fix your issues, make sure that the fetch-crl-cron and fetch-crl-boot services are enabled and running. HTCondor-CE Troubleshooting Items \u00b6 This section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. Before troubleshooting, we recommend increasing the log level: Write the following into /etc/condor-ce/config.d/99-local.conf to increase the log level for all daemons: ALL_DEBUG = D_ALWAYS:2 D_CAT Ensure that the configuration is in place: root@host # condor_ce_reconfig Reproduce the issue Note Before spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running condor_ce_reconfig . Daemons fail to start \u00b6 If there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to startup. Check the following subsections in order: Symptoms Daemon startup failure may manifest in many ways, the following are few symptoms of the problem. The service fails to start: root@host # service condor-ce start Starting Condor-CE daemons: [ FAIL ] condor_ce_q fails with a lengthy error message: user@host $ condor_ce_q Error: Extra Info: You probably saw this error because the condor_schedd is not running on the machine you are trying to query. If the condor_schedd is not running, the Condor system will not be able to find an address and port to connect to and satisfy this request. Please make sure the Condor daemons are running and try again. Extra Info: If the condor_schedd is running on the machine you are trying to query and you still see the error, the most likely cause is that you have setup a personal Condor, you have not defined SCHEDD_NAME in your condor_config file, and something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define either or both of those settings in your config file, or you must use the -name option to condor_q. Please see the Condor manual for details on SCHEDD_NAME and SCHEDD_ADDRESS_FILE. Next actions If the MasterLog is filled with ERROR:SECMAN...TCP connection to collector...failed : This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in this section of the install guide. If the MasterLog is filled with DC_AUTHENTICATE errors: The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in /etc/condor-ce/condor_mapfile . If the SchedLog is filled with Can\u2019t find address for negotiator : You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one. Jobs fail to submit to the CE \u00b6 If a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the culprit, then you may have encountered an authentication or authorization issue. You may see error messages like the following in your SchedLog : 08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa) 08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189 08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done! Next actions Check voms-mapfile or grid-mapfile and ensure that the user's DN or VOMS attributes are known to your authentication method , and that the mapped users exist on your CE and cluster. Check for lcmaps errors in /var/log/messages If you do not see helpful error messages in /var/log/messages , adjust the debug level by adding export LCMAPS_DEBUG_LEVEL=5 to /etc/sysconfig/condor-ce , restarting the condor-ce service, and checking /var/log/messages for errors again. Jobs stay idle on the CE \u00b6 Check the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is busy. Idle jobs on CE: Make sure the underlying batch system can run jobs \u00b6 HTCondor-CE delegates jobs to your batch system, which is then responsible for matching jobs to worker nodes. If you cannot manually submit jobs (e.g., condor_submit , qsub ) on the CE host to your batch system, then HTCondor-CE won't be able to either. Procedure Manually create and submit a simple job (e.g., one that runs sleep ) Check for errors in the submission itself Watch the job in the batch system queue (e.g., condor_q , qstat ) If the job does not run, check for errors on the batch system Next actions Consult troubleshooting documentation or support avenues for your batch system. Once you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again. Idle jobs on CE: Is the job router handling the incoming job? \u00b6 Jobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things if you suspect that the jobs are not being matched. Check if the JobRouter sees a job before that by looking at the job router log and looking for the text src=<JOB-ID>\u2026claimed job . Next actions Use condor_ce_job_router_info to see why your idle job does not match any routes Idle jobs on CE: Verify correct operation between the CE and your local batch system \u00b6 For HTCondor batch systems \u00b6 HTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch system interaction will appear in the JobRouterLog . Next actions Check the JobRouterLog for failures. Verify that the local HTCondor is functional. Use condor_ce_config_val to verify that the JOB_ROUTER_SCHEDD2_NAME , JOB_ROUTER_SCHEDD2_POOL , and JOB_ROUTER_SCHEDD2_SPOOL configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively. Use condor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE and verify that it is set to .* . For non-HTCondor batch systems \u00b6 HTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch system interaction will appear in the GridmanagerLog . Look for gm state change\u2026 lines to figure out where the issures are occuring. Next actions If you see failures in the GridmanagerLog during job submission: Save the submit files by adding the appropriate entry to blah.config and submit it manually to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the <batch system>_binpath in /etc/blah.config . If you see failures in the GridmanagerLog during queries for job status: Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in /usr/libexec/blahp/<batch system>_status.sh (e.g., /usr/libexec/blahp/lsf_status.sh ) that take the argument batch system/YYYMMDD/job ID (e.g., lsf/20141008/65053 ). Run the appropriate status script for your batch system and upon success, you should see the following output: root@host # /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053 [ BatchjobId = \"894862\"; JobStatus = 4; ExitCode = 0; WorkerNode = \"atl-prod08\" ] If the script fails, request help from the OSG. Idle jobs on CE: Verify ability to change permissions on key files \u00b6 HTCondor-CE needs the ability to write and chown files in its spool directory and if it cannot, jobs will not run at all. Spool permission errors can appear in the SchedLog and the JobRouterLog . Symptoms 09/17/14 14:45:42 Error: Unable to chown '/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env' from 12345 to 54321 Next actions As root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions. Verify that there aren't any underlying file system issues in the specified location Jobs stay idle on a remote host submitting to the CE \u00b6 If you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not see a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not authorized to run there. Note that jobs may take several minutes or longer if the CE is busy. Remote idle jobs: Can you contact the CE? \u00b6 To check basic connectivity to a CE, use condor_ce_ping : Symptoms user@host $ condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE ERROR: couldn't locate condorce.example.com! Next actions Make sure that the HTCondor-CE daemons are running with condor_ce_status . Verify that your CE is reachable from your submit host, replacing condorce.example.com with the hostname of your CE: user@host $ ping condorce.example.com Remote idle jobs: Are you authorized to run jobs on the CE? \u00b6 The CE will only run jobs from users that authenticate through the HTCondor-CE configuration . You can use condor_ce_ping to check if you are authorized and what user your proxy is being mapped to. Symptoms user@host $ condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE Remote Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Local Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Session ID: condorce:3343:1412790611:0 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: GSI Remote Mapping: gsi@unmapped Authorized: FALSE Notice the failures in the above message: Remote Mapping: gsi@unmapped and Authorized: FALSE Next actions Verify that an authentication method is set up on the CE Verify that your user DN is mapped to an existing system user Jobs go on hold \u00b6 Jobs will be put on held with a HoldReason attribute that can be inspected with condor_ce_q : user@host $ condor_ce_q -l <JOB-ID> -attr HoldReason HoldReason = \"CE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to no matching routes, route job limit, or route failure threshold.\" Held jobs: no matching routes, route job limit, or route failure threshold \u00b6 Jobs on the CE will be put on hold if they are not claimed by the job router within 30 minutes. The most common cases for this behavior are as follows: The job does not match any job routes: use condor_ce_job_router_info to see why your idle job does not match any routes . The route(s) that the job matches to are full: See limiting the number of jobs . The job router is throttling submission to your batch system due to submission failures: See the HTCondor manual for FailureRateThreshold . Check for errors in the JobRouterLog or GridmanagerLog for HTCondor and non-HTCondor batch systems, respectively. Held jobs: Missing/expired user proxy \u00b6 HTCondor-CE requires a valid user proxy for each job that is submitted. You can check the status of your proxy with the following user@host $ voms-proxy-info -all Next actions Ensure that the owner of the job generates their proxy with voms-proxy-init . Held jobs: Invalid job universe \u00b6 The HTCondor-CE only accepts jobs that have universe in their submit files set to vanilla , standard , local , or scheduler . These universes also have corresponding integer values that can be found in the HTCondor manual . Next actions Ensure jobs submitted locally, from the CE host, are submitted with universe = vanilla Ensure jobs submitted from a remote submit point are submitted with: universe = grid grid_resource = condor condorce.example.com condorce.example.com:9619 replacing condorce.example.com with the hostname of the CE. Identifying the corresponding job ID on the local batch system \u00b6 When troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID and the resultant job ID on the batch system. The methods for finding the resultant job ID differs between batch systems. HTCondor batch systems \u00b6 To inspect the CE\u2019s job ad, use condor_ce_q or condor_ce_history : Use condor_ce_q if the job is still in the CE\u2019s queue: user@host $ condor_ce_q <JOB-ID> -af RoutedToJobId Use condor_ce_history if the job has left the CE\u2019s queue: user@host $ condor_ce_history <JOB-ID> -af RoutedToJobId Parse the JobRouterLog for the CE\u2019s job ID. Non-HTCondor batch systems \u00b6 When HTCondor-CE records the corresponding batch system job ID, it is written in the form <BATCH-SYSTEM>/<DATE>/<JOB ID> : lsf/20141206/482046 To inspect the CE\u2019s job ad, use condor_ce_q : user@host $ condor_ce_q <JOB-ID> -af GridJobId Parse the GridmanagerLog for the CE\u2019s job ID. Jobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only) \u00b6 By design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. Therefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps: Identify the misbehaving job ID in your batch system queue Find the job's corresponding CE job ID: user@host $ condor_q <JOB-ID> -af RoutedFromJobId Use condor_ce_rm to remove the CE job from the queue Missing HTCondor tools \u00b6 Most of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. If you are trying to use HTCondor-CE tools and you see the following error: user@host $ condor_ce_job_router_info /usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found This means that the condor_job_router_info (note this is not the CE version), is not in your PATH . Next Actions Either the condor RPM is missing or there are some other issues with it (try rpm --verify condor ). You have installed HTCondor in a non-standard location that is not in your PATH . The condor_job_router_info tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming). HTCondor-CE Troubleshooting Tools \u00b6 HTCondor-CE has its own separate set of of the HTCondor tools with ce in the name (i.e., condor_ce_submit vs condor_submit ). Some of the the commands are only for the CE (e.g., condor_ce_run and condor_ce_trace ) but many of them are just HTCondor commands configured to interact with the CE (e.g., condor_ce_q , condor_ce_status ). It is important to differentiate the two: condor_ce_config_val will provide configuration values for your HTCondor-CE while condor_config_val will provide configuration values for your HTCondor batch system. If you are not running an HTCondor batch system, the non-CE commands will return errors. condor_ce_trace \u00b6 Usage \u00b6 condor_ce_trace is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job. Note You must have generated a proxy (e.g., voms-proxy-init ) and your DN must be added to your chosen authentication method . user@host $ condor_ce_trace condorce.example.com Replacing the condorce.example.com with the hostname of the CE. If you are familiar with the output of condor commands, the command also takes a --debug option that displays verbose condor output. Troubleshooting \u00b6 If the command fails with \u201cFailed ping\u2026\u201d: Make sure that the HTCondor-CE daemons are running on the CE If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line: Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our installation document . If the job submits but does not complete: Look at the status of the job and perform the relevant troubleshooting steps . condor_ce_host_network_check \u00b6 Usage \u00b6 condor_ce_host_network_check is a tool for testing an HTCondor-CE's networking configuration: root@host # condor_ce_host_network_check Starting analysis of host networking for HTCondor-CE System hostname: fermicloud360.fnal.gov FQDN matches hostname Forward resolution of hostname fermicloud360.fnal.gov is 131.225.155.96. Backward resolution of IPv4 131.225.155.96 is fermicloud360.fnal.gov. Forward and backward resolution match! HTCondor is considering all network interfaces and addresses. HTCondor would pick address of 131.225.155.96 as primary address. HTCondor primary address 131.225.155.96 matches system preferred address. Host network configuration should work with HTCondor-CE Troubleshooting \u00b6 If the tool reports that Host network configuration not expected to work with HTCondor-CE , ensure that forward and reverse DNS resolution return the public IP and hostname. condor_ce_run \u00b6 Usage \u00b6 Similar to globus-job-run , condor_ce_run is a tool that submits a simple job to your CE, so it is useful for quickly submitting jobs through your CE. To submit a job to the CE and run the env command on the remote batch system: Note You must have generated a proxy (e.g., voms-proxy-init ) and your DN must be added to your chosen authentication method . user@host $ condor_ce_run -r condorce.example.com:9619 /bin/env Replacing the condorce.example.com with the hostname of the CE. If you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you can run commands locally on the CE with condor_ce_run with the -l option. The following example outputs the JobRouterLog of the CE in question: user@host $ condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog Replacing the condorce.example.com text with the hostname of the CE. To disable this feature on your CE, consult this section of the install documentation. Troubleshooting \u00b6 If you do not see any results: condor_ce_run does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use condor_ce_q in a separate terminal to track the job on the CE. If you never see any results, use condor_ce_trace to pinpoint errors. If you see an error message that begins with \u201cFailed to\u2026\u201d: Check connectivity to the CE with condor_ce_trace or condor_ce_ping condor_ce_submit \u00b6 See the OSG documentation for submitting to HTCondor-CE for details. condor_ce_ping \u00b6 Usage \u00b6 Use the following condor_ce_ping command to test your ability to submit jobs to an HTCondor-CE, replacing condorce.example.com with the hostname of your CE: user@host $ condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE The following shows successful output where I am able to submit jobs ( Authorized: TRUE ) as the glow user ( Remote Mapping: glow@users.opensciencegrid.org ): Remote Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Local Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Session ID: condorce:27407:1412286981:3 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: GSI Remote Mapping: glow@users.opensciencegrid.org Authorized: TRUE Note If you run the condor_ce_ping command on the CE that you are testing, omit the -name and -pool options. condor_ce_ping takes the same arguments as condor_ping and is documented in the HTCondor manual . Troubleshooting \u00b6 If you see \u201cERROR: couldn\u2019t locate (null)\u201d , that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE: MASTER_DEBUG = D_ALWAYS:2 D_CAT SCHEDD_DEBUG = D_ALWAYS:2 D_CAT Then look in the MasterLog and SchedLog for any errors. If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line , this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our installation document . condor_ce_q \u00b6 Usage \u00b6 condor_ce_q can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 To inspect the full ClassAd for a specific job, specify the -l flag and the job ID: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l <JOB-ID> Note If you run the condor_ce_q command on the CE that you are testing, omit the -name and -pool options. condor_ce_q takes the same arguments as condor_q and is documented in the HTCondor manual . Troubleshooting \u00b6 If the jobs that you are submiting to a CE are not completing, condor_ce_q can tell you the status of your jobs. If the schedd is not running: You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with: MASTER_DEBUG = D_ALWAYS:2 D_CAT SCHEDD_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig Then look in the MasterLog and SchedLog on the CE for any errors. If there are issues with contacting the collector: You will see the following message: user@host $ condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu -- Failed to fetch ads from: <129.59.197.223:9620?sock`33630_8b33_4> : ce1.accre.vanderbilt.edu This may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the ALLOW_READ configuration value is not set: user@host $ condor_ce_config_val -v ALLOW_READ Not defined: ALLOW_READ If it is defined, remove it from the file that is returned in the output. If a job is held: There should be an accompanying HoldReason that will tell you why it is being held. The HoldReason is in the job\u2019s ClassAd, so you can use the long form of condor_ce_q to extract its value: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l <Job ID> | grep HoldReason If a job is idle: The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the condor_ce_job_router_info . condor_ce_history \u00b6 Usage \u00b6 condor_ce_history can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE: user@host $ condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 To inspect the full ClassAd for a specific job, specify the -l flag and the job ID: user@host $ condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l <Job ID> Note If you run the condor_ce_history command on the CE that you are testing, omit the -name and -pool options. condor_ce_history takes the same arguments as condor_history and is documented in the HTCondor manual . condor_ce_job_router_info \u00b6 Usage \u00b6 Use the condor_ce_job_router_info command to help troubleshoot your routes and how jobs will match to them. To see all of your routes (the output is long because it combines your routes with the JOB_ROUTER_DEFAULTS configuration variable): root@host # condor_ce_job_router_info -config To see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of condor_ce_q (replace the <JOB-ID> with the job ID that you are interested in): root@host # condor_ce_q -l <JOB-ID> | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - To inspect a job that has already left the queue, use condor_ce_history instead of condor_ce_q : root@host # condor_ce_history -l <JOB-ID> | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - Note If the proxy for the job has expired, the job will not match any routes. To work around this constraint: root@host # condor_ce_history -l <JOB-ID> | sed \"s/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date '+1 sec'`/\" | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - Alternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file: root@host # condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads <JOBAD-FILE> Troubleshooting \u00b6 If the job does not match any route: You can identify this case when you see 0 candidate jobs found in the condor_job_router_info output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to true . When troubleshooting, look at all of the expressions prior to the target.ProcId >= 0 expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again. If your job matches more than one route: the tool will tell you by showing all matching routes after the job ID: Checking Job src=162,0 against all routes Route Matches: Local_PBS Route Matches: Condor_Test To troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is highlighted below: Umbrella constraint: ((target.x509userproxysubject =!= UNDEFINED) && (target.x509UserProxyExpiration =!= UNDEFINED) && (time() < target.x509UserProxyExpiration) && (target.JobUniverse =?= 5 || target.JobUniverse =?= 1)) && ( (target.osgTestPBS is true) || (true) ) && (target.ProcId >= 0 && target.JobStatus == 1 && (target.StageInStart is undefined || target.StageInFinish isnt undefined) && target.Managed isnt \"ScheddDone\" && target.Managed isnt \"Extenal\" && target.Owner isnt Undefined && target.RoutedBy isnt \"htcondor-ce\") Both routes evaluate to true for the job\u2019s ClassAd because it contained osgTestPBS = true . Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the job route configuration page for more details. If it is unclear why jobs are matching a route: wrap the route's requirements expression in debug() and check the JobRouterLog for more information. condor_ce_router_q \u00b6 Usage \u00b6 If you have multiple job routes and many jobs, condor_ce_router_q is a useful tool to see how jobs are being routed and their statuses: user@host $ condor_ce_router_q condor_ce_router_q takes the same options as condor_router_q and condor_q and is documented in the HTCondor manual condor_ce_status \u00b6 Usage \u00b6 To see the daemons running on a CE, run the following command: user@host $ condor_ce_status -any condor_ce_status takes the same arguments as condor_status , which are documented in the HTCondor manual . \"Missing\" Worker Nodes An HTCondor-CE will not show any worker nodes (e.g. Machine entries in the condor_ce_status -any output) if it does not have any running GlideinWMS pilot jobs. This is expected since HTCondor-CE only forwards incoming grid jobs to your batch system and does not match jobs to worker nodes. Troubleshooting \u00b6 If the output of condor_ce_status -any does not show at least the following daemons: Collector Scheduler DaemonMaster Job_Router Increase the debug level and consult the HTCondor-CE logs for errors. condor_ce_config_val \u00b6 Usage \u00b6 To see the value of configuration variables and where they are set, use condor_ce_config_val . Primarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. To see the value of a single variable and where it is set: user@host $ condor_ce_config_val -v <CONFIGURATION-VARIABLE> To see a list of all configuration variables and their values: user@host $ condor_ce_config_val -dump To see a list of all the files that are used to create your configuration and the order that they are parsed, use the following command: user@host $ condor_ce_config_val -config condor_ce_config_val takes the same arguments as condor_config_val and is documented in the HTCondor manual . condor_ce_reconfig \u00b6 Usage \u00b6 To ensure that your configuration changes have taken effect, run condor_ce_reconfig . user@host $ condor_ce_reconfig condor_ce_{on,off,restart} \u00b6 Usage \u00b6 To turn on/off/restart HTCondor-CE daemons, use the following commands: root@host # condor_ce_on root@host # condor_ce_off root@host # condor_ce_restart The HTCondor-CE service uses the previous commands with default values. Using these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart: If you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command: root@host # condor_ce_restart -fast This will cause HTCondor-CE to restart and quickly reconnect to all running jobs. If you need to stop running new jobs, run the following: root@host # condor_ce_off -peaceful This will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down. HTCondor-CE Troubleshooting Data \u00b6 The following files are located on the CE host. MasterLog \u00b6 The HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if they fail to start. Location: /var/log/condor-ce/MasterLog Key contents: Start-up, shut-down, and communication with other HTCondor daemons Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: MASTER_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig What to look for: \u00b6 Successful daemon start-up. The following line shows that the Collector daemon started successfully: 10/07/14 14:20:27 Started DaemonCore process \"/usr/sbin/condor_collector -f -port 9619\", pid and pgroup = 7318 SchedLog \u00b6 The HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. It contains valuable information when trying to troubleshoot authentication issues. Location: /var/log/condor-ce/SchedLog Key contents: Every job submitted to the CE User authorization events Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: SCHEDD_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig What to look for \u00b6 Job is submitted to the CE queue: 10/07/14 16:52:17 Submitting new job 234.0 In this example, the ID of the submitted job is 234.0 . Job owner is authorized and mapped: 10/07/14 16:52:17 Command=QMGMT_WRITE_CMD, peer=<131.225.154.68:42262> 10/07/14 16:52:17 AuthMethod=GSI, AuthId=/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Lin 1047, /GLOW/Role=NULL/Capability=NULL, <CondorId=glow@users.opensciencegrid.org> In this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the glow user. User job submission fails due to improper authentication or authorization: 08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa) 08/30/16 16:53:12 PERMISSION DENIED to <gsi@unmapped> from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189 08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done Missing negotiator: 10/18/14 17:32:21 Can't find address for negotiator 10/18/14 17:32:21 Failed to send RESCHEDULE to unknown daemon: Since HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes: 06/23/15 11:15:03 Number of Active Workers 0 Corrupted job_queue.log : 02/07/17 10:55:49 WARNING: Encountered corrupt log record _654 (byte offset 5046225) 02/07/17 10:55:49 103 1354325.0 PeriodicRemove ( StageInFinish > 0 ) 105 02/07/17 10:55:49 Lines following corrupt log record _654 (up to 3): 02/07/17 10:55:49 103 1346101.0 RemoteWallClockTime 116668.000000 02/07/17 10:55:49 104 1346101.0 WallClockCheckpoint 02/07/17 10:55:49 104 1346101.0 ShadowBday 02/07/17 10:55:49 ERROR \"Error: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction, recovery failed\" at line 1080 in file /builddir/build/BUILD/condor-8.4.8/src/condor_utils/classad_log.cpp This means /var/lib/condor-ce/spool/job_queue.log has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the Lines following corrupt log record... line. The most common culprit of the corruption is that the disk containing the job_queue.log has filled up. To avoid this problem, you can change the location of job_queue.log by setting JOB_QUEUE_LOG in /etc/condor-ce/config.d/ to a path, preferably one on a large SSD. JobRouterLog \u00b6 The HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to troubleshoot issues with job routing. Location: /var/log/condor-ce/JobRouterLog Key contents: Every attempt to route a job Routing success messages Job attribute changes, based on chosen route Job submission errors to an HTCondor batch system Corresponding job IDs on an HTCondor batch system Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: JOB_ROUTER_DEBUG = D_ALWAYS:2 D_CAT Apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig Known Errors \u00b6 (HTCondor batch systems only) If you see the following error message: Can't find address of schedd This means that HTCondor-CE cannot communicate with your HTCondor batch system. Verify that the condor service is running on the HTCondor-CE host and is configured for your central manager. (HTCondor batch systems only) If you see the following error message: JobRouter failure (src=2810466.0,dest=47968.0,route=MWT2_UCORE): giving up, because submitted job is still not in job queue mirror (submitted 605 seconds ago). Perhaps it has been removed? Ensure that condor_config_val SPOOL and condor_ce_config_val JOB_ROUTER_SCHEDD2_SPOOL return the same value. If they don't, change the value of JOB_ROUTER_SCHEDD2_SPOOL in your HTCondor-CE configuration to match SPOOL from your HTCondor configuration. If you have D_ALWAYS:2 turned on for the job router, you will see errors like the following: 06/12/15 14:00:28 HOOK_UPDATE_JOB_INFO not configured. You can safely ignore these. What to look for \u00b6 Job is considered for routing: 09/17/14 15:00:56 JobRouter (src=86.0,route=Local_LSF): found candidate job In parentheses are the original HTCondor-CE job ID (e.g., 86.0 ) and the route (e.g., Local_LSF ). Job is successfully routed: 09/17/14 15:00:57 JobRouter (src=86.0,route=Local_LSF): claimed job Finding the corresponding job ID on your HTCondor batch system: 09/17/14 15:00:57 JobRouter (src=86.0,dest=205.0,route=Local_Condor): claimed job In parentheses are the original HTCondor-CE job ID (e.g., 86.0 ) and the resultant job ID on the HTCondor batch system (e.g., 205.0 ) If your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the condor_ce_job_router_info HTCondor batch systems only : The following error occurs when the job router daemon cannot submit the routed job: 10/19/14 13:09:15 Can't resolve collector condorce.example.com; skipping 10/19/14 13:09:15 ERROR (pool condorce.example.com) Can't find address of schedd 10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job GridmanagerLog \u00b6 The HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. It contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. Details on how to read the Gridmanager log can be found on the HTCondor Wiki . Location: /var/log/condor-ce/GridmanagerLog.<JOB-OWNER> Key contents: Every attempt to submit a job to a batch system or other grid resource Status updates of submitted jobs Corresponding job IDs on non-HTCondor batch systems Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: MAX_GRIDMANAGER_LOG = 6h MAX_NUM_GRIDMANAGER_LOG = 8 GRIDMANAGER_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig What to look for \u00b6 Job is submitted to the batch system: 09/17/14 09:51:34 [12997] (85.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED Every state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)). Job status being updated: 09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE 09/17/14 15:07:24 [25543] GAHP[25563] <- 'BLAH_JOB_STATUS 3 lsf/20140917/482046' 09/17/14 15:07:24 [25543] GAHP[25563] -> 'S' 09/17/14 15:07:25 [25543] GAHP[25563] <- 'RESULTS' 09/17/14 15:07:25 [25543] GAHP[25563] -> 'R' 09/17/14 15:07:25 [25543] GAHP[25563] -> 'S' '1' 09/17/14 15:07:25 [25543] GAHP[25563] -> '3' '0' 'No Error' '4' '[ BatchjobId = \"482046\"; JobStatus = 4; ExitCode = 0; WorkerNode = \"atl-prod08\" ]' The first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here. Finding the corresponding job ID on your non-HTCondor batch system: 09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE 09/17/14 15:07:24 [25543] GAHP[25563] <- 'BLAH_JOB_STATUS 3 lsf/20140917/482046' On the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses, (87.0) . At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes, lsf/20140917/482046 . Job completion on the batch system: 09/17/14 15:07:25 [25543] (87.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE SharedPortLog \u00b6 The HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the collector. This log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found here . Location: /var/log/condor-ce/SharedPortLog Key contents: Every attempt to connect to HTCondor-CE (except collector queries) Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: SHARED_PORT_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig Messages log \u00b6 The messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. If there are issues with the authentication setup , the errors may appear here. Location: /var/log/messages Key contents: User authentication What to look for \u00b6 A user is mapped: Oct 6 10:35:32 osgserv06 htondor-ce-llgt[12147]: Callout to \"LCMAPS\" returned local user (service condor): \"osgglow01\" BLAHP Configuration File \u00b6 HTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client tools. Location: /etc/blah.config Key contents: Locations of the batch system's client binaries and logs Location to save files that are submitted to the local batch system You can also tell the BLAHP to save the files that are being submitted to the local batch system to <DIR-NAME> by adding the following line: blah_debug_save_submit_info=<DIR_NAME> The BLAHP will then create a directory with the format bl_* for each submission to the local jobmanager with the submit file and proxy used. Note Whitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within <DIR_NAME> . Getting Help \u00b6 If you are still experiencing issues after using this document, please let us know! Gather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.) Gather system information: root@host # osg-system-profiler Start a support request using a web interface or by email to help@opensciencegrid.org Describe issue and expected or desired behavior Include basic HTCondor-CE and related information Attach the osg-system-profiler output Reference \u00b6 Here are some other HTCondor-CE documents that might be helpful: HTCondor-CE overview and architecture Installing HTCondor-CE Configuring HTCondor-CE job routes","title":"Troubleshooting"},{"location":"v3/troubleshooting/troubleshooting/#htcondor-ce-troubleshooting-guide","text":"In this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of common issues with suggested troubleshooting steps.","title":"HTCondor-CE Troubleshooting Guide"},{"location":"v3/troubleshooting/troubleshooting/#known-issues","text":"","title":"Known Issues"},{"location":"v3/troubleshooting/troubleshooting/#submit_attrs-are-not-applied-to-jobs-on-the-local-htcondor","text":"If you are adding attributes to jobs submitted to your HTCondor pool with SUBMIT_ATTRS , these will not be applied to jobs that are entering your pool from the HTCondor-CE. To get around this, you will want to add the attributes to your job routes . If the CE is the only entry point for jobs into your pool, you can get rid of SUBMIT_ATTRS on your backend. Otherwise, you will have to maintain your list of attributes both in your list of routes and in your SUBMIT_ATTRS .","title":"SUBMIT_ATTRS are not applied to jobs on the local HTCondor"},{"location":"v3/troubleshooting/troubleshooting/#general-troubleshooting-items","text":"","title":"General Troubleshooting Items"},{"location":"v3/troubleshooting/troubleshooting/#making-sure-packages-are-up-to-date","text":"It is important to make sure that the HTCondor-CE and related RPMs are up-to-date. root@host # yum update \"htcondor-ce*\" blahp condor If you just want to see the packages to update, but do not want to perform the update now, answer N at the prompt.","title":"Making sure packages are up-to-date"},{"location":"v3/troubleshooting/troubleshooting/#verify-package-contents","text":"If the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the contents of your packages (ignoring changes to configuration files): user@host $ rpm -q --verify htcondor-ce htcondor-ce-client blahp | grep -v '/var/' | awk '$2 != \"c\" {print $0}' If the verification command returns output, this means that your packages have been changed. To fix this, you can reinstall the packages: user@host $ yum reinstall htcondor-ce htcondor-ce-client blahp Note The reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an .rpmnew suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration.","title":"Verify package contents"},{"location":"v3/troubleshooting/troubleshooting/#verify-clocks-are-synchronized","text":"Like all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is synchronized using a utility such as ntpd . Additionally, HTCondor itself is sensitive to time skews on the NFS server. If you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew.","title":"Verify clocks are synchronized"},{"location":"v3/troubleshooting/troubleshooting/#verify-host-cerificates-and-crls-are-valid","text":"An expired host certificate or CRLs will cause various issues with GSI authentication. Verify that your host certificate is valid by running: root@host # openssl x509 -in /etc/grid-security/hostcert.pem -noout -dates Likewise, run the fetch-crl script to update your CRLs: root@host # fetch-crl If updating CRLs fix your issues, make sure that the fetch-crl-cron and fetch-crl-boot services are enabled and running.","title":"Verify host cerificates and CRLs are valid"},{"location":"v3/troubleshooting/troubleshooting/#htcondor-ce-troubleshooting-items","text":"This section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. Before troubleshooting, we recommend increasing the log level: Write the following into /etc/condor-ce/config.d/99-local.conf to increase the log level for all daemons: ALL_DEBUG = D_ALWAYS:2 D_CAT Ensure that the configuration is in place: root@host # condor_ce_reconfig Reproduce the issue Note Before spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running condor_ce_reconfig .","title":"HTCondor-CE Troubleshooting Items"},{"location":"v3/troubleshooting/troubleshooting/#daemons-fail-to-start","text":"If there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to startup. Check the following subsections in order: Symptoms Daemon startup failure may manifest in many ways, the following are few symptoms of the problem. The service fails to start: root@host # service condor-ce start Starting Condor-CE daemons: [ FAIL ] condor_ce_q fails with a lengthy error message: user@host $ condor_ce_q Error: Extra Info: You probably saw this error because the condor_schedd is not running on the machine you are trying to query. If the condor_schedd is not running, the Condor system will not be able to find an address and port to connect to and satisfy this request. Please make sure the Condor daemons are running and try again. Extra Info: If the condor_schedd is running on the machine you are trying to query and you still see the error, the most likely cause is that you have setup a personal Condor, you have not defined SCHEDD_NAME in your condor_config file, and something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define either or both of those settings in your config file, or you must use the -name option to condor_q. Please see the Condor manual for details on SCHEDD_NAME and SCHEDD_ADDRESS_FILE. Next actions If the MasterLog is filled with ERROR:SECMAN...TCP connection to collector...failed : This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in this section of the install guide. If the MasterLog is filled with DC_AUTHENTICATE errors: The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in /etc/condor-ce/condor_mapfile . If the SchedLog is filled with Can\u2019t find address for negotiator : You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one.","title":"Daemons fail to start"},{"location":"v3/troubleshooting/troubleshooting/#jobs-fail-to-submit-to-the-ce","text":"If a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the culprit, then you may have encountered an authentication or authorization issue. You may see error messages like the following in your SchedLog : 08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa) 08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189 08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done! Next actions Check voms-mapfile or grid-mapfile and ensure that the user's DN or VOMS attributes are known to your authentication method , and that the mapped users exist on your CE and cluster. Check for lcmaps errors in /var/log/messages If you do not see helpful error messages in /var/log/messages , adjust the debug level by adding export LCMAPS_DEBUG_LEVEL=5 to /etc/sysconfig/condor-ce , restarting the condor-ce service, and checking /var/log/messages for errors again.","title":"Jobs fail to submit to the CE"},{"location":"v3/troubleshooting/troubleshooting/#jobs-stay-idle-on-the-ce","text":"Check the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is busy.","title":"Jobs stay idle on the CE"},{"location":"v3/troubleshooting/troubleshooting/#idle-jobs-on-ce-make-sure-the-underlying-batch-system-can-run-jobs","text":"HTCondor-CE delegates jobs to your batch system, which is then responsible for matching jobs to worker nodes. If you cannot manually submit jobs (e.g., condor_submit , qsub ) on the CE host to your batch system, then HTCondor-CE won't be able to either. Procedure Manually create and submit a simple job (e.g., one that runs sleep ) Check for errors in the submission itself Watch the job in the batch system queue (e.g., condor_q , qstat ) If the job does not run, check for errors on the batch system Next actions Consult troubleshooting documentation or support avenues for your batch system. Once you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again.","title":"Idle jobs on CE: Make sure the underlying batch system can run jobs"},{"location":"v3/troubleshooting/troubleshooting/#idle-jobs-on-ce-is-the-job-router-handling-the-incoming-job","text":"Jobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things if you suspect that the jobs are not being matched. Check if the JobRouter sees a job before that by looking at the job router log and looking for the text src=<JOB-ID>\u2026claimed job . Next actions Use condor_ce_job_router_info to see why your idle job does not match any routes","title":"Idle jobs on CE: Is the job router handling the incoming job?"},{"location":"v3/troubleshooting/troubleshooting/#idle-jobs-on-ce-verify-correct-operation-between-the-ce-and-your-local-batch-system","text":"","title":"Idle jobs on CE: Verify correct operation between the CE and your local batch system"},{"location":"v3/troubleshooting/troubleshooting/#for-htcondor-batch-systems","text":"HTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch system interaction will appear in the JobRouterLog . Next actions Check the JobRouterLog for failures. Verify that the local HTCondor is functional. Use condor_ce_config_val to verify that the JOB_ROUTER_SCHEDD2_NAME , JOB_ROUTER_SCHEDD2_POOL , and JOB_ROUTER_SCHEDD2_SPOOL configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively. Use condor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE and verify that it is set to .* .","title":"For HTCondor batch systems"},{"location":"v3/troubleshooting/troubleshooting/#for-non-htcondor-batch-systems","text":"HTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch system interaction will appear in the GridmanagerLog . Look for gm state change\u2026 lines to figure out where the issures are occuring. Next actions If you see failures in the GridmanagerLog during job submission: Save the submit files by adding the appropriate entry to blah.config and submit it manually to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the <batch system>_binpath in /etc/blah.config . If you see failures in the GridmanagerLog during queries for job status: Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in /usr/libexec/blahp/<batch system>_status.sh (e.g., /usr/libexec/blahp/lsf_status.sh ) that take the argument batch system/YYYMMDD/job ID (e.g., lsf/20141008/65053 ). Run the appropriate status script for your batch system and upon success, you should see the following output: root@host # /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053 [ BatchjobId = \"894862\"; JobStatus = 4; ExitCode = 0; WorkerNode = \"atl-prod08\" ] If the script fails, request help from the OSG.","title":"For non-HTCondor batch systems"},{"location":"v3/troubleshooting/troubleshooting/#idle-jobs-on-ce-verify-ability-to-change-permissions-on-key-files","text":"HTCondor-CE needs the ability to write and chown files in its spool directory and if it cannot, jobs will not run at all. Spool permission errors can appear in the SchedLog and the JobRouterLog . Symptoms 09/17/14 14:45:42 Error: Unable to chown '/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env' from 12345 to 54321 Next actions As root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions. Verify that there aren't any underlying file system issues in the specified location","title":"Idle jobs on CE: Verify ability to change permissions on key files"},{"location":"v3/troubleshooting/troubleshooting/#jobs-stay-idle-on-a-remote-host-submitting-to-the-ce","text":"If you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not see a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not authorized to run there. Note that jobs may take several minutes or longer if the CE is busy.","title":"Jobs stay idle on a remote host submitting to the CE"},{"location":"v3/troubleshooting/troubleshooting/#remote-idle-jobs-can-you-contact-the-ce","text":"To check basic connectivity to a CE, use condor_ce_ping : Symptoms user@host $ condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE ERROR: couldn't locate condorce.example.com! Next actions Make sure that the HTCondor-CE daemons are running with condor_ce_status . Verify that your CE is reachable from your submit host, replacing condorce.example.com with the hostname of your CE: user@host $ ping condorce.example.com","title":"Remote idle jobs: Can you contact the CE?"},{"location":"v3/troubleshooting/troubleshooting/#remote-idle-jobs-are-you-authorized-to-run-jobs-on-the-ce","text":"The CE will only run jobs from users that authenticate through the HTCondor-CE configuration . You can use condor_ce_ping to check if you are authorized and what user your proxy is being mapped to. Symptoms user@host $ condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE Remote Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Local Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Session ID: condorce:3343:1412790611:0 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: GSI Remote Mapping: gsi@unmapped Authorized: FALSE Notice the failures in the above message: Remote Mapping: gsi@unmapped and Authorized: FALSE Next actions Verify that an authentication method is set up on the CE Verify that your user DN is mapped to an existing system user","title":"Remote idle jobs: Are you authorized to run jobs on the CE?"},{"location":"v3/troubleshooting/troubleshooting/#jobs-go-on-hold","text":"Jobs will be put on held with a HoldReason attribute that can be inspected with condor_ce_q : user@host $ condor_ce_q -l <JOB-ID> -attr HoldReason HoldReason = \"CE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to no matching routes, route job limit, or route failure threshold.\"","title":"Jobs go on hold"},{"location":"v3/troubleshooting/troubleshooting/#held-jobs-no-matching-routes-route-job-limit-or-route-failure-threshold","text":"Jobs on the CE will be put on hold if they are not claimed by the job router within 30 minutes. The most common cases for this behavior are as follows: The job does not match any job routes: use condor_ce_job_router_info to see why your idle job does not match any routes . The route(s) that the job matches to are full: See limiting the number of jobs . The job router is throttling submission to your batch system due to submission failures: See the HTCondor manual for FailureRateThreshold . Check for errors in the JobRouterLog or GridmanagerLog for HTCondor and non-HTCondor batch systems, respectively.","title":"Held jobs: no matching routes, route job limit, or route failure threshold"},{"location":"v3/troubleshooting/troubleshooting/#held-jobs-missingexpired-user-proxy","text":"HTCondor-CE requires a valid user proxy for each job that is submitted. You can check the status of your proxy with the following user@host $ voms-proxy-info -all Next actions Ensure that the owner of the job generates their proxy with voms-proxy-init .","title":"Held jobs: Missing/expired user proxy"},{"location":"v3/troubleshooting/troubleshooting/#held-jobs-invalid-job-universe","text":"The HTCondor-CE only accepts jobs that have universe in their submit files set to vanilla , standard , local , or scheduler . These universes also have corresponding integer values that can be found in the HTCondor manual . Next actions Ensure jobs submitted locally, from the CE host, are submitted with universe = vanilla Ensure jobs submitted from a remote submit point are submitted with: universe = grid grid_resource = condor condorce.example.com condorce.example.com:9619 replacing condorce.example.com with the hostname of the CE.","title":"Held jobs: Invalid job universe"},{"location":"v3/troubleshooting/troubleshooting/#identifying-the-corresponding-job-id-on-the-local-batch-system","text":"When troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID and the resultant job ID on the batch system. The methods for finding the resultant job ID differs between batch systems.","title":"Identifying the corresponding job ID on the local batch system"},{"location":"v3/troubleshooting/troubleshooting/#htcondor-batch-systems","text":"To inspect the CE\u2019s job ad, use condor_ce_q or condor_ce_history : Use condor_ce_q if the job is still in the CE\u2019s queue: user@host $ condor_ce_q <JOB-ID> -af RoutedToJobId Use condor_ce_history if the job has left the CE\u2019s queue: user@host $ condor_ce_history <JOB-ID> -af RoutedToJobId Parse the JobRouterLog for the CE\u2019s job ID.","title":"HTCondor batch systems"},{"location":"v3/troubleshooting/troubleshooting/#non-htcondor-batch-systems","text":"When HTCondor-CE records the corresponding batch system job ID, it is written in the form <BATCH-SYSTEM>/<DATE>/<JOB ID> : lsf/20141206/482046 To inspect the CE\u2019s job ad, use condor_ce_q : user@host $ condor_ce_q <JOB-ID> -af GridJobId Parse the GridmanagerLog for the CE\u2019s job ID.","title":"Non-HTCondor batch systems"},{"location":"v3/troubleshooting/troubleshooting/#jobs-removed-from-the-local-htcondor-pool-become-resubmitted-htcondor-batch-systems-only","text":"By design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. Therefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps: Identify the misbehaving job ID in your batch system queue Find the job's corresponding CE job ID: user@host $ condor_q <JOB-ID> -af RoutedFromJobId Use condor_ce_rm to remove the CE job from the queue","title":"Jobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only)"},{"location":"v3/troubleshooting/troubleshooting/#missing-htcondor-tools","text":"Most of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. If you are trying to use HTCondor-CE tools and you see the following error: user@host $ condor_ce_job_router_info /usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found This means that the condor_job_router_info (note this is not the CE version), is not in your PATH . Next Actions Either the condor RPM is missing or there are some other issues with it (try rpm --verify condor ). You have installed HTCondor in a non-standard location that is not in your PATH . The condor_job_router_info tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming).","title":"Missing HTCondor tools"},{"location":"v3/troubleshooting/troubleshooting/#htcondor-ce-troubleshooting-tools","text":"HTCondor-CE has its own separate set of of the HTCondor tools with ce in the name (i.e., condor_ce_submit vs condor_submit ). Some of the the commands are only for the CE (e.g., condor_ce_run and condor_ce_trace ) but many of them are just HTCondor commands configured to interact with the CE (e.g., condor_ce_q , condor_ce_status ). It is important to differentiate the two: condor_ce_config_val will provide configuration values for your HTCondor-CE while condor_config_val will provide configuration values for your HTCondor batch system. If you are not running an HTCondor batch system, the non-CE commands will return errors.","title":"HTCondor-CE Troubleshooting Tools"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_trace","text":"","title":"condor_ce_trace"},{"location":"v3/troubleshooting/troubleshooting/#usage","text":"condor_ce_trace is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job. Note You must have generated a proxy (e.g., voms-proxy-init ) and your DN must be added to your chosen authentication method . user@host $ condor_ce_trace condorce.example.com Replacing the condorce.example.com with the hostname of the CE. If you are familiar with the output of condor commands, the command also takes a --debug option that displays verbose condor output.","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#troubleshooting","text":"If the command fails with \u201cFailed ping\u2026\u201d: Make sure that the HTCondor-CE daemons are running on the CE If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line: Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our installation document . If the job submits but does not complete: Look at the status of the job and perform the relevant troubleshooting steps .","title":"Troubleshooting"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_host_network_check","text":"","title":"condor_ce_host_network_check"},{"location":"v3/troubleshooting/troubleshooting/#usage_1","text":"condor_ce_host_network_check is a tool for testing an HTCondor-CE's networking configuration: root@host # condor_ce_host_network_check Starting analysis of host networking for HTCondor-CE System hostname: fermicloud360.fnal.gov FQDN matches hostname Forward resolution of hostname fermicloud360.fnal.gov is 131.225.155.96. Backward resolution of IPv4 131.225.155.96 is fermicloud360.fnal.gov. Forward and backward resolution match! HTCondor is considering all network interfaces and addresses. HTCondor would pick address of 131.225.155.96 as primary address. HTCondor primary address 131.225.155.96 matches system preferred address. Host network configuration should work with HTCondor-CE","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#troubleshooting_1","text":"If the tool reports that Host network configuration not expected to work with HTCondor-CE , ensure that forward and reverse DNS resolution return the public IP and hostname.","title":"Troubleshooting"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_run","text":"","title":"condor_ce_run"},{"location":"v3/troubleshooting/troubleshooting/#usage_2","text":"Similar to globus-job-run , condor_ce_run is a tool that submits a simple job to your CE, so it is useful for quickly submitting jobs through your CE. To submit a job to the CE and run the env command on the remote batch system: Note You must have generated a proxy (e.g., voms-proxy-init ) and your DN must be added to your chosen authentication method . user@host $ condor_ce_run -r condorce.example.com:9619 /bin/env Replacing the condorce.example.com with the hostname of the CE. If you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you can run commands locally on the CE with condor_ce_run with the -l option. The following example outputs the JobRouterLog of the CE in question: user@host $ condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog Replacing the condorce.example.com text with the hostname of the CE. To disable this feature on your CE, consult this section of the install documentation.","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#troubleshooting_2","text":"If you do not see any results: condor_ce_run does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use condor_ce_q in a separate terminal to track the job on the CE. If you never see any results, use condor_ce_trace to pinpoint errors. If you see an error message that begins with \u201cFailed to\u2026\u201d: Check connectivity to the CE with condor_ce_trace or condor_ce_ping","title":"Troubleshooting"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_submit","text":"See the OSG documentation for submitting to HTCondor-CE for details.","title":"condor_ce_submit"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_ping","text":"","title":"condor_ce_ping"},{"location":"v3/troubleshooting/troubleshooting/#usage_3","text":"Use the following condor_ce_ping command to test your ability to submit jobs to an HTCondor-CE, replacing condorce.example.com with the hostname of your CE: user@host $ condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE The following shows successful output where I am able to submit jobs ( Authorized: TRUE ) as the glow user ( Remote Mapping: glow@users.opensciencegrid.org ): Remote Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Local Version: $CondorVersion: 8.0.7 Sep 24 2014 $ Session ID: condorce:27407:1412286981:3 Instruction: WRITE Command: 60021 Encryption: none Integrity: MD5 Authenticated using: GSI All authentication methods: GSI Remote Mapping: glow@users.opensciencegrid.org Authorized: TRUE Note If you run the condor_ce_ping command on the CE that you are testing, omit the -name and -pool options. condor_ce_ping takes the same arguments as condor_ping and is documented in the HTCondor manual .","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#troubleshooting_3","text":"If you see \u201cERROR: couldn\u2019t locate (null)\u201d , that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE: MASTER_DEBUG = D_ALWAYS:2 D_CAT SCHEDD_DEBUG = D_ALWAYS:2 D_CAT Then look in the MasterLog and SchedLog for any errors. If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line , this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our installation document .","title":"Troubleshooting"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_q","text":"","title":"condor_ce_q"},{"location":"v3/troubleshooting/troubleshooting/#usage_4","text":"condor_ce_q can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 To inspect the full ClassAd for a specific job, specify the -l flag and the job ID: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l <JOB-ID> Note If you run the condor_ce_q command on the CE that you are testing, omit the -name and -pool options. condor_ce_q takes the same arguments as condor_q and is documented in the HTCondor manual .","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#troubleshooting_4","text":"If the jobs that you are submiting to a CE are not completing, condor_ce_q can tell you the status of your jobs. If the schedd is not running: You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with: MASTER_DEBUG = D_ALWAYS:2 D_CAT SCHEDD_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig Then look in the MasterLog and SchedLog on the CE for any errors. If there are issues with contacting the collector: You will see the following message: user@host $ condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu -- Failed to fetch ads from: <129.59.197.223:9620?sock`33630_8b33_4> : ce1.accre.vanderbilt.edu This may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the ALLOW_READ configuration value is not set: user@host $ condor_ce_config_val -v ALLOW_READ Not defined: ALLOW_READ If it is defined, remove it from the file that is returned in the output. If a job is held: There should be an accompanying HoldReason that will tell you why it is being held. The HoldReason is in the job\u2019s ClassAd, so you can use the long form of condor_ce_q to extract its value: user@host $ condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l <Job ID> | grep HoldReason If a job is idle: The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the condor_ce_job_router_info .","title":"Troubleshooting"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_history","text":"","title":"condor_ce_history"},{"location":"v3/troubleshooting/troubleshooting/#usage_5","text":"condor_ce_history can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE: user@host $ condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 To inspect the full ClassAd for a specific job, specify the -l flag and the job ID: user@host $ condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l <Job ID> Note If you run the condor_ce_history command on the CE that you are testing, omit the -name and -pool options. condor_ce_history takes the same arguments as condor_history and is documented in the HTCondor manual .","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_job_router_info","text":"","title":"condor_ce_job_router_info"},{"location":"v3/troubleshooting/troubleshooting/#usage_6","text":"Use the condor_ce_job_router_info command to help troubleshoot your routes and how jobs will match to them. To see all of your routes (the output is long because it combines your routes with the JOB_ROUTER_DEFAULTS configuration variable): root@host # condor_ce_job_router_info -config To see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of condor_ce_q (replace the <JOB-ID> with the job ID that you are interested in): root@host # condor_ce_q -l <JOB-ID> | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - To inspect a job that has already left the queue, use condor_ce_history instead of condor_ce_q : root@host # condor_ce_history -l <JOB-ID> | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - Note If the proxy for the job has expired, the job will not match any routes. To work around this constraint: root@host # condor_ce_history -l <JOB-ID> | sed \"s/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date '+1 sec'`/\" | condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads - Alternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file: root@host # condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads <JOBAD-FILE>","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#troubleshooting_5","text":"If the job does not match any route: You can identify this case when you see 0 candidate jobs found in the condor_job_router_info output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to true . When troubleshooting, look at all of the expressions prior to the target.ProcId >= 0 expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again. If your job matches more than one route: the tool will tell you by showing all matching routes after the job ID: Checking Job src=162,0 against all routes Route Matches: Local_PBS Route Matches: Condor_Test To troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is highlighted below: Umbrella constraint: ((target.x509userproxysubject =!= UNDEFINED) && (target.x509UserProxyExpiration =!= UNDEFINED) && (time() < target.x509UserProxyExpiration) && (target.JobUniverse =?= 5 || target.JobUniverse =?= 1)) && ( (target.osgTestPBS is true) || (true) ) && (target.ProcId >= 0 && target.JobStatus == 1 && (target.StageInStart is undefined || target.StageInFinish isnt undefined) && target.Managed isnt \"ScheddDone\" && target.Managed isnt \"Extenal\" && target.Owner isnt Undefined && target.RoutedBy isnt \"htcondor-ce\") Both routes evaluate to true for the job\u2019s ClassAd because it contained osgTestPBS = true . Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the job route configuration page for more details. If it is unclear why jobs are matching a route: wrap the route's requirements expression in debug() and check the JobRouterLog for more information.","title":"Troubleshooting"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_router_q","text":"","title":"condor_ce_router_q"},{"location":"v3/troubleshooting/troubleshooting/#usage_7","text":"If you have multiple job routes and many jobs, condor_ce_router_q is a useful tool to see how jobs are being routed and their statuses: user@host $ condor_ce_router_q condor_ce_router_q takes the same options as condor_router_q and condor_q and is documented in the HTCondor manual","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_status","text":"","title":"condor_ce_status"},{"location":"v3/troubleshooting/troubleshooting/#usage_8","text":"To see the daemons running on a CE, run the following command: user@host $ condor_ce_status -any condor_ce_status takes the same arguments as condor_status , which are documented in the HTCondor manual . \"Missing\" Worker Nodes An HTCondor-CE will not show any worker nodes (e.g. Machine entries in the condor_ce_status -any output) if it does not have any running GlideinWMS pilot jobs. This is expected since HTCondor-CE only forwards incoming grid jobs to your batch system and does not match jobs to worker nodes.","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#troubleshooting_6","text":"If the output of condor_ce_status -any does not show at least the following daemons: Collector Scheduler DaemonMaster Job_Router Increase the debug level and consult the HTCondor-CE logs for errors.","title":"Troubleshooting"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_config_val","text":"","title":"condor_ce_config_val"},{"location":"v3/troubleshooting/troubleshooting/#usage_9","text":"To see the value of configuration variables and where they are set, use condor_ce_config_val . Primarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. To see the value of a single variable and where it is set: user@host $ condor_ce_config_val -v <CONFIGURATION-VARIABLE> To see a list of all configuration variables and their values: user@host $ condor_ce_config_val -dump To see a list of all the files that are used to create your configuration and the order that they are parsed, use the following command: user@host $ condor_ce_config_val -config condor_ce_config_val takes the same arguments as condor_config_val and is documented in the HTCondor manual .","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_reconfig","text":"","title":"condor_ce_reconfig"},{"location":"v3/troubleshooting/troubleshooting/#usage_10","text":"To ensure that your configuration changes have taken effect, run condor_ce_reconfig . user@host $ condor_ce_reconfig","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#condor_ce_onoffrestart","text":"","title":"condor_ce_{on,off,restart}"},{"location":"v3/troubleshooting/troubleshooting/#usage_11","text":"To turn on/off/restart HTCondor-CE daemons, use the following commands: root@host # condor_ce_on root@host # condor_ce_off root@host # condor_ce_restart The HTCondor-CE service uses the previous commands with default values. Using these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart: If you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command: root@host # condor_ce_restart -fast This will cause HTCondor-CE to restart and quickly reconnect to all running jobs. If you need to stop running new jobs, run the following: root@host # condor_ce_off -peaceful This will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down.","title":"Usage"},{"location":"v3/troubleshooting/troubleshooting/#htcondor-ce-troubleshooting-data","text":"The following files are located on the CE host.","title":"HTCondor-CE Troubleshooting Data"},{"location":"v3/troubleshooting/troubleshooting/#masterlog","text":"The HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if they fail to start. Location: /var/log/condor-ce/MasterLog Key contents: Start-up, shut-down, and communication with other HTCondor daemons Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: MASTER_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"MasterLog"},{"location":"v3/troubleshooting/troubleshooting/#what-to-look-for","text":"Successful daemon start-up. The following line shows that the Collector daemon started successfully: 10/07/14 14:20:27 Started DaemonCore process \"/usr/sbin/condor_collector -f -port 9619\", pid and pgroup = 7318","title":"What to look for:"},{"location":"v3/troubleshooting/troubleshooting/#schedlog","text":"The HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. It contains valuable information when trying to troubleshoot authentication issues. Location: /var/log/condor-ce/SchedLog Key contents: Every job submitted to the CE User authorization events Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: SCHEDD_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"SchedLog"},{"location":"v3/troubleshooting/troubleshooting/#what-to-look-for_1","text":"Job is submitted to the CE queue: 10/07/14 16:52:17 Submitting new job 234.0 In this example, the ID of the submitted job is 234.0 . Job owner is authorized and mapped: 10/07/14 16:52:17 Command=QMGMT_WRITE_CMD, peer=<131.225.154.68:42262> 10/07/14 16:52:17 AuthMethod=GSI, AuthId=/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Lin 1047, /GLOW/Role=NULL/Capability=NULL, <CondorId=glow@users.opensciencegrid.org> In this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the glow user. User job submission fails due to improper authentication or authorization: 08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa) 08/30/16 16:53:12 PERMISSION DENIED to <gsi@unmapped> from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189 08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done Missing negotiator: 10/18/14 17:32:21 Can't find address for negotiator 10/18/14 17:32:21 Failed to send RESCHEDULE to unknown daemon: Since HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes: 06/23/15 11:15:03 Number of Active Workers 0 Corrupted job_queue.log : 02/07/17 10:55:49 WARNING: Encountered corrupt log record _654 (byte offset 5046225) 02/07/17 10:55:49 103 1354325.0 PeriodicRemove ( StageInFinish > 0 ) 105 02/07/17 10:55:49 Lines following corrupt log record _654 (up to 3): 02/07/17 10:55:49 103 1346101.0 RemoteWallClockTime 116668.000000 02/07/17 10:55:49 104 1346101.0 WallClockCheckpoint 02/07/17 10:55:49 104 1346101.0 ShadowBday 02/07/17 10:55:49 ERROR \"Error: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction, recovery failed\" at line 1080 in file /builddir/build/BUILD/condor-8.4.8/src/condor_utils/classad_log.cpp This means /var/lib/condor-ce/spool/job_queue.log has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the Lines following corrupt log record... line. The most common culprit of the corruption is that the disk containing the job_queue.log has filled up. To avoid this problem, you can change the location of job_queue.log by setting JOB_QUEUE_LOG in /etc/condor-ce/config.d/ to a path, preferably one on a large SSD.","title":"What to look for"},{"location":"v3/troubleshooting/troubleshooting/#jobrouterlog","text":"The HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to troubleshoot issues with job routing. Location: /var/log/condor-ce/JobRouterLog Key contents: Every attempt to route a job Routing success messages Job attribute changes, based on chosen route Job submission errors to an HTCondor batch system Corresponding job IDs on an HTCondor batch system Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: JOB_ROUTER_DEBUG = D_ALWAYS:2 D_CAT Apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"JobRouterLog"},{"location":"v3/troubleshooting/troubleshooting/#known-errors","text":"(HTCondor batch systems only) If you see the following error message: Can't find address of schedd This means that HTCondor-CE cannot communicate with your HTCondor batch system. Verify that the condor service is running on the HTCondor-CE host and is configured for your central manager. (HTCondor batch systems only) If you see the following error message: JobRouter failure (src=2810466.0,dest=47968.0,route=MWT2_UCORE): giving up, because submitted job is still not in job queue mirror (submitted 605 seconds ago). Perhaps it has been removed? Ensure that condor_config_val SPOOL and condor_ce_config_val JOB_ROUTER_SCHEDD2_SPOOL return the same value. If they don't, change the value of JOB_ROUTER_SCHEDD2_SPOOL in your HTCondor-CE configuration to match SPOOL from your HTCondor configuration. If you have D_ALWAYS:2 turned on for the job router, you will see errors like the following: 06/12/15 14:00:28 HOOK_UPDATE_JOB_INFO not configured. You can safely ignore these.","title":"Known Errors"},{"location":"v3/troubleshooting/troubleshooting/#what-to-look-for_2","text":"Job is considered for routing: 09/17/14 15:00:56 JobRouter (src=86.0,route=Local_LSF): found candidate job In parentheses are the original HTCondor-CE job ID (e.g., 86.0 ) and the route (e.g., Local_LSF ). Job is successfully routed: 09/17/14 15:00:57 JobRouter (src=86.0,route=Local_LSF): claimed job Finding the corresponding job ID on your HTCondor batch system: 09/17/14 15:00:57 JobRouter (src=86.0,dest=205.0,route=Local_Condor): claimed job In parentheses are the original HTCondor-CE job ID (e.g., 86.0 ) and the resultant job ID on the HTCondor batch system (e.g., 205.0 ) If your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the condor_ce_job_router_info HTCondor batch systems only : The following error occurs when the job router daemon cannot submit the routed job: 10/19/14 13:09:15 Can't resolve collector condorce.example.com; skipping 10/19/14 13:09:15 ERROR (pool condorce.example.com) Can't find address of schedd 10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job","title":"What to look for"},{"location":"v3/troubleshooting/troubleshooting/#gridmanagerlog","text":"The HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. It contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. Details on how to read the Gridmanager log can be found on the HTCondor Wiki . Location: /var/log/condor-ce/GridmanagerLog.<JOB-OWNER> Key contents: Every attempt to submit a job to a batch system or other grid resource Status updates of submitted jobs Corresponding job IDs on non-HTCondor batch systems Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: MAX_GRIDMANAGER_LOG = 6h MAX_NUM_GRIDMANAGER_LOG = 8 GRIDMANAGER_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"GridmanagerLog"},{"location":"v3/troubleshooting/troubleshooting/#what-to-look-for_3","text":"Job is submitted to the batch system: 09/17/14 09:51:34 [12997] (85.0) gm state change: GM_SUBMIT_SAVE -> GM_SUBMITTED Every state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)). Job status being updated: 09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE 09/17/14 15:07:24 [25543] GAHP[25563] <- 'BLAH_JOB_STATUS 3 lsf/20140917/482046' 09/17/14 15:07:24 [25543] GAHP[25563] -> 'S' 09/17/14 15:07:25 [25543] GAHP[25563] <- 'RESULTS' 09/17/14 15:07:25 [25543] GAHP[25563] -> 'R' 09/17/14 15:07:25 [25543] GAHP[25563] -> 'S' '1' 09/17/14 15:07:25 [25543] GAHP[25563] -> '3' '0' 'No Error' '4' '[ BatchjobId = \"482046\"; JobStatus = 4; ExitCode = 0; WorkerNode = \"atl-prod08\" ]' The first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here. Finding the corresponding job ID on your non-HTCondor batch system: 09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -> GM_POLL_ACTIVE 09/17/14 15:07:24 [25543] GAHP[25563] <- 'BLAH_JOB_STATUS 3 lsf/20140917/482046' On the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses, (87.0) . At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes, lsf/20140917/482046 . Job completion on the batch system: 09/17/14 15:07:25 [25543] (87.0) gm state change: GM_TRANSFER_OUTPUT -> GM_DONE_SAVE","title":"What to look for"},{"location":"v3/troubleshooting/troubleshooting/#sharedportlog","text":"The HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the collector. This log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found here . Location: /var/log/condor-ce/SharedPortLog Key contents: Every attempt to connect to HTCondor-CE (except collector queries) Increasing the debug level: Set the following value in /etc/condor-ce/config.d/99-local.conf on the CE host: SHARED_PORT_DEBUG = D_ALWAYS:2 D_CAT To apply these changes, reconfigure HTCondor-CE: root@host # condor_ce_reconfig","title":"SharedPortLog"},{"location":"v3/troubleshooting/troubleshooting/#messages-log","text":"The messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. If there are issues with the authentication setup , the errors may appear here. Location: /var/log/messages Key contents: User authentication","title":"Messages log"},{"location":"v3/troubleshooting/troubleshooting/#what-to-look-for_4","text":"A user is mapped: Oct 6 10:35:32 osgserv06 htondor-ce-llgt[12147]: Callout to \"LCMAPS\" returned local user (service condor): \"osgglow01\"","title":"What to look for"},{"location":"v3/troubleshooting/troubleshooting/#blahp-configuration-file","text":"HTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client tools. Location: /etc/blah.config Key contents: Locations of the batch system's client binaries and logs Location to save files that are submitted to the local batch system You can also tell the BLAHP to save the files that are being submitted to the local batch system to <DIR-NAME> by adding the following line: blah_debug_save_submit_info=<DIR_NAME> The BLAHP will then create a directory with the format bl_* for each submission to the local jobmanager with the submit file and proxy used. Note Whitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within <DIR_NAME> .","title":"BLAHP Configuration File"},{"location":"v3/troubleshooting/troubleshooting/#getting-help","text":"If you are still experiencing issues after using this document, please let us know! Gather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.) Gather system information: root@host # osg-system-profiler Start a support request using a web interface or by email to help@opensciencegrid.org Describe issue and expected or desired behavior Include basic HTCondor-CE and related information Attach the osg-system-profiler output","title":"Getting Help"},{"location":"v3/troubleshooting/troubleshooting/#reference","text":"Here are some other HTCondor-CE documents that might be helpful: HTCondor-CE overview and architecture Installing HTCondor-CE Configuring HTCondor-CE job routes","title":"Reference"}]}